import json
import logging
from pathlib import Path
from typing import Dict, List, Tuple

class TranslationSyncer:
    def __init__(self):
        self.setup_logging()
        self.source_dir = Path("data/workflow")
        self.target_dir = Path("data/newworkflow")
        
        # éœ€è¦åŒæ­¥çš„ç¿»è¯‘å­—æ®µ
        self.translation_fields = [
            'readme_zh',
            'title_zh', 
            'publish_date_zh',
            'workflow_json_zh'
        ]
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        log_dir = Path("data/newworkflow/logs")
        log_dir.mkdir(parents=True, exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler(),
                logging.FileHandler('data/newworkflow/logs/translation_sync.log', 'a', 'utf-8')
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def ensure_directories(self):
        """ç¡®ä¿ç›®å½•å­˜åœ¨"""
        self.source_dir.mkdir(parents=True, exist_ok=True)
        self.target_dir.mkdir(parents=True, exist_ok=True)
        (self.target_dir / "logs").mkdir(parents=True, exist_ok=True)
    
    def get_json_files(self, directory: Path) -> Dict[str, Path]:
        """è·å–ç›®å½•ä¸­æ‰€æœ‰JSONæ–‡ä»¶ï¼Œè¿”å›æ–‡ä»¶ååˆ°è·¯å¾„çš„æ˜ å°„"""
        json_files = {}
        
        if not directory.exists():
            return json_files
        
        # å¤„ç†data/workflowç›®å½• - éœ€è¦éå†å­ç›®å½•
        if directory.name == "workflow":
            self.logger.info(f"æ‰«æ {directory} ç›®å½•çš„å­æ–‡ä»¶å¤¹...")
            for subfolder in directory.iterdir():
                if subfolder.is_dir():
                    self.logger.info(f"  æ£€æŸ¥å­æ–‡ä»¶å¤¹: {subfolder.name}")
                    for json_file in subfolder.glob("*.json"):
                        json_files[json_file.name] = json_file
                        self.logger.debug(f"    æ‰¾åˆ°æ–‡ä»¶: {json_file.name}")
        else:
            # å¤„ç†data/newworkflowç›®å½• - ç›´æ¥æ‰«æJSONæ–‡ä»¶
            self.logger.info(f"æ‰«æ {directory} ç›®å½•...")
            for json_file in directory.glob("*.json"):
                json_files[json_file.name] = json_file
                self.logger.debug(f"  æ‰¾åˆ°æ–‡ä»¶: {json_file.name}")
        
        return json_files
    
    def load_json_file(self, file_path: Path) -> Dict:
        """åŠ è½½JSONæ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            self.logger.error(f"åŠ è½½æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
            return {}
    
    def save_json_file(self, file_path: Path, data: Dict) -> bool:
        """ä¿å­˜JSONæ–‡ä»¶"""
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            return True
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
            return False
    
    def check_missing_fields(self, target_data: Dict) -> List[str]:
        """æ£€æŸ¥ç›®æ ‡æ–‡ä»¶ä¸­ç¼ºå¤±çš„ç¿»è¯‘å­—æ®µ"""
        missing_fields = []
        for field in self.translation_fields:
            if field not in target_data or not target_data[field]:
                missing_fields.append(field)
        return missing_fields
    
    def get_available_fields(self, source_data: Dict) -> List[str]:
        """è·å–æºæ–‡ä»¶ä¸­å¯ç”¨çš„ç¿»è¯‘å­—æ®µ"""
        available_fields = []
        for field in self.translation_fields:
            if field in source_data and source_data[field]:
                available_fields.append(field)
        return available_fields
    
    def sync_translation_fields(self, source_file: Path, target_file: Path) -> Tuple[bool, List[str]]:
        """åŒæ­¥ç¿»è¯‘å­—æ®µä»æºæ–‡ä»¶åˆ°ç›®æ ‡æ–‡ä»¶"""
        # åŠ è½½æºæ–‡ä»¶å’Œç›®æ ‡æ–‡ä»¶
        source_data = self.load_json_file(source_file)
        target_data = self.load_json_file(target_file)
        
        if not source_data or not target_data:
            return False, []
        
        # æ£€æŸ¥ç›®æ ‡æ–‡ä»¶ç¼ºå¤±çš„å­—æ®µ
        missing_fields = self.check_missing_fields(target_data)
        if not missing_fields:
            self.logger.debug(f"ç›®æ ‡æ–‡ä»¶ {target_file.name} å·²åŒ…å«æ‰€æœ‰ç¿»è¯‘å­—æ®µï¼Œè·³è¿‡")
            return True, []
        
        # æ£€æŸ¥æºæ–‡ä»¶ä¸­å¯ç”¨çš„å­—æ®µ
        available_fields = self.get_available_fields(source_data)
        if not available_fields:
            self.logger.debug(f"æºæ–‡ä»¶ {source_file.name} ä¸åŒ…å«ä»»ä½•ç¿»è¯‘å­—æ®µï¼Œè·³è¿‡")
            return True, []
        
        # æ‰¾å‡ºéœ€è¦åŒæ­¥çš„å­—æ®µï¼ˆæ—¢ç¼ºå¤±åˆå¯ç”¨çš„å­—æ®µï¼‰
        fields_to_sync = [field for field in missing_fields if field in available_fields]
        
        if not fields_to_sync:
            self.logger.debug(f"æ–‡ä»¶ {target_file.name} ä¸ {source_file.name} æ— å¯åŒæ­¥å­—æ®µ")
            return True, []
        
        # åŒæ­¥å­—æ®µ
        updated = False
        synced_fields = []
        
        for field in fields_to_sync:
            target_data[field] = source_data[field]
            synced_fields.append(field)
            updated = True
            self.logger.info(f"   ğŸ“‹ åŒæ­¥å­—æ®µ '{field}' åˆ°ç›®æ ‡æ–‡ä»¶")
        
        # ä¿å­˜æ›´æ–°åçš„ç›®æ ‡æ–‡ä»¶
        if updated:
            if self.save_json_file(target_file, target_data):
                self.logger.info(f"   ğŸ’¾ æˆåŠŸä¿å­˜åˆ° {target_file.name}ï¼Œæ›´æ–°äº† {len(synced_fields)} ä¸ªå­—æ®µ")
                return True, synced_fields
            else:
                self.logger.error(f"   âŒ ä¿å­˜ç›®æ ‡æ–‡ä»¶ {target_file.name} å¤±è´¥")
                return False, []
        
        return True, []
    
    def sync_all_files(self):
        """åŒæ­¥æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶"""
        self.ensure_directories()
        
        # è·å–ä¸¤ä¸ªç›®å½•ä¸­çš„æ‰€æœ‰JSONæ–‡ä»¶
        self.logger.info("å¼€å§‹æ‰«ææ–‡ä»¶...")
        source_files = self.get_json_files(self.source_dir)
        target_files = self.get_json_files(self.target_dir)
        
        self.logger.info(f"æºç›®å½• (data/workflow) åŒ…å« {len(source_files)} ä¸ªJSONæ–‡ä»¶")
        self.logger.info(f"ç›®æ ‡ç›®å½• (data/newworkflow) åŒ…å« {len(target_files)} ä¸ªJSONæ–‡ä»¶")
        
        # æ‰¾å‡ºåŒåçš„æ–‡ä»¶
        common_files = set(source_files.keys()) & set(target_files.keys())
        
        if not common_files:
            self.logger.info("æ²¡æœ‰æ‰¾åˆ°åŒåçš„JSONæ–‡ä»¶")
            return
        
        self.logger.info(f"æ‰¾åˆ° {len(common_files)} ä¸ªåŒåæ–‡ä»¶éœ€è¦å¤„ç†")
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_processed = 0
        total_updated = 0
        total_fields_synced = 0
        
        # å¤„ç†æ¯ä¸ªåŒåæ–‡ä»¶
        for filename in sorted(common_files):
            source_file = source_files[filename]  # data/workflow/å­ç›®å½•/æ–‡ä»¶.json
            target_file = target_files[filename]  # data/newworkflow/æ–‡ä»¶.json
            
            self.logger.info(f"\n[{total_processed + 1}/{len(common_files)}] å¤„ç†æ–‡ä»¶: {filename}")
            
            # ä¿®å¤è·¯å¾„æ˜¾ç¤ºé—®é¢˜ - æ˜ç¡®æ˜¾ç¤ºä¸åŒç›®å½•
            try:
                source_path_str = str(source_file.relative_to(Path.cwd()))
            except ValueError:
                source_path_str = str(source_file)
            
            try:
                target_path_str = str(target_file.relative_to(Path.cwd()))
            except ValueError:
                target_path_str = str(target_file)
            
            # éªŒè¯æ–‡ä»¶ç¡®å®æ¥è‡ªä¸åŒç›®å½•
            if source_file.parent.name == target_file.parent.name:
                self.logger.warning(f"âš ï¸  æºæ–‡ä»¶å’Œç›®æ ‡æ–‡ä»¶åœ¨åŒä¸€ç›®å½•! è·³è¿‡å¤„ç†")
                self.logger.warning(f"   æº: {source_path_str}")
                self.logger.warning(f"   ç›®æ ‡: {target_path_str}")
                continue
            
            self.logger.info(f"  ğŸ“‚ æºæ–‡ä»¶ (æœ‰ç¿»è¯‘): {source_path_str}")
            self.logger.info(f"  ğŸ“ ç›®æ ‡æ–‡ä»¶ (å¾…è¡¥å……): {target_path_str}")
            
            success, synced_fields = self.sync_translation_fields(source_file, target_file)
            total_processed += 1
            
            if success and synced_fields:
                total_updated += 1
                total_fields_synced += len(synced_fields)
                self.logger.info(f"âœ… æˆåŠŸåŒæ­¥ {len(synced_fields)} ä¸ªå­—æ®µ: {', '.join(synced_fields)}")
                self.logger.info(f"   æ›´æ–°çš„æ–‡ä»¶: {target_path_str}")
            elif success:
                self.logger.info("â„¹ï¸  æ— éœ€æ›´æ–° (ç›®æ ‡æ–‡ä»¶å·²æœ‰æ‰€æœ‰ç¿»è¯‘æˆ–æºæ–‡ä»¶æ— ç¿»è¯‘)")
            else:
                self.logger.error("âŒ å¤„ç†å¤±è´¥")
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        self.logger.info(f"\n=== åŒæ­¥å®Œæˆ ===")
        self.logger.info(f"æ€»å…±å¤„ç†: {total_processed} ä¸ªæ–‡ä»¶")
        self.logger.info(f"æˆåŠŸæ›´æ–°: {total_updated} ä¸ªæ–‡ä»¶")
        self.logger.info(f"åŒæ­¥å­—æ®µ: {total_fields_synced} ä¸ª")
        self.logger.info(f"æ‰€æœ‰æ›´æ–°éƒ½ä¿å­˜åœ¨: data/newworkflow/ ç›®å½•ä¸­")
        
        # å¤„ç†ç›®æ ‡ç›®å½•ä¸­ç‹¬æœ‰çš„æ–‡ä»¶
        target_only_files = set(target_files.keys()) - set(source_files.keys())
        if target_only_files:
            self.logger.info(f"\nğŸ“‹ data/newworkflow ç‹¬æœ‰æ–‡ä»¶ ({len(target_only_files)} ä¸ª):")
            self.logger.info("   è¿™äº›æ–‡ä»¶åœ¨ data/workflow ä¸­æ²¡æœ‰å¯¹åº”çš„ç¿»è¯‘æºæ–‡ä»¶")
            for filename in sorted(list(target_only_files)[:10]):
                self.logger.info(f"     - {filename}")
            if len(target_only_files) > 10:
                self.logger.info(f"     ... è¿˜æœ‰ {len(target_only_files) - 10} ä¸ªæ–‡ä»¶")
        
        # å¤„ç†æºç›®å½•ä¸­ç‹¬æœ‰çš„æ–‡ä»¶
        source_only_files = set(source_files.keys()) - set(target_files.keys())
        if source_only_files:
            self.logger.info(f"\nğŸ“š data/workflow ç‹¬æœ‰æ–‡ä»¶ ({len(source_only_files)} ä¸ª):")
            self.logger.info("   è¿™äº›ç¿»è¯‘æ–‡ä»¶åœ¨ data/newworkflow ä¸­æ²¡æœ‰å¯¹åº”çš„ç›®æ ‡æ–‡ä»¶")
            for filename in sorted(list(source_only_files)[:10]):
                self.logger.info(f"     - {filename}")
            if len(source_only_files) > 10:
                self.logger.info(f"     ... è¿˜æœ‰ {len(source_only_files) - 10} ä¸ªæ–‡ä»¶")
    
    def check_file_status(self, filename: str = None):
        """æ£€æŸ¥æ–‡ä»¶çŠ¶æ€ï¼ˆç”¨äºè°ƒè¯•ï¼‰"""
        source_files = self.get_json_files(self.source_dir)
        target_files = self.get_json_files(self.target_dir)
        
        if filename:
            # æ£€æŸ¥ç‰¹å®šæ–‡ä»¶
            if filename in source_files and filename in target_files:
                source_data = self.load_json_file(source_files[filename])
                target_data = self.load_json_file(target_files[filename])
                
                print(f"\nğŸ“„ æ–‡ä»¶åˆ†æ: {filename}")
                
                # ä¿®å¤è·¯å¾„æ˜¾ç¤ºé—®é¢˜
                try:
                    source_path_str = str(source_files[filename].relative_to(Path.cwd()))
                except ValueError:
                    source_path_str = str(source_files[filename])
                
                try:
                    target_path_str = str(target_files[filename].relative_to(Path.cwd()))
                except ValueError:
                    target_path_str = str(target_files[filename])
                
                print(f"ğŸ“š æºæ–‡ä»¶è·¯å¾„ (æœ‰ç¿»è¯‘): {source_path_str}")
                print(f"ğŸ“ ç›®æ ‡æ–‡ä»¶è·¯å¾„ (å¾…è¡¥å……): {target_path_str}")
                
                # éªŒè¯æ˜¯å¦æ¥è‡ªä¸åŒç›®å½•
                if source_files[filename].parent.name == target_files[filename].parent.name:
                    print("âš ï¸  è­¦å‘Š: æºæ–‡ä»¶å’Œç›®æ ‡æ–‡ä»¶åœ¨åŒä¸€ç›®å½•!")
                
                print("\nğŸ“š æºæ–‡ä»¶ç¿»è¯‘å­—æ®µ (data/workflow):")
                for field in self.translation_fields:
                    status = "âœ… æœ‰å†…å®¹" if field in source_data and source_data[field] else "âŒ æ— å†…å®¹"
                    value_preview = ""
                    if field in source_data and source_data[field]:
                        preview = str(source_data[field])[:50]
                        if len(str(source_data[field])) > 50:
                            preview += "..."
                        value_preview = f" ({preview})"
                    print(f"  {status} {field}{value_preview}")
                
                print("\nğŸ“ ç›®æ ‡æ–‡ä»¶ç¿»è¯‘å­—æ®µ (data/newworkflow):")
                for field in self.translation_fields:
                    status = "âœ… å·²æœ‰" if field in target_data and target_data[field] else "âŒ ç¼ºå¤±"
                    value_preview = ""
                    if field in target_data and target_data[field]:
                        preview = str(target_data[field])[:50]
                        if len(str(target_data[field])) > 50:
                            preview += "..."
                        value_preview = f" ({preview})"
                    print(f"  {status} {field}{value_preview}")
                
                # åˆ†æå¯åŒæ­¥çš„å­—æ®µ
                missing_fields = self.check_missing_fields(target_data)
                available_fields = self.get_available_fields(source_data)
                syncable_fields = [field for field in missing_fields if field in available_fields]
                
                print(f"\nğŸ”„ å¯åŒæ­¥å­—æ®µ: {', '.join(syncable_fields) if syncable_fields else 'æ— '}")
                if syncable_fields:
                    print(f"   è¿™äº›å­—æ®µå°†ä»æºæ–‡ä»¶å¤åˆ¶åˆ°ç›®æ ‡æ–‡ä»¶")
                
            else:
                if filename not in source_files and filename not in target_files:
                    print(f"âŒ æ–‡ä»¶ {filename} åœ¨ä¸¤ä¸ªç›®å½•ä¸­éƒ½ä¸å­˜åœ¨")
                elif filename not in source_files:
                    print(f"âŒ æ–‡ä»¶ {filename} åœ¨æºç›®å½• (data/workflow) ä¸­ä¸å­˜åœ¨")
                else:
                    print(f"âŒ æ–‡ä»¶ {filename} åœ¨ç›®æ ‡ç›®å½• (data/newworkflow) ä¸­ä¸å­˜åœ¨")
        else:
            # æ£€æŸ¥æ‰€æœ‰åŒåæ–‡ä»¶çš„çŠ¶æ€
            common_files = set(source_files.keys()) & set(target_files.keys())
            
            print(f"\nğŸ“ ç›®å½•ç»“æ„è¯´æ˜:")
            print(f"  ğŸ“š æºç›®å½•: {self.source_dir} (åµŒå¥—å­ç›®å½•ï¼ŒåŒ…å«ç¿»è¯‘å†…å®¹)")
            print(f"  ğŸ“ ç›®æ ‡ç›®å½•: {self.target_dir} (ç›´æ¥æ–‡ä»¶ï¼Œéœ€è¦è¡¥å……ç¿»è¯‘)")
            
            print(f"\nğŸ“Š åŒåæ–‡ä»¶çŠ¶æ€æ±‡æ€» (å…± {len(common_files)} ä¸ª):")
            
            # ç»Ÿè®¡ä¿¡æ¯
            files_needing_sync = 0
            total_syncable_fields = 0
            
            display_count = min(15, len(common_files))
            for filename in sorted(list(common_files)[:display_count]):
                source_data = self.load_json_file(source_files[filename])
                target_data = self.load_json_file(target_files[filename])
                
                missing_count = len(self.check_missing_fields(target_data))
                available_count = len(self.get_available_fields(source_data))
                syncable_count = len([field for field in self.check_missing_fields(target_data) 
                                    if field in self.get_available_fields(source_data)])
                
                if syncable_count > 0:
                    files_needing_sync += 1
                    total_syncable_fields += syncable_count
                
                status_icon = "ğŸ”„ éœ€åŒæ­¥" if syncable_count > 0 else "âœ… å®Œæ•´" if missing_count == 0 else "â„¹ï¸  æ— æº"
                print(f"  {status_icon} {filename}: ç›®æ ‡ç¼ºå¤± {missing_count}/4, æºå¯ç”¨ {available_count}/4, å¯åŒæ­¥ {syncable_count}")
            
            if len(common_files) > display_count:
                print(f"  ... è¿˜æœ‰ {len(common_files) - display_count} ä¸ªæ–‡ä»¶")
            
            print(f"\nğŸ“ˆ ç»Ÿè®¡æ±‡æ€»:")
            print(f"  ğŸ”„ éœ€è¦åŒæ­¥çš„æ–‡ä»¶: {files_needing_sync} ä¸ª")
            print(f"  ğŸ“‹ å¯åŒæ­¥çš„å­—æ®µ: {total_syncable_fields} ä¸ª")
            print(f"  ğŸ’¾ æ‰€æœ‰æ›´æ–°å°†ä¿å­˜åœ¨ data/newworkflow ç›®å½•ä¸­")
def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ç¿»è¯‘å­—æ®µåŒæ­¥å·¥å…·')
    parser.add_argument('--check', type=str, nargs='?', const='all',
                       help='æ£€æŸ¥æ–‡ä»¶çŠ¶æ€ï¼Œå¯æŒ‡å®šæ–‡ä»¶åæˆ–ä½¿ç”¨ "all" æ£€æŸ¥æ‰€æœ‰')
    parser.add_argument('--sync', action='store_true',
                       help='æ‰§è¡ŒåŒæ­¥æ“ä½œ')
    
    args = parser.parse_args()
    
    syncer = TranslationSyncer()
    
    if args.check is not None:
        if args.check == 'all':
            syncer.check_file_status()
        else:
            syncer.check_file_status(args.check)
    elif args.sync:
        syncer.sync_all_files()
    else:
        print("è¯·æŒ‡å®šæ“ä½œ:")
        print("  --sync     æ‰§è¡ŒåŒæ­¥æ“ä½œ")
        print("  --check    æ£€æŸ¥æ‰€æœ‰æ–‡ä»¶çŠ¶æ€")
        print("  --check filename.json  æ£€æŸ¥ç‰¹å®šæ–‡ä»¶çŠ¶æ€")

if __name__ == "__main__":
    main()