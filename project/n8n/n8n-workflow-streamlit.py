#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
è¯¥è„šæœ¬åˆ›å»ºä¸€ä¸ªStreamlité¡µé¢ï¼Œç”¨äºå±•ç¤ºdata/workflowç›®å½•ä¸‹çš„æ‰€æœ‰JSONæ–‡ä»¶å†…å®¹ã€‚
åŠŸèƒ½åŒ…æ‹¬ï¼šæ–‡ä»¶æœç´¢ã€åˆ†ç±»è¿‡æ»¤ã€é¢„è§ˆå±•ç¤ºã€åˆ†é¡µæµè§ˆã€æ–‡ä»¶å†…å®¹ä¸‹è½½å’Œç¿»è¯‘ã€‚
"""

import os
import json
import streamlit as st
from pathlib import Path
import math
import base64
import re
from datetime import datetime
import time
import requests
import n8n_json_to_context
from dotenv import load_dotenv
import logging
import sys

# è®¾ç½®é¡µé¢æ ‡é¢˜å’Œé…ç½®
st.set_page_config(page_title="N8Nå·¥ä½œæµæŸ¥çœ‹å™¨", page_icon="ğŸ”„", layout="wide", initial_sidebar_state="expanded")
st.title("N8Nå·¥ä½œæµæ•°æ®æŸ¥çœ‹")

# å®šä¹‰æ•°æ®ç›®å½•è·¯å¾„
current_dir = Path(__file__).parent
data_dir = current_dir / "data" / "workflow"

# ç¡®ä¿logsç›®å½•å­˜åœ¨
logs_dir = current_dir / 'logs'
if not logs_dir.exists():
    logs_dir.mkdir(parents=True, exist_ok=True)

# é…ç½®æ—¥å¿— - åŒæ—¶è¾“å‡ºåˆ°æ–‡ä»¶å’Œæ§åˆ¶å°
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# æ–‡ä»¶å¤„ç†å™¨ - è¾“å‡ºåˆ°æ—¥å¿—æ–‡ä»¶
file_handler = logging.FileHandler(logs_dir / 'workflow_streamlit.log')
file_handler.setLevel(logging.INFO)
file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
file_handler.setFormatter(file_formatter)

# æ§åˆ¶å°å¤„ç†å™¨ - è¾“å‡ºåˆ°æ§åˆ¶å°
console_handler = logging.StreamHandler(sys.stdout)
console_handler.setLevel(logging.INFO)
console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
console_handler.setFormatter(console_formatter)

# æ·»åŠ å¤„ç†å™¨åˆ°logger
logger.addHandler(file_handler)
logger.addHandler(console_handler)

# è®¾ç½®ä¼šè¯çŠ¶æ€å˜é‡
if 'page_number' not in st.session_state:
    st.session_state.page_number = 1

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

def extract_json_info(json_path):
    """ä»JSONæ–‡ä»¶ä¸­æå–å…³é”®ä¿¡æ¯"""
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        info = {
            "filename": json_path.name,
            "path": json_path,
            "title": data.get("title", ""),
            "title_zh": data.get("title_zh", ""),
            "tags": data.get("tags", []), 
            "category": data.get("category", "æœªåˆ†ç±»"),
            "category_url": data.get("category_url", ""),
            "publish_date": data.get("publish_date", ""),
            "has_workflow_json": "workflow_json" in data and bool(data["workflow_json"]),
            "has_workflow_json_zh": "workflow_json_zh" in data and bool(data["workflow_json_zh"]),
            "has_readme": "readme" in data and bool(data["readme"]),
            "has_readme_zh": "readme_zh" in data and bool(data["readme_zh"])
        }
        return info
    except Exception as e:
        st.error(f"æå–æ–‡ä»¶ä¿¡æ¯å‡ºé”™ {json_path.name}: {e}")
        return None

def get_all_json_files(directory, query="", category_filter="å…¨éƒ¨", has_translation=None):
    """è·å–ç›®å½•åŠå…¶å­ç›®å½•ä¸‹çš„æ‰€æœ‰JSONæ–‡ä»¶è·¯å¾„"""
    all_files = []
    all_info = []
    
    if not directory.exists():
        st.error(f"ç›®å½•ä¸å­˜åœ¨: {directory}")
        return all_files, all_info
    
    for path in directory.glob('**/*.json'):
        if path.is_file() and path.name != "n8n_categories.json":
            file_info = extract_json_info(path)
            if file_info:
                if query:
                    title = file_info["title"] + " " + file_info["title_zh"]
                    if not (query.lower() in path.name.lower() or query.lower() in title.lower()):
                        continue
                
                if category_filter != "å…¨éƒ¨" and file_info.get("category", "æœªåˆ†ç±»") != category_filter:
                    continue
                
                if has_translation is not None:
                    has_title_zh = file_info.get("title_zh", "") != ""
                    has_readme_zh = file_info.get("has_readme_zh", False)
                    has_workflow_zh = file_info.get("has_workflow_json_zh", False)
                    
                    fully_translated = has_title_zh and has_readme_zh and has_workflow_zh
                    partially_translated = (has_title_zh or has_readme_zh or has_workflow_zh) and not fully_translated
                    not_translated = not (has_title_zh or has_readme_zh or has_workflow_zh)
                    
                    if has_translation == True and not fully_translated:
                        continue
                    elif has_translation == False and not not_translated:
                        continue
                    elif has_translation == "partial" and not partially_translated:
                        continue
                
                all_files.append(path)
                all_info.append(file_info)
    
    if all_info:
        def get_date_sort_key(info):
            date_str = info.get("publish_date_absolute", "")
            if not date_str:
                date_str = info.get("publish_date", "")
            
            try:
                if "ago" in date_str.lower():
                    if "hour" in date_str.lower(): return datetime.now().timestamp() - 3600
                    elif "day" in date_str.lower(): return datetime.now().timestamp() - 86400
                    elif "week" in date_str.lower(): return datetime.now().timestamp() - 604800
                    elif "month" in date_str.lower(): return datetime.now().timestamp() - 2592000
                    else: return 0
                
                for fmt in ["%Y-%m-%dT%H:%M:%S.%fZ", "%Y-%m-%d %H:%M:%S", "%Y-%m-%d", "%B %d, %Y"]:
                    try:
                        dt = datetime.strptime(date_str, fmt)
                        return dt.timestamp()
                    except ValueError:
                        continue
                return 0
            except:
                return 0
        
        sorted_pairs = sorted(zip(all_info, all_files), key=lambda x: get_date_sort_key(x[0]), reverse=True)
        all_info, all_files = zip(*sorted_pairs) if sorted_pairs else ([], [])
        all_info = list(all_info)
        all_files = list(all_files)
    
    return all_files, all_info

def parse_relative_date(relative_date_str):
    """
    ä»ç›¸å¯¹æ—¥æœŸæè¿°ï¼ˆå¦‚'Last update 4 days ago'ï¼‰æ¨ç®—å‡ºç»å¯¹æ—¥æœŸ
    
    Args:
        relative_date_str (str): ç›¸å¯¹æ—¥æœŸæè¿°å­—ç¬¦ä¸²
    
    Returns:
        str: YYYY-MM-DDæ ¼å¼çš„ç»å¯¹æ—¥æœŸï¼Œå¦‚æœæ— æ³•è§£æåˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
    """
    today = datetime.now()
    
    # å¤„ç†"days ago"æ ¼å¼
    days_pattern = re.compile(r"(\d+)\s*days?\s*ago", re.IGNORECASE)
    days_match = days_pattern.search(relative_date_str)
    if days_match:
        days = int(days_match.group(1))
        result_date = today - datetime.timedelta(days=days)
        return result_date.strftime("%Y-%m-%d")
    
    # å¤„ç†"weeks ago"æ ¼å¼
    weeks_pattern = re.compile(r"(\d+)\s*weeks?\s*ago", re.IGNORECASE)
    weeks_match = weeks_pattern.search(relative_date_str)
    if weeks_match:
        weeks = int(weeks_match.group(1))
        result_date = today - datetime.timedelta(weeks=weeks)
        return result_date.strftime("%Y-%m-%d")
    
    # å¤„ç†"months ago"æ ¼å¼
    months_pattern = re.compile(r"(\d+)\s*months?\s*ago", re.IGNORECASE)
    months_match = months_pattern.search(relative_date_str)
    if months_match:
        months = int(months_match.group(1))
        # ç®€å•å¤„ç†æœˆä»½å‡æ³•ï¼ˆä¸è€ƒè™‘ä¸åŒæœˆä»½å¤©æ•°ï¼‰
        year = today.year
        month = today.month - months
        while month <= 0:
            year -= 1
            month += 12
        result_date = today.replace(year=year, month=month)
        return result_date.strftime("%Y-%m-%d")
    
    # å¤„ç†"years ago"æ ¼å¼
    years_pattern = re.compile(r"(\d+)\s*years?\s*ago", re.IGNORECASE)
    years_match = years_pattern.search(relative_date_str)
    if years_match:
        years = int(years_match.group(1))
        result_date = today.replace(year=today.year - years)
        return result_date.strftime("%Y-%m-%d")
    
    # å¤„ç†"yesterday"
    if re.search(r"yesterday", relative_date_str, re.IGNORECASE):
        result_date = today - datetime.timedelta(days=1)
        return result_date.strftime("%Y-%m-%d")
    
    # å¤„ç†"last week"
    if re.search(r"last\s*week", relative_date_str, re.IGNORECASE):
        result_date = today - datetime.timedelta(weeks=1)
        return result_date.strftime("%Y-%m-%d")
    
    # å¤„ç†"last month"
    if re.search(r"last\s*month", relative_date_str, re.IGNORECASE):
        year = today.year
        month = today.month - 1
        if month <= 0:
            year -= 1
            month += 12
        result_date = today.replace(year=year, month=month)
        return result_date.strftime("%Y-%m-%d")
    
    # å¤„ç†"today"
    if re.search(r"today", relative_date_str, re.IGNORECASE):
        return today.strftime("%Y-%m-%d")
    
    # å°è¯•è§£æå·²æœ‰çš„æ—¥æœŸæ ¼å¼ï¼ˆå¦‚æœç›¸å¯¹æ—¥æœŸä¸­å·²ç»åŒ…å«ç¡®åˆ‡æ—¥æœŸï¼‰
    date_patterns = [
        # ç¾å¼æ—¥æœŸ: Month DD, YYYY
        (r"(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\s+(\d{1,2}),?\s+(\d{4})", "%b %d %Y"),
        # æ¬§å¼æ—¥æœŸ: DD Month YYYY
        (r"(\d{1,2})\s+(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\s+(\d{4})", "%d %b %Y"),
        # ISOæ ¼å¼: YYYY-MM-DD
        (r"(\d{4})-(\d{2})-(\d{2})", "%Y-%m-%d"),
        # å…¶ä»–å¸¸è§æ ¼å¼: DD/MM/YYYY
        (r"(\d{1,2})/(\d{1,2})/(\d{4})", "%d/%m/%Y"),
        # å…¶ä»–å¸¸è§æ ¼å¼: MM/DD/YYYY
        (r"(\d{1,2})/(\d{1,2})/(\d{4})", "%m/%d/%Y")
    ]
    
    for pattern, date_format in date_patterns:
        match = re.search(pattern, relative_date_str)
        if match:
            try:
                if date_format == "%b %d %Y":
                    date_str = f"{match.group(1)} {match.group(2)} {match.group(3)}"
                elif date_format == "%d %b %Y":
                    date_str = f"{match.group(1)} {match.group(2)} {match.group(3)}"
                else:
                    date_str = match.group(0)
                
                parsed_date = datetime.strptime(date_str, date_format)
                return parsed_date.strftime("%Y-%m-%d")
            except ValueError:
                continue
    
    # å¦‚æœæ— æ³•è§£æï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
    return ""

def translate_with_openrouter(file_path, data, model_id, force_translate=False):
    """ä½¿ç”¨OpenRouter APIè¿›è¡Œç¿»è¯‘"""
    api_key = os.environ.get("OPENROUTER_API_KEY")
    if not api_key:
        st.error("æœªè®¾ç½®OPENROUTER_API_KEYç¯å¢ƒå˜é‡")
        return False
    
    # è®¾ç½®åº”ç”¨æ ‡è¯†ä¿¡æ¯
    app_name = "N8N Workflow Viewer"
    site_url = "https://github.com/geallenboy/ai-python"
    
    title = data.get("title", "")
    readme = data.get("readme", "")
    workflow_json_str = data.get("workflow_json", "")
    
    fields_to_translate = []
    if title and (not data.get("title_zh") or force_translate): fields_to_translate.append("title")
    if readme and (not data.get("readme_zh") or force_translate): fields_to_translate.append("readme")
    if workflow_json_str and (not data.get("workflow_json_zh") or force_translate): fields_to_translate.append("workflow_json")
    
    # å¤„ç†publish_date_absoluteå­—æ®µï¼Œæ— è®ºæ˜¯å¦å¼ºåˆ¶ç¿»è¯‘
    if "publish_date" in data and data["publish_date"]:
        if "publish_date_absolute" not in data or not data["publish_date_absolute"] or force_translate:
            logger.info(f"å¤„ç†publish_date_absoluteå­—æ®µ: {file_path}")
            
            # ä»ç›¸å¯¹æ—¥æœŸæ¨ç®—ç»å¯¹æ—¥æœŸ
            relative_date = data["publish_date"]
            absolute_date = parse_relative_date(relative_date)
            
            # æ·»åŠ publish_date_absoluteå­—æ®µ
            data["publish_date_absolute"] = absolute_date
            logger.info(f"å·²æˆåŠŸæ¨ç®—publish_date_absoluteå­—æ®µ: {relative_date} -> {absolute_date}")
    
    if not fields_to_translate:
        if force_translate:
            st.info(f"æ–‡ä»¶ {file_path.name} æ²¡æœ‰å¯ç¿»è¯‘å†…å®¹")
        else:
            st.info(f"æ–‡ä»¶ {file_path.name} æ‰€æœ‰å­—æ®µå·²ç¿»è¯‘")
        return True
    
    st.info(f"éœ€è¦ç¿»è¯‘çš„å­—æ®µ: {', '.join(fields_to_translate)}")
    
    for field in fields_to_translate:
        content = data.get(field, "")
        if not content: continue
            
        if field == "title":
            prompt = f"å°†ä»¥ä¸‹n8nå·¥ä½œæµæ ‡é¢˜ç¿»è¯‘æˆä¸­æ–‡ã€‚åªè¿”å›ç¿»è¯‘åçš„æ–‡æœ¬ï¼Œä¸è¦åŒ…å«åŸæ–‡æˆ–å…¶ä»–è§£é‡Šï¼š\n\n{content}"
        elif field == "readme":
            prompt = f"å°†ä»¥ä¸‹n8nå·¥ä½œæµæè¿°ç¿»è¯‘æˆä¸­æ–‡ã€‚ä¿æŒåŸæ–‡çš„markdownæ ¼å¼ã€‚åªè¿”å›ç¿»è¯‘åçš„æ–‡æœ¬ï¼Œä¸è¦åŒ…å«åŸæ–‡æˆ–å…¶ä»–è§£é‡Šï¼š\n\n{content}"
        elif field == "workflow_json":
            try:
                workflow_data = json.loads(content) if isinstance(content, str) else content
                translate_fields = {
                    "name": workflow_data.get("name", ""),
                    "description": workflow_data.get("description", ""),
                    "nodes": [{"name": node.get("name", ""), "description": node.get("description", "")} 
                             for node in workflow_data.get("nodes", [])]
                }
                prompt = f"""å°†ä»¥ä¸‹n8nå·¥ä½œæµçš„æ–‡æœ¬å­—æ®µç¿»è¯‘æˆä¸­æ–‡ã€‚åªç¿»è¯‘JSONä¸­çš„æ–‡æœ¬å€¼ï¼Œä¿æŒåŸå§‹JSONç»“æ„ä¸å˜ã€‚
                è¿”å›æ ¼å¼å¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONï¼š
                
                {json.dumps(translate_fields, ensure_ascii=False, indent=2)}"""
            except Exception as e:
                st.error(f"è§£æworkflow_jsonæ—¶å‡ºé”™: {str(e)}")
                continue
        
        try:
            st.info(f"æ­£åœ¨ä½¿ç”¨OpenRouter ({model_id}) ç¿»è¯‘ {field}...")
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json",
                "HTTP-Referer": site_url,
                "X-Title": app_name
            }
            
            payload = {
                "model": model_id,
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šç¿»è¯‘ï¼Œè¯·å°†æä¾›çš„å†…å®¹ç²¾ç¡®ç¿»è¯‘æˆä¸­æ–‡ã€‚ä¿æŒåŸæ–‡çš„æ ¼å¼å’Œä¿¡æ¯ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": 4000 if field == "workflow_json" else 2000
            }
            
            response = requests.post("https://openrouter.ai/api/v1/chat/completions", headers=headers, json=payload)
            
            if response.status_code != 200:
                st.error(f"OpenRouter APIè¯·æ±‚å¤±è´¥: {response.status_code}, {response.text}")
                
                # å°è¯•ä½¿ç”¨DeepSeekä½œä¸ºå¤‡é€‰
                if os.environ.get("DEEPSEEK_API_KEY"):
                    st.warning("å°è¯•ä½¿ç”¨DeepSeek APIä½œä¸ºå¤‡é€‰ç¿»è¯‘å¼•æ“...")
                    translated_text = translate_with_deepseek(content, field)
                    if translated_text:
                        if field == "workflow_json":
                            try:
                                # å¤„ç†workflow_jsonçš„ç‰¹æ®Šæƒ…å†µ
                                workflow_data_orig = json.loads(content) if isinstance(content, str) else content
                                translated_json_data = json.loads(translated_text)
                                
                                if "name" in translated_json_data: workflow_data_orig["name"] = translated_json_data["name"]
                                if "description" in translated_json_data: workflow_data_orig["description"] = translated_json_data["description"]
                                
                                if "nodes" in translated_json_data and "nodes" in workflow_data_orig:
                                    for i, node in enumerate(workflow_data_orig["nodes"]):
                                        if i < len(translated_json_data["nodes"]):
                                            translated_node = translated_json_data["nodes"][i]
                                            if "name" in translated_node: node["name"] = translated_node["name"]
                                            if "description" in translated_node: node["description"] = translated_node["description"]
                                
                                # å¤„ç†sticky notesèŠ‚ç‚¹
                                for node in workflow_data_orig.get("nodes", []):
                                    if node.get("type") == "n8n-nodes-base.stickyNote" and "parameters" in node and "content" in node["parameters"]:
                                        sticky_content = node["parameters"]["content"]
                                        if sticky_content and len(sticky_content) > 0:
                                            # ç¿»è¯‘ä¾¿åˆ©è´´å†…å®¹
                                            sticky_prompt = f"å°†ä»¥ä¸‹ä¾¿åˆ©è´´å†…å®¹ç¿»è¯‘æˆä¸­æ–‡ï¼Œä¿æŒæ ¼å¼ï¼š\n\n{sticky_content}"
                                            try:
                                                sticky_payload = payload.copy()
                                                sticky_payload["messages"][1]["content"] = sticky_prompt
                                                sticky_response = requests.post("https://openrouter.ai/api/v1/chat/completions", 
                                                                              headers=headers, json=sticky_payload)
                                                if sticky_response.status_code == 200:
                                                    sticky_result = sticky_response.json()
                                                    node["parameters"]["content"] = sticky_result.get("choices", [{}])[0].get("message", {}).get("content", "")
                                            except Exception as e:
                                                logger.error(f"ç¿»è¯‘ä¾¿åˆ©è´´å†…å®¹æ—¶å‡ºé”™: {str(e)}")
                            except Exception as e:
                                logger.error(f"å¤„ç†workflow_jsonçš„ç‰¹æ®Šæƒ…å†µæ—¶å‡ºé”™: {str(e)}")
                                continue
                    
                    data[f"{field}_zh"] = json.dumps(workflow_data_orig, ensure_ascii=False)
                    st.success(f"ä½¿ç”¨DeepSeekæˆåŠŸç¿»è¯‘äº† {field}")
                    continue
                else:
                    continue
                
            result = response.json()
            translated_text = result.get("choices", [{}])[0].get("message", {}).get("content", "")
            
            if field == "workflow_json":
                try:
                    # å¤„ç†å¯èƒ½çš„Markdownä»£ç å—åŒ…è£…
                    json_match = re.search(r'```json\s*([\s\S]*?)\s*```', translated_text)
                    if json_match:
                        translated_json_text = json_match.group(1)
                    else:
                        translated_json_text = translated_text
                    
                    translated_json_data = json.loads(translated_json_text)
                    
                    workflow_data_orig = json.loads(content) if isinstance(content, str) else content
                    
                    if "name" in translated_json_data: workflow_data_orig["name"] = translated_json_data["name"]
                    if "description" in translated_json_data: workflow_data_orig["description"] = translated_json_data["description"]
                    
                    if "nodes" in translated_json_data and "nodes" in workflow_data_orig:
                        for i, node in enumerate(workflow_data_orig["nodes"]):
                            if i < len(translated_json_data["nodes"]):
                                translated_node = translated_json_data["nodes"][i]
                                if "name" in translated_node: node["name"] = translated_node["name"]
                                if "description" in translated_node: node["description"] = translated_node["description"]
                    
                    # å¤„ç†sticky notesèŠ‚ç‚¹
                    for node in workflow_data_orig.get("nodes", []):
                        if node.get("type") == "n8n-nodes-base.stickyNote" and "parameters" in node and "content" in node["parameters"]:
                            sticky_content = node["parameters"]["content"]
                            if sticky_content and len(sticky_content) > 0:
                                # ç¿»è¯‘ä¾¿åˆ©è´´å†…å®¹
                                sticky_prompt = f"å°†ä»¥ä¸‹ä¾¿åˆ©è´´å†…å®¹ç¿»è¯‘æˆä¸­æ–‡ï¼Œä¿æŒæ ¼å¼ï¼š\n\n{sticky_content}"
                                try:
                                    sticky_payload = payload.copy()
                                    sticky_payload["messages"][1]["content"] = sticky_prompt
                                    sticky_response = requests.post("https://openrouter.ai/api/v1/chat/completions", 
                                                                  headers=headers, json=sticky_payload)
                                    if sticky_response.status_code == 200:
                                        sticky_result = sticky_response.json()
                                        node["parameters"]["content"] = sticky_result.get("choices", [{}])[0].get("message", {}).get("content", "")
                                except Exception as e:
                                    logger.error(f"ç¿»è¯‘ä¾¿åˆ©è´´å†…å®¹æ—¶å‡ºé”™: {str(e)}")
                    
                    data[f"{field}_zh"] = json.dumps(workflow_data_orig, ensure_ascii=False)
                except Exception as e:
                    st.error(f"å¤„ç†ç¿»è¯‘åçš„workflow_jsonæ—¶å‡ºé”™: {str(e)}")
                    continue
            else:
                data[f"{field}_zh"] = translated_text
        
        except Exception as e:
            st.error(f"ç¿»è¯‘ {field} æ—¶å‡ºé”™: {str(e)}")
            continue
        
        # å¦‚æœå­—æ®µæ˜¯publish_dateï¼ŒåŒæ—¶å¤„ç†publish_date_zh
        if field == "title" and "publish_date" in data and data["publish_date"]:
            if not data.get("publish_date_zh") or force_translate:
                try:
                    publish_date = data["publish_date"]
                    prompt = f"å°†ä»¥ä¸‹n8nå·¥ä½œæµå‘å¸ƒæ—¥æœŸç¿»è¯‘æˆä¸­æ–‡ã€‚åªè¿”å›ç¿»è¯‘åçš„æ–‡æœ¬ï¼Œä¸è¦åŒ…å«åŸæ–‡æˆ–å…¶ä»–è§£é‡Šï¼š\n\n{publish_date}"
                    
                    st.info(f"æ­£åœ¨ä½¿ç”¨OpenRouter ({model_id}) ç¿»è¯‘ publish_date...")
                    
                    payload["messages"][1]["content"] = prompt
                    response = requests.post("https://openrouter.ai/api/v1/chat/completions", headers=headers, json=payload)
                    
                    if response.status_code == 200:
                        result = response.json()
                        translated_date = result.get("choices", [{}])[0].get("message", {}).get("content", "")
                        data["publish_date_zh"] = translated_date
                        st.success(f"å·²æˆåŠŸç¿»è¯‘publish_date: {publish_date} -> {translated_date}")
                    else:
                        st.error(f"ç¿»è¯‘publish_dateå¤±è´¥: {response.status_code}, {response.text}")
                except Exception as e:
                    st.error(f"ç¿»è¯‘publish_dateæ—¶å‡ºé”™: {str(e)}")
    
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        st.success(f"æ–‡ä»¶ {file_path.name} ç¿»è¯‘å®Œæˆå¹¶ä¿å­˜")
        return True
    except Exception as e:
        st.error(f"ä¿å­˜ç¿»è¯‘åçš„æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {str(e)}")
        return False

def translate_with_deepseek(content, field_type):
    """ä½¿ç”¨DeepSeek APIè¿›è¡Œç¿»è¯‘"""
    api_key = os.environ.get("DEEPSEEK_API_KEY")
    if not api_key:
        logger.error("æœªè®¾ç½®DEEPSEEK_API_KEYç¯å¢ƒå˜é‡")
        return None
    
    try:
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }
        
        # æ ¹æ®å­—æ®µç±»å‹æ„å»ºä¸åŒçš„æç¤º
        if field_type == "title":
            system_prompt = "ä½ æ˜¯ä¸€ä½ä¸“ä¸šç¿»è¯‘ï¼Œè¯·å°†æä¾›çš„n8nå·¥ä½œæµæ ‡é¢˜ç¿»è¯‘æˆä¸­æ–‡ã€‚åªè¿”å›ç¿»è¯‘åçš„æ–‡æœ¬ï¼Œä¸è¦åŒ…å«åŸæ–‡æˆ–å…¶ä»–è§£é‡Šã€‚"
        elif field_type == "readme":
            system_prompt = "ä½ æ˜¯ä¸€ä½ä¸“ä¸šç¿»è¯‘ï¼Œè¯·å°†æä¾›çš„n8nå·¥ä½œæµæè¿°ç¿»è¯‘æˆä¸­æ–‡ã€‚ä¿æŒåŸæ–‡çš„markdownæ ¼å¼ã€‚åªè¿”å›ç¿»è¯‘åçš„æ–‡æœ¬ï¼Œä¸è¦åŒ…å«åŸæ–‡æˆ–å…¶ä»–è§£é‡Šã€‚"
        elif field_type == "workflow_json":
            system_prompt = "ä½ æ˜¯ä¸€ä½ä¸“ä¸šç¿»è¯‘ï¼Œè¯·å°†æä¾›çš„n8nå·¥ä½œæµJSONä¸­çš„æ–‡æœ¬å­—æ®µç¿»è¯‘æˆä¸­æ–‡ã€‚åªç¿»è¯‘JSONä¸­çš„æ–‡æœ¬å€¼ï¼Œä¿æŒåŸå§‹JSONç»“æ„ä¸å˜ã€‚è¿”å›æ ¼å¼å¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONã€‚"
        else:
            system_prompt = "ä½ æ˜¯ä¸€ä½ä¸“ä¸šç¿»è¯‘ï¼Œè¯·å°†æä¾›çš„å†…å®¹ç¿»è¯‘æˆä¸­æ–‡ã€‚åªè¿”å›ç¿»è¯‘åçš„æ–‡æœ¬ï¼Œä¸è¦åŒ…å«åŸæ–‡æˆ–å…¶ä»–è§£é‡Šã€‚"
        
        data = {
            "model": "deepseek-chat",
            "messages": [
                {
                    "role": "system",
                    "content": system_prompt
                },
                {
                    "role": "user",
                    "content": content
                }
            ],
            "temperature": 0.1
        }
        
        response = requests.post("https://api.deepseek.com/v1/chat/completions", headers=headers, json=data)
        response.raise_for_status()
        
        result = response.json()
        translated_text = result['choices'][0]['message']['content'].strip()
        
        # æ·»åŠ çŸ­æš‚å»¶è¿Ÿä»¥é¿å…APIé™åˆ¶
        time.sleep(0.5)
        return translated_text
    
    except Exception as e:
        logger.error(f"DeepSeekç¿»è¯‘å¤±è´¥: {e}")
        return None

def process_json_file(file_path, model="anthropic/claude-3-7-sonnet", force_translate=False):
    """
    å¤„ç†å•ä¸ªJSONæ–‡ä»¶ï¼Œç¿»è¯‘å…¶ä¸­çš„å†…å®¹
    
    Args:
        file_path: JSONæ–‡ä»¶è·¯å¾„
        model: ä½¿ç”¨çš„OpenRouteræ¨¡å‹ID
        force_translate: æ˜¯å¦å¼ºåˆ¶é‡æ–°ç¿»è¯‘å·²æœ‰å†…å®¹
        
    Returns:
        bool: æ˜¯å¦æˆåŠŸå¤„ç†
    """
    try:
        logs_dir = Path(__file__).parent / 'logs'
        logs_dir.mkdir(exist_ok=True)
        
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        title = data.get("title", "")
        readme = data.get("readme", "")
        workflow_json = data.get("workflow_json", "")
        
        # æ£€æŸ¥æ˜¯å¦å·²å…¨éƒ¨ç¿»è¯‘ï¼Œè‹¥æ˜¯ï¼Œåˆ™æ ¹æ®force_translateå†³å®šæ˜¯å¦ç»§ç»­
        if not force_translate and (title and data.get("title_zh")) and \
           (readme and data.get("readme_zh")) and \
           (workflow_json and data.get("workflow_json_zh")):
            st.info(f"æ–‡ä»¶ {file_path.name} å·²ç»ç¿»è¯‘å®Œæˆï¼Œä½¿ç”¨\"é‡æ–°ç¿»è¯‘\"æŒ‰é’®å¯è¦†ç›–ç°æœ‰ç¿»è¯‘")
            return True
        
        # ä½¿ç”¨OpenRouterè¿›è¡Œç¿»è¯‘
        success = translate_with_openrouter(file_path, data, model, force_translate)
        
        return success
        
    except Exception as e:
        st.error(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {str(e)}")
        with open(Path(__file__).parent / 'logs' / 'translation_errors.log', 'a', encoding='utf-8') as f:
            f.write(f"[{time.strftime('%Y-%m-%d %H:%M:%S')}] å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {str(e)}\n")
        return False

def get_download_link(content, filename, text):
    """ç”Ÿæˆä¸‹è½½é“¾æ¥"""
    if isinstance(content, str):
        content_bytes = content.encode('utf-8')
    else:
        content_bytes = json.dumps(content, ensure_ascii=False, indent=2).encode('utf-8')
    
    b64 = base64.b64encode(content_bytes).decode()
    href = f'<a href="data:application/json;base64,{b64}" download="{filename}" class="download-btn">{text}</a>'
    return href

def format_datetime(date_str):
    """æ ¼å¼åŒ–æ—¥æœŸæ—¶é—´å­—ç¬¦ä¸²"""
    if not date_str: return ""
    try:
        formats = ["%Y-%m-%dT%H:%M:%S.%fZ", "%Y-%m-%d %H:%M:%S", "%Y-%m-%d", "%B %d, %Y"]
        for fmt in formats:
            try:
                dt = datetime.strptime(date_str, fmt)
                return dt.strftime("%Y-%m-%d")
            except ValueError:
                continue
        return date_str
    except Exception:
        return date_str

def display_json_content(json_path, index):
    """å±•ç¤ºå•ä¸ªJSONæ–‡ä»¶çš„å†…å®¹"""
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        title_zh = data.get("title_zh", "")
        title_en = data.get("title", "")
        display_title = title_zh if title_zh else title_en if title_en else json_path.name
        
        publish_date = format_datetime(data.get("publish_date", ""))
        tags = data.get("tags", [])
        tag_display = " ".join([f"<span class='tag'>{tag}</span>" for tag in tags]) if tags else ""
        
        has_zh_title = bool("title_zh" in data and data["title_zh"])
        has_zh_readme = bool("readme_zh" in data and data["readme_zh"])
        has_zh_workflow = bool("workflow_json_zh" in data and data["workflow_json_zh"])
        
        total_items = 3
        translated_items = sum([has_zh_title, has_zh_readme, has_zh_workflow])
        translation_percent = int((translated_items / total_items) * 100)
        
        if translation_percent == 100:
            translation_badge = f"<span class='badge badge-success'>å·²ç¿»è¯‘</span>"
        elif translation_percent > 0:
            translation_badge = f"<span class='badge badge-warning'>éƒ¨åˆ†ç¿»è¯‘ {translation_percent}%</span>"
        else:
            translation_badge = f"<span class='badge badge-danger'>æœªç¿»è¯‘</span>"
        
        with st.expander(f"#{index} - {display_title} {publish_date} {translation_badge}", expanded=False):
            col1, col2, col3 = st.columns([3, 1, 1])
            with col1:
                st.caption(f"ç¿»è¯‘çŠ¶æ€: æ ‡é¢˜ {'âœ…' if has_zh_title else 'âŒ'} | æè¿° {'âœ…' if has_zh_readme else 'âŒ'} | å·¥ä½œæµ {'âœ…' if has_zh_workflow else 'âŒ'}")
            with col2:
                # ç¿»è¯‘æŒ‰é’® - æ— è®ºæ˜¯å¦å·²ç¿»è¯‘éƒ½æ˜¾ç¤º
                if st.button("ğŸŒ ç¿»è¯‘æ­¤æ–‡ä»¶", key=f"translate_{index}_{json_path.name}"):
                    success = False
                    with st.spinner("æ­£åœ¨ç¿»è¯‘..."):
                        selected_model = st.session_state.selected_model
                        model_name = st.session_state.model_options.get(selected_model, selected_model.split("/")[-1])
                        
                        st.info(f"ä½¿ç”¨ {model_name} è¿›è¡Œç¿»è¯‘...")
                        success = process_json_file(json_path, model=selected_model)
                    
                    # æ˜¾ç¤ºç¿»è¯‘ç»“æœ
                    if success:
                        st.success("âœ… ç¿»è¯‘æˆåŠŸï¼")
                        # è¯»å–æ›´æ–°åçš„æ–‡ä»¶å†…å®¹
                        with open(json_path, 'r', encoding='utf-8') as f_updated:
                            updated_data = json.load(f_updated)
                        
                        # æ›´æ–°å½“å‰æ•°æ®ï¼Œè¿™æ ·åŸºæœ¬ä¿¡æ¯æ ‡ç­¾é¡µä¼šæ˜¾ç¤ºæœ€æ–°ç¿»è¯‘å†…å®¹
                        data = updated_data
                        
                        # æ›´æ–°ç¿»è¯‘çŠ¶æ€æŒ‡æ ‡
                        has_zh_title = bool("title_zh" in data and data["title_zh"])
                        has_zh_readme = bool("readme_zh" in data and data["readme_zh"])
                        has_zh_workflow = bool("workflow_json_zh" in data and data["workflow_json_zh"])
                        translated_items = sum([has_zh_title, has_zh_readme, has_zh_workflow])
                        translation_percent = int((translated_items / total_items) * 100)
                        
                        st.info("âœ¨ ç¿»è¯‘å·²å®Œæˆï¼Œè¯·åœ¨ã€ŒåŸºæœ¬ä¿¡æ¯ã€æ ‡ç­¾é¡µæŸ¥çœ‹ä¸­æ–‡ç¿»è¯‘å†…å®¹")
                        
                        # æ·»åŠ åˆ·æ–°æŒ‰é’®ï¼Œç”¨äºå®Œå…¨åˆ·æ–°é¡µé¢ï¼ˆæ›´æ–°expanderæ ‡é¢˜ä¸­çš„ç¿»è¯‘çŠ¶æ€ç­‰ï¼‰
                        if st.button("ğŸ”„ åˆ·æ–°é¡µé¢æ›´æ–°çŠ¶æ€", key=f"refresh_{index}"):
                            st.rerun()
                    else:
                        st.error("âŒ ç¿»è¯‘å¤±è´¥ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—æˆ–æ£€æŸ¥APIå¯†é’¥è®¾ç½®")
            
            with col3:
                # é‡æ–°ç¿»è¯‘æŒ‰é’® - åªæœ‰å½“è‡³å°‘æœ‰ä¸€é¡¹å·²ç¿»è¯‘æ—¶æ‰æ˜¾ç¤º
                if translation_percent > 0:
                    if st.button("ğŸ”„ é‡æ–°ç¿»è¯‘", key=f"retranslate_{index}_{json_path.name}"):
                        with st.spinner("æ­£åœ¨é‡æ–°ç¿»è¯‘..."):
                            selected_model = st.session_state.selected_model
                            model_name = st.session_state.model_options.get(selected_model, selected_model.split("/")[-1])
                            
                            st.info(f"ä½¿ç”¨ {model_name} é‡æ–°ç¿»è¯‘...")
                            # ä½¿ç”¨force_translateå‚æ•°å¼ºåˆ¶é‡æ–°ç¿»è¯‘
                            success = process_json_file(json_path, model=selected_model, force_translate=True)
                        
                        if success:
                            st.success("âœ… é‡æ–°ç¿»è¯‘æˆåŠŸï¼")
                            # è¯»å–æ›´æ–°åçš„æ–‡ä»¶å†…å®¹
                            with open(json_path, 'r', encoding='utf-8') as f_updated:
                                updated_data = json.load(f_updated)
                            
                            # æ›´æ–°å½“å‰æ•°æ®
                            data = updated_data
                            
                            st.info("âœ¨ ç¿»è¯‘å·²æ›´æ–°ï¼Œè¯·åœ¨ã€ŒåŸºæœ¬ä¿¡æ¯ã€æ ‡ç­¾é¡µæŸ¥çœ‹ä¸­æ–‡ç¿»è¯‘å†…å®¹")
                            
                            # æ·»åŠ åˆ·æ–°æŒ‰é’®
                            if st.button("ğŸ”„ åˆ·æ–°é¡µé¢æŸ¥çœ‹æ›´æ–°", key=f"refresh_retranslate_{index}"):
                                st.rerun()
                        else:
                            st.error("âŒ é‡æ–°ç¿»è¯‘å¤±è´¥ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—æˆ–æ£€æŸ¥APIå¯†é’¥è®¾ç½®")
            
            if tag_display:
                st.markdown(f"<div class='tags-container'>{tag_display}</div>", unsafe_allow_html=True)
            st.caption(f"æ–‡ä»¶è·¯å¾„: {json_path}")
            
            # ä¿®æ”¹æ ‡ç­¾é¡µæ˜¾ç¤ºï¼Œé¿å…åµŒå¥—é—®é¢˜
            tabs = st.tabs(["åŸºæœ¬ä¿¡æ¯", "å·¥ä½œæµæ•°æ®", "å·¥ä½œæµåˆ†æ", "åŸå§‹JSON"])
            
            # Tab 1: åŸºæœ¬ä¿¡æ¯
            with tabs[0]:
                info_tabs = st.tabs(["ä¸­æ–‡ä¿¡æ¯", "è‹±æ–‡ä¿¡æ¯"])
                # ä¸­æ–‡ä¿¡æ¯æ ‡ç­¾
                with info_tabs[0]:
                    st.write("**ä¸­æ–‡æ ‡é¢˜:** ", data.get("title_zh", "(æœªç¿»è¯‘)" if data.get("title") else ""))
                    st.write("**ä¸­æ–‡æ—¥æœŸ:** ", data.get("publish_date_zh", "(æœªç¿»è¯‘)" if data.get("publish_date") else ""))
                    if tags: st.write("**æ ‡ç­¾:** ", ", ".join(tags))
                    if "readme_zh" in data and data["readme_zh"]:
                        st.markdown("### ä¸­æ–‡æè¿°")
                        st.markdown(data["readme_zh"])
                    elif "readme" in data and data["readme"]:
                        st.markdown("### ä¸­æ–‡æè¿°")
                        st.write("(å†…å®¹æœªç¿»è¯‘)")
                
                # è‹±æ–‡ä¿¡æ¯æ ‡ç­¾
                with info_tabs[1]:
                    if "title" in data and data["title"]: st.write("**æ ‡é¢˜ (Title):** ", data["title"])
                    if "publish_date" in data and data["publish_date"]: st.write("**å‘å¸ƒæ—¥æœŸ (Publish Date):** ", publish_date)
                    if tags: st.write("**æ ‡ç­¾ (Tags):** ", ", ".join(tags))
                    if "readme" in data and data["readme"]:
                        st.markdown("### æè¿° (Description)")
                        st.markdown(data["readme"])
            
            # Tab 2: å·¥ä½œæµæ•°æ®
            with tabs[1]:
                if "workflow_json" in data and data["workflow_json"]:
                    col1, col2 = st.columns([3, 1])
                    with col1: st.write("**åŸå§‹å·¥ä½œæµæ•°æ®:**")
                    with col2:
                        filename = f"{os.path.splitext(json_path.name)[0]}_workflow_json.json"
                        st.markdown(get_download_link(data["workflow_json"], filename, "ğŸ“¥ ä¸‹è½½åŸå§‹å·¥ä½œæµ"), unsafe_allow_html=True)
                    try:
                        workflow = json.loads(data["workflow_json"]) if isinstance(data["workflow_json"], str) else data["workflow_json"]
                        st.write(f"å·¥ä½œæµåç§°: {workflow.get('name', 'æœªå‘½å')}")
                        st.write(f"èŠ‚ç‚¹æ•°é‡: {len(workflow.get('nodes', []))}")
                    except: st.write("æ— æ³•è§£æå·¥ä½œæµæ•°æ®")
                
                if "workflow_json_zh" in data and data["workflow_json_zh"]:
                    st.markdown("---")
                    col1, col2 = st.columns([3, 1])
                    with col1: st.write("**ä¸­æ–‡å·¥ä½œæµæ•°æ®:**")
                    with col2:
                        filename = f"{os.path.splitext(json_path.name)[0]}_workflow_json_zh.json"
                        st.markdown(get_download_link(data["workflow_json_zh"], filename, "ğŸ“¥ ä¸‹è½½ä¸­æ–‡å·¥ä½œæµ"), unsafe_allow_html=True)
            
            # Tab 3: å·¥ä½œæµåˆ†æ
            with tabs[2]:
                if "workflow_analysis" in data and data["workflow_analysis"]:
                    st.markdown(data["workflow_analysis"])
                    st.write("---")
                    if st.button("ğŸ”„ ä½¿ç”¨å½“å‰é€‰æ‹©çš„æ¨¡å‹é‡æ–°åˆ†æ", key=f"reanalyze_{index}_{json_path.name}"):
                        selected_model = st.session_state.selected_model
                        model_name = st.session_state.model_options.get(selected_model, selected_model.split("/")[-1]).split(" (")[0]
                        with st.spinner(f"ä½¿ç”¨ {model_name} é‡æ–°åˆ†æå·¥ä½œæµ..."):
                            workflow_content = data["workflow_json_zh"] if "workflow_json_zh" in data and data["workflow_json_zh"] else data.get("workflow_json")
                            if workflow_content:
                                analysis = n8n_json_to_context.get_workflow_analysis(workflow_content, model=selected_model)
                                success = n8n_json_to_context.save_workflow_file(json_path, analysis)
                                if success:
                                    st.success("åˆ†æå®Œæˆå¹¶å·²ä¿å­˜")
                                    with open(json_path, 'r', encoding='utf-8') as f_reloaded:
                                        data = json.load(f_reloaded)
                                    st.markdown(analysis)
                                    # æ·»åŠ åˆ·æ–°æŒ‰é’®
                                    if st.button("ğŸ”„ åˆ·æ–°é¡µé¢æŸ¥çœ‹å®Œæ•´ç»“æœ", key=f"refresh_analysis_{index}"):
                                        st.rerun()
                                else: 
                                    st.error("ä¿å­˜åˆ†æç»“æœå¤±è´¥")
                            else: 
                                st.warning("æ— å·¥ä½œæµæ•°æ®å¯ä¾›åˆ†æã€‚")
                else:
                    st.info("è¯¥å·¥ä½œæµå°šæœªè¿›è¡Œåˆ†æã€‚")
                
                workflow_content_for_analysis = data.get("workflow_json_zh") or data.get("workflow_json")
                if workflow_content_for_analysis:
                    analyze_button_text = "ğŸ” ä½¿ç”¨å½“å‰é€‰æ‹©çš„æ¨¡å‹åˆ†æ" if not ("workflow_analysis" in data and data["workflow_analysis"]) else "ğŸ” ä½¿ç”¨å½“å‰æ¨¡å‹é‡æ–°åˆ†æ"
                    if st.button(analyze_button_text, key=f"analyze_direct_{index}_{json_path.name}"):
                        selected_model = st.session_state.selected_model
                        model_name = st.session_state.model_options.get(selected_model, selected_model.split("/")[-1]).split(" (")[0]
                        with st.spinner(f"ä½¿ç”¨ {model_name} åˆ†æå·¥ä½œæµ..."):
                            analysis = n8n_json_to_context.get_workflow_analysis(workflow_content_for_analysis, model=selected_model)
                            success = n8n_json_to_context.save_workflow_file(json_path, analysis)
                            if success:
                                st.success("åˆ†æå®Œæˆå¹¶å·²ä¿å­˜")
                                st.markdown(analysis)
                                # æ·»åŠ åˆ·æ–°æŒ‰é’®
                                if st.button("ğŸ”„ åˆ·æ–°é¡µé¢æŸ¥çœ‹å®Œæ•´ç»“æœ", key=f"refresh_direct_{index}"):
                                    st.rerun()
                            else: 
                                st.error("ä¿å­˜åˆ†æç»“æœå¤±è´¥")
                elif not ("workflow_analysis" in data and data["workflow_analysis"]):
                     st.warning("æ­¤æ–‡ä»¶ä¸åŒ…å«å·¥ä½œæµæ•°æ®ï¼Œæ— æ³•è¿›è¡Œåˆ†æã€‚")

            # Tab 4: åŸå§‹JSON
            with tabs[3]:
                st.json(data)
                
    except Exception as e:
        st.error(f"è¯»å–æ–‡ä»¶ {json_path.name} æ—¶å‡ºé”™: {str(e)}")
        st.exception(e)  # æ·»åŠ è¯¦ç»†é”™è¯¯ä¿¡æ¯

def load_categories():
    """ä»n8n_categories.jsonåŠ è½½åˆ†ç±»ä¿¡æ¯"""
    categories_file = Path(__file__).parent / "data" / "workflow" / "n8n_categories.json"
    if not categories_file.exists():
        st.warning("æœªæ‰¾åˆ°åˆ†ç±»æ•°æ®æ–‡ä»¶ n8n_categories.json")
        return [{"category_name": "å…¨éƒ¨", "category_url": ""}]
    try:
        with open(categories_file, 'r', encoding='utf-8') as f:
            categories_data = json.load(f)
        categories = [{"category_name": "å…¨éƒ¨", "category_url": ""}]
        for item in categories_data:
            if isinstance(item, dict) and "category_name" in item and "category_url" in item:
                categories.append(item)
        return categories
    except Exception as e:
        st.error(f"åŠ è½½åˆ†ç±»æ•°æ®å‡ºé”™: {e}")
        return [{"category_name": "å…¨éƒ¨", "category_url": ""}]

def batch_translate_files(file_paths, force_translate=False):
    """æ‰¹é‡ç¿»è¯‘é€‰å®šçš„æ–‡ä»¶"""
    if not file_paths:
        st.warning("æ²¡æœ‰é€‰æ‹©ä»»ä½•æ–‡ä»¶")
        return
    
    selected_model = st.session_state.selected_model
    model_name = st.session_state.model_options.get(selected_model, selected_model.split("/")[-1])
    
    st.info(f"ä½¿ç”¨ {model_name} æ‰¹é‡ç¿»è¯‘...")
    progress_bar = st.progress(0)
    status_text = st.empty()
    success_count = 0
    failed_files = []
    
    for i, file_path in enumerate(file_paths):
        progress = (i) / len(file_paths)
        progress_bar.progress(progress)
        status_text.text(f"æ­£åœ¨ç¿»è¯‘ ({i+1}/{len(file_paths)}): {file_path.name}")
        
        try:
            success = process_json_file(file_path, model=selected_model, force_translate=force_translate)
            if success: success_count += 1
            else: failed_files.append(file_path.name)
        except Exception as e:
            st.error(f"ç¿»è¯‘æ–‡ä»¶ {file_path.name} æ—¶å‡ºé”™: {str(e)}")
            failed_files.append(file_path.name)
        time.sleep(1) # API rate limit avoidance
    
    progress_bar.progress(1.0)
    status_text.text(f"ç¿»è¯‘å®Œæˆ! æˆåŠŸ: {success_count}/{len(file_paths)}")
    if success_count > 0: st.success(f"æˆåŠŸç¿»è¯‘ {success_count} ä¸ªæ–‡ä»¶")
    if failed_files: st.warning(f"æœ‰ {len(failed_files)} ä¸ªæ–‡ä»¶ç¿»è¯‘å¤±è´¥: {', '.join(failed_files)}")
    if success_count > 0:
        st.info("3ç§’ååˆ·æ–°é¡µé¢...")
        time.sleep(3)
        st.rerun()

def main():
    st.markdown("""
    <style>
    .download-btn { display: inline-block; padding: 0.3rem 0.6rem; background-color: #4CAF50; color: white !important; text-decoration: none; border-radius: 4px; text-align: center; transition: background-color 0.3s; }
    .download-btn:hover { background-color: #45a049; text-decoration: none; }
    .tag { display: inline-block; background-color: #f1f1f1; padding: 2px 6px; margin: 2px; border-radius: 3px; font-size: 0.8rem; }
    .tags-container { margin-bottom: 10px; }
    .category-badge { background-color: #e8f0fe; color: #1967d2; padding: 3px 8px; border-radius: 12px; font-size: 0.85rem; display: inline-block; margin-right: 5px; }
    .badge { display: inline-block; padding: 2px 8px; border-radius: 10px; font-size: 0.8rem; font-weight: bold; }
    .badge-success { background-color: #28a745; color: white; }
    .badge-warning { background-color: #ffc107; color: black; }
    .badge-danger { background-color: #dc3545; color: white; }
    </style>
    """, unsafe_allow_html=True)
    
    categories = load_categories()
    category_names = [cat["category_name"] for cat in categories]
    
    with st.sidebar:
        st.header("æœç´¢å’Œè¿‡æ»¤")
        search_query = st.text_input("æœç´¢æ–‡ä»¶åæˆ–æ ‡é¢˜", "")
        selected_category = st.selectbox("æŒ‰åˆ†ç±»è¿‡æ»¤", category_names)
        
        translation_options = ["å…¨éƒ¨", "å·²ç¿»è¯‘", "æœªç¿»è¯‘", "éƒ¨åˆ†ç¿»è¯‘"]
        translation_filter = st.radio("ç¿»è¯‘çŠ¶æ€", translation_options)
        has_translation = None
        if translation_filter == "å·²ç¿»è¯‘": has_translation = True
        elif translation_filter == "æœªç¿»è¯‘": has_translation = False
        elif translation_filter == "éƒ¨åˆ†ç¿»è¯‘": has_translation = "partial"
        
        items_per_page = st.slider("æ¯é¡µæ˜¾ç¤ºæ•°é‡", 5, 50, 10, 5)
        
        if st.button("é‡ç½®è¿‡æ»¤å™¨"):
            st.session_state.page_number = 1
            st.rerun()
            
        st.header("AI æ¨¡å‹é€‰æ‹©")
        model_list = n8n_json_to_context.get_available_models()
        
        # Default model options if API call fails or returns empty
        default_model_options_list = {
            "anthropic/claude-3-7-sonnet": "Claude 3.7 Sonnet (æ¨è)",
            "anthropic/claude-3-5-sonnet": "Claude 3.5 Sonnet",
            "anthropic/claude-3-haiku": "Claude 3 Haiku (å¿«é€Ÿ)",
            "anthropic/claude-3-opus": "Claude 3 Opus (é«˜è´¨é‡)",
            "openai/gpt-4o": "GPT-4o",
            "openai/gpt-4-turbo": "GPT-4 Turbo",
            "deepseek/deepseek-chat": "DeepSeek Chat (é€šç”¨)",
        }

        current_model_options = {model["id"]: f"{model['name']} ({model.get('context_length', 0)//1000}K)" for model in model_list} if model_list else default_model_options_list

        if 'model_options' not in st.session_state or st.session_state.model_options != current_model_options:
             st.session_state.model_options = current_model_options
        
        default_model_id = "anthropic/claude-3-7-sonnet"
        available_model_ids = list(st.session_state.model_options.keys())
        
        default_index = available_model_ids.index(default_model_id) if default_model_id in available_model_ids else 0
        
        if 'selected_model' not in st.session_state or st.session_state.selected_model not in available_model_ids:
            st.session_state.selected_model = available_model_ids[default_index]
        
        # Unified model selection for analysis and translation
        st.session_state.selected_model = st.selectbox(
            "é€‰æ‹©AIæ¨¡å‹ (ç”¨äºåˆ†æå’Œç¿»è¯‘)", 
            options=available_model_ids,
            format_func=lambda x: st.session_state.model_options[x],
            index=available_model_ids.index(st.session_state.selected_model)
        )
        st.info(f"å½“å‰é€‰æ‹©: {st.session_state.model_options[st.session_state.selected_model]}")

    json_files, _ = get_all_json_files(
        data_dir, 
        query=search_query, 
        category_filter=selected_category,
        has_translation=has_translation
    )
    
    st.write(f"å…±æ‰¾åˆ° {len(json_files)} ä¸ªæ–‡ä»¶")
    if not json_files:
        st.info("æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆè¿‡æ»¤æ¡ä»¶çš„æ–‡ä»¶ã€‚è¯·å°è¯•è°ƒæ•´è¿‡æ»¤å™¨æˆ–æœç´¢æ¡ä»¶ã€‚")
        return
    
    total_pages = math.ceil(len(json_files) / items_per_page)
    if st.session_state.page_number < 1: st.session_state.page_number = 1
    elif st.session_state.page_number > total_pages and total_pages > 0: st.session_state.page_number = total_pages
    elif total_pages == 0 : st.session_state.page_number = 1


    start_idx = (st.session_state.page_number - 1) * items_per_page
    end_idx = min(start_idx + items_per_page, len(json_files))
    
    if total_pages > 0: # Only show pagination if there are pages
        st.write(f"ç¬¬ {st.session_state.page_number} é¡µï¼Œå…± {total_pages} é¡µ")
        
        page_cols = st.columns([1, 3, 1])
        if st.session_state.page_number > 1:
            if page_cols[0].button("ä¸Šä¸€é¡µ"):
                st.session_state.page_number -= 1
                st.rerun()
        
        page_options = list(range(1, total_pages + 1)) if total_pages > 0 else [1]
        # Ensure selected_page_index is valid
        current_page_index = st.session_state.page_number - 1
        if current_page_index >= len(page_options): current_page_index = len(page_options) -1
        if current_page_index < 0: current_page_index = 0

        selected_page_from_selectbox = page_cols[1].selectbox("è·³è½¬åˆ°é¡µ", page_options, index=current_page_index, label_visibility="collapsed")
        if selected_page_from_selectbox != st.session_state.page_number:
            st.session_state.page_number = selected_page_from_selectbox
            st.rerun()
        
        if st.session_state.page_number < total_pages:
            if page_cols[2].button("ä¸‹ä¸€é¡µ"):
                st.session_state.page_number += 1
                st.rerun()
    
    for i, json_path in enumerate(json_files[start_idx:end_idx], start=start_idx+1):
        display_json_content(json_path, i)

    st.header("æ‰¹é‡ç¿»è¯‘å·¥å…·")
    openrouter_api_key = os.environ.get("OPENROUTER_API_KEY")
    deepseek_api_key = os.environ.get("DEEPSEEK_API_KEY")
    
    if not openrouter_api_key and not deepseek_api_key:
        st.warning("æœªè®¾ç½® OPENROUTER_API_KEY æˆ– DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡ã€‚æ‰¹é‡ç¿»è¯‘åŠŸèƒ½ä¸å¯ç”¨ã€‚è¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®è‡³å°‘ä¸€ä¸ªAPIå¯†é’¥ã€‚")
    else:
        api_keys_info = []
        if openrouter_api_key: api_keys_info.append("OpenRouter")
        if deepseek_api_key: api_keys_info.append("DeepSeek (å¤‡é€‰)")
        
        st.success(f"å·²é…ç½®APIå¯†é’¥: {', '.join(api_keys_info)}ï¼Œæ‰¹é‡ç¿»è¯‘åŠŸèƒ½å¯ç”¨ã€‚")
        
        # æ·»åŠ "é‡æ–°ç¿»è¯‘"é€‰é¡¹
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ç¿»è¯‘å½“å‰é¡µé¢æ˜¾ç¤ºçš„æ–‡ä»¶"):
                current_page_files = json_files[start_idx:end_idx]
                batch_translate_files(current_page_files)
        
        with col2:
            if st.button("ç¿»è¯‘å½“å‰é¡µé¢æ–‡ä»¶ (å¼ºåˆ¶é‡æ–°ç¿»è¯‘)"):
                current_page_files = json_files[start_idx:end_idx]
                batch_translate_files(current_page_files, force_translate=True)
        
        # å…¨å±€æ‰¹é‡ç¿»è¯‘é€‰é¡¹
        col3, col4 = st.columns(2)
        with col3:
            if st.button("ç¿»è¯‘æ‰€æœ‰æœªç¿»è¯‘/éƒ¨åˆ†ç¿»è¯‘çš„æ–‡ä»¶ (æœ€å¤š20ä¸ª)"):
                all_files_for_batch, all_info_for_batch = get_all_json_files(data_dir)
                untranslated_files = []
                for file_path, info in zip(all_files_for_batch, all_info_for_batch):
                    fully_translated = (info.get("has_workflow_json_zh", False) and 
                                        info.get("has_readme_zh", False) and 
                                        info.get("title_zh", ""))
                    if not fully_translated:
                        untranslated_files.append(file_path)
                
                if untranslated_files:
                    st.info(f"æ‰¾åˆ° {len(untranslated_files)} ä¸ªæœªå®Œå…¨ç¿»è¯‘çš„æ–‡ä»¶ã€‚å°†å¤„ç†æœ€å¤š20ä¸ªã€‚")
                    max_batch = min(20, len(untranslated_files))
                    batch_translate_files(untranslated_files[:max_batch])
                else:
                    st.info("æ‰€æœ‰æ–‡ä»¶éƒ½å·²å®Œå…¨ç¿»è¯‘ã€‚")
        
        with col4:
            if st.button("å¼ºåˆ¶é‡æ–°ç¿»è¯‘æ‰€æœ‰å·²ç¿»è¯‘æ–‡ä»¶ (æœ€å¤š10ä¸ª)"):
                all_files_for_batch, all_info_for_batch = get_all_json_files(data_dir)
                translated_files = []
                for file_path, info in zip(all_files_for_batch, all_info_for_batch):
                    has_any_translation = (info.get("has_workflow_json_zh", False) or 
                                        info.get("has_readme_zh", False) or 
                                        info.get("title_zh", ""))
                    if has_any_translation:
                        translated_files.append(file_path)
                
                if translated_files:
                    st.info(f"æ‰¾åˆ° {len(translated_files)} ä¸ªå·²æœ‰ç¿»è¯‘çš„æ–‡ä»¶ã€‚å°†å¼ºåˆ¶é‡æ–°ç¿»è¯‘æœ€å¤š10ä¸ªã€‚")
                    max_batch = min(10, len(translated_files))
                    batch_translate_files(translated_files[:max_batch], force_translate=True)
                else:
                    st.info("æ²¡æœ‰æ‰¾åˆ°å·²ç¿»è¯‘çš„æ–‡ä»¶ã€‚")

if __name__ == "__main__":
    main()