{
  "url": "https://blog.n8n.io/ai-workflows-for-the-cautious-enterprise/",
  "title": "AI Workflows for the Cautious Enterprise",
  "excerpt": "",
  "thumbnail": "https://blog.n8n.io/content/images/size/w1200/2025/02/post_02.png",
  "tags": [
    "AI",
    "ITOps"
  ],
  "html": "<p>We designed this guide for the risk-sensitive enterprises that need to start considering their AI adoption strategy to remain competitive, but have little tolerance for data integrity or privacy-related risks. It outlines a range of techniques, such as optimizing LLM accuracy, adding guardrails, running AI models locally in the workflow automation tool, scalability, and other considerations that can bring AI to an enterprise-grade standard.</p><p>Some workflow automation tools - like n8n - can integrate AI within the automation logic. Executing AI as part of a wider customizable workflow is the best way for enterprises to mitigate the inherent risks of large language models, computer vision, and other AI algorithms.&nbsp;</p><p>Running AI as part of an automation workflow has three main considerations:</p><ol><li>The workflow automation tool can embed AI locally, or it can call external services.</li><li>AI agents are just one component of the workflow, so additional logic can be defined for both the AI input and output.</li><li>Automation logic can easily integrate AI with proprietary tools and legacy stack.</li></ol><p>When workflow-based automation tools act as a wrapper around AI workloads such as large language models, organizations can address some of the most pressing AI-related challenges, which include:</p><ul><li><strong>Data privacy</strong> - the AI models can either be run locally or in a trusted location. Additional privacy and data loss prevention mechanisms can be implemented before data is sent to the AI agent and after the response is generated.</li><li><strong>Insufficient talent and lack of skills to integrate AI</strong> - workflow-based automation tools offer intuitive graphical user interfaces that allow both developer and non-developer audiences to build automation logic that implements AI.</li><li><strong>Minimizing hallucinations</strong> - these refer to non-factual or nonsensical LLM responses, which must be subject to output controls that can detect errors or noncompliant responses.</li><li><strong>Sufficient and organized data for Small Language Models (SLM) or contextual responses</strong> - workflow automation tools can access disparate data sets and normalize data before for the AI model to generate responses.</li></ul><p></p><h2 id=\"types-of-ai-and-ml-models\">Types of AI and ML models</h2><p>Since 2023, the term AI has been used interchangeably with large language models (LLMs) and text-based generative AI, courtesy of the viral popularity of ChatGPT. However, itâ€™s important to distinguish between different types of production-ready AI workloads</p><ul><li><strong>Generative AI </strong>- consists of large and small language models, image generation, and video generation, among others.&nbsp;<ul><li><strong>Large language models</strong> - Neural networks trained on vast text datasets, capable of understanding context, generating human-like text, code, and performing complex reasoning tasks (e.g., GPT-4, Claude, PaLM). For example, you can create a <a href=\"https://n8n.io/workflows/2457-multi-agent-pdf-to-blog-content-generation/?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>Multi-Agent PDF-to-Blog Content Generation</u></a></li><li><strong>Small/Specialized language models</strong> - these models are trained for specific tasks like text completion, sentiment analysis, or domain-specific generation, with significantly fewer parameters than LLMs.</li><li><strong>Image generation</strong> - Models that create new images from text descriptions or modify existing images (e.g., DALL-E, Midjourney, Stable Diffusion). <a href=\"https://n8n.io/workflows/2417-flux-ai-image-generator/?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>Here is one use case for AI-based image generation.</u></a></li><li><strong>Video generation</strong> - AI systems that can create or edit video content from text prompts or existing footage (e.g., Runway Gen-2, Google Image Video, Meta's Make-A-Video).</li><li><strong>Speech generation</strong> - A model that converts text to audio, such as <a href=\"https://n8n.io/workflows/2092-convert-text-to-speech-with-openai/?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>this workflow that uses the OpenAI TTS mode</u></a>l.</li></ul></li><li><strong>Computer vision</strong> - can be used for object detection and recognition in images/video, facial recognition and analysis. <a href=\"https://n8n.io/workflows/2420-automate-image-validation-tasks-using-ai-vision/?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>Here is one case for Automate Image Validation Tasks using AI Vision</u></a>.</li><li><strong>Speech recognition</strong> - can be used to convert audio to text in near real-time and across multiple languages.</li><li><strong>Time series analysis -</strong> can be used for analyzing and forecasting trends, detecting seasonal patterns, identifying anomalies, and making predictions based on sequential, time-dependent data.</li></ul><p>Based on the above, some achievable and production-ready use cases that implement AI today include agent support and Slack Bots, scheduling appointments, summarizing and chatting with internal PDFs, web scraping and webpage summarization, adding memory to AI agents, automating competitor research, image captioning, and customer support issue resolution using text classifier. A <a href=\"https://n8n.io/workflows/categories/ai/?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>full library of AI templates can be found here.</u></a></p><h1 id=\"optimizing-llm-accuracy\">Optimizing LLM Accuracy</h1><p>Out of all AI models available today, large language models have the highest potential of delivering value for enterprises in the short-term. As such, this section of the article will focus on optimizing these for specific use cases and proactively addressing hallucination-related challenges.</p><p>LLM optimization typically consists of prompt engineering, retrieval-augmented generation (RAG), and model fine-tuning. These address different optimization use cases and are typically used together. We will also discuss guardrail mechanisms that protect against issues such as prompt injection and data leaks.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXeVifFw4DR_FcFYImLi4HuTQPSQNpDtu2MFYjnV9P33_erhAJm58PqrAFWSiaCKnjhMOQZ87-v-sMPbccMrTfmGbdWuBetYxijQp7_SXXuWsYHiPof5eRHMUp27VmkpnINhPcWd?key=5Bu_i2rT_sZgQ-_nz7V586fH\" class=\"kg-image lightense-target\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"351\"></figure><p><em>Source: https://platform.openai.com/docs/guides/optimizing-llm-accuracy/</em></p><p>In the above graphic, context optimization refers to the model lacking contextual knowledge if data was missing from the training set, its knowledge is out of date, or if it requires knowledge of proprietary information. LLM optimization refers to the consistency of the behavior, and must be considered if the model - given suitable prompts - is producing inconsistent results with incorrect formatting, the tone or style of speech is not correct, or the reasoning is not being followed consistently.&nbsp;</p><h2 id=\"prompt-engineering\">Prompt Engineering</h2><p>Prompt engineering is the adjustment of the input request made to the LLM, and is often the only method needed for use cases like summarization, translation, and code generation where a zero-shot approach can reach production levels of accuracy and consistency.</p><p>Basic prompt engineering techniques look at aligning the input prompt to produce a desired output. Some guidelines recommend starting with a simple prompt and an expected output in mind, and then optimizing the prompt by adding context, instructions, or examples.</p><p>We recommend this<a href=\"https://platform.openai.com/docs/guides/prompt-engineering?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u> OpenAI Guide on Prompt Engineering.</u></a> At a high level, the guide suggests the following strategies:</p><ul><li>Provide reference text</li><li>Split complex tasks into simpler subtasks</li><li>Give the model time to \"think\"</li><li>Use external tools</li><li>Test changes systematically</li></ul><p>You can also follow prompt engineering guidelines from <a href=\"https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>Anthropic</u></a> and <a href=\"https://docs.mistral.ai/guides/prompting_capabilities/?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>Mistral</u></a>.</p><h2 id=\"retrieval-augmented-generation-rag\">Retrieval-Augmented Generation (RAG)</h2><p>RAG is the process of fetching relevant content to provide additional context for an LLM before it generates an answer. It is used to give the model access to domain-specific context to solve a task.</p><p>For example, if you need an LLM to produce answers that contain statistics, it can do so by retrieving information from a relevant database. When users ask a question, the prompt is embedded and used to retrieve the most relevant content from the knowledge base. This is presented to the model, which answers the question.</p><p>While this is a great technique for referencing hard data, RAG also introduces a new dimension to consider - retrieval. If the LLM is supplied the wrong context, it cannot answer correctly, and if it is supplied too much irrelevant context, it may increase hallucinations.</p><p>Retrieval itself must be optimized, which consists of tuning the search to return the right results, tuning the search to include less noise, and providing more information in each retrieved result. Libraries like LlamaIndex and LangChain are useful tools to optimize RAG.&nbsp;</p><p><a href=\"https://community.n8n.io/t/no-code-week-2024-from-gpts-to-custom-tailored-rag/48754?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>This presentation</u></a> offers a good overview of retrieval-augmented generation.</p><h2 id=\"fine-tuning\">Fine-Tuning</h2><p>Fine-tuning is the process of continuing the training process of the LLM on a smaller, domain-specific dataset to optimize it for the specific task. Fine-tuning is typically performed to improve model accuracy on a specific task by providing many examples of that task being performed correctly, and to improve model efficiency by achieving the same accuracy for fewer tokens or by using a smaller model.</p><p>The most important step in the fine-tuning process is preparing a dataset of training examples, which contains clean, labeled, and unbiased data.</p><h2 id=\"guardrails\">Guardrails</h2><p>Guardrails refer to controlling the LLM input and output controls that detect, quantify and mitigate the presence of specific types of risks, such as prompt injection. Thereâ€™s an <a href=\"https://github.com/guardrails-ai/guardrails?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>eponymous open source</u></a> project that provides a Python framework for implementing these additional protection mechanisms.&nbsp;</p><p>These can be used to check whether the generated text is toxic, that responses are provided in a neutral or positive tone, that responses do not contain any financial advice in line with FINRA guidelines, to prevent user's personal data from being leaked in the response, to prevent mentions of competitors and replace with alternate phrasing, and other similar use cases.</p><h1 id=\"deployment-models-for-ai-enhanced-workflow-automation\">Deployment Models for AI-Enhanced Workflow Automation</h1><p>There are two options to run AI models within a workflow automation platform, namely to run the AI model natively in the tool, or to run the AI model as a standalone service and make requests over the network.&nbsp;</p><p><strong>AI Natively Integrated in the workflow automation tool</strong></p><ul><li><strong>Pros</strong>: No dependency on an external AI service, no data leaves the tool, no network-induced latency, and no per-request cost incurred.</li><li><strong>Cons</strong>: More complex setup and configuration, harder to change models.</li></ul><p><strong>Standalone AI model</strong></p><ul><li><strong>Pros</strong>: Easy setup and configuration, wide choice of models.</li><li><strong>Cons</strong>: Privacy and data loss concerns as data leaves the automation tool, induces network latency, incurs per-request cost.</li></ul><p>Both of these components - the AI model and the workflow tool - can either be deployed in an as-a-service model, where it is run and managed by the vendor or third party, or can be self-hosted.</p><p></p>\n<!--kg-card-begin: html-->\n<table style=\"border:none;border-collapse:collapse;table-layout:fixed;width:468pt\"><colgroup><col><col><col></colgroup><tbody><tr style=\"height:0pt\"><td style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"><br></td><td style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"><p dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">AI Natively Integrated in the Workflow automation tool</span></p></td><td style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"><p dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">Standalone AI model</span></p></td></tr><tr style=\"height:0pt\"><td style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"><p dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">Self-hosted</span></p></td><td style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"><p dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">Enterprise manages and runs the whole platform in their environment, whether on-premises or in the cloud</span></p></td><td style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"><p dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">Enterprise manages and runs the LLM in their environment (e.g., Ollama), </span><span style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">whether on-premises or in the cloud</span></p></td></tr><tr style=\"height:0pt\"><td style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"><p dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">As-a-service</span></p></td><td style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"><p dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">Platform run by the vendor in the Cloud</span></p></td><td style=\"border-left:solid #000000 1pt;border-right:solid #000000 1pt;border-bottom:solid #000000 1pt;border-top:solid #000000 1pt;vertical-align:top;padding:5pt 5pt 5pt 5pt;overflow:hidden;overflow-wrap:break-word;\"><p dir=\"ltr\" style=\"line-height:1.2;margin-top:0pt;margin-bottom:0pt;\"><span style=\"font-size:11pt;font-family:Arial,sans-serif;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;\">LLM run in the Cloud and integrated via API requests (e.g., ChatGPT)</span></p></td></tr></tbody></table>\n<!--kg-card-end: html-->\n<p>Out of the deployment models described above, the one that is uncommon and is worth further exploration is the self-hosted version of a workflow automation tool with AI natively integrated. We describe this in further detail below.</p><h2 id=\"self-hosting-workflow-automation-tools-with-integrated-ai\">Self-Hosting Workflow Automation Tools with Integrated AI</h2><figure class=\"kg-card kg-image-card\"><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdAZ-4mMp2B6v04knnnw3-GMGx5XTWjhiR9QUdlszogp1kQzlHEWbWpL2ySmNdU3xyPgUjLI1TLCy30LVMSjL61nBrm-CpUn8WyWqQwwcy3s5zNgKLkSZ-bHwlMmTX_XH2GoeN5?key=5Bu_i2rT_sZgQ-_nz7V586fH\" class=\"kg-image lightense-target\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"351\"></figure><p>This model offers enterprises more control over how the AI model is run and managed, but entails a more hands-on approach compared to sending API requests to ChatGPT. Some of the difficulties in running AI locally include the choice of models, vector stores, and frameworks and configuring these components to work together.&nbsp;</p><p>To address this high entry barrier, n8n has released a <a href=\"https://blog.n8n.io/self-hosted-ai/\"><u>Self-hosted AI Starter Kit </u></a>since August 2024. It consists of an open-source Docker Compose template that can initialize a local AI and low-code development environment. It includes the n8n workflow automation platform and a selection of best-in-class local AI tools, designed to be the fastest way to start building self-hosted AI workflows. As a virtual container-based appliance, it can be deployed both on-premises and in customer-managed cloud environments.</p><p>The kit uses Ollama as the API to interact with language models locally, Qdrant as a vector database, and PostgreSQL as a relational database. It also contains preconfigured AI workflow templates.&nbsp;</p><p>The starter kit also provides networking configurations to deploy locally or in cloud instances such as Digital Ocean and runpod.io. The starter kit is also designed to help organizations access local files by creating a shared folder which is mounted to the n8n container and allows n8n to access files on disk.</p><h1 id=\"integrating-ai-in-automation-workflows\">Integrating AI in Automation workflows</h1><p>AI-based workflows typically focus on the capabilities delivered by the AI model, such as interpreting natural language commands, object recognition and categorization, or unstructured data analysis. AI-based workflows have custom logic defined before the AI model, such as the conditions when the workflow is triggered and fetching the right data, and after the AI model, such as validating responses and preventing data leaks.</p><p>When considering the time-to-value associated with automation tools that implement AI, organizations need to evaluate the vendorâ€™s portfolio of out-of-the-box workflows and the workflow designer itself.</p><p><strong>Ready-built workflows</strong> - the vendor can provide a marketplace or library of pre-built, pre-configured, and pre-validated AI templates that organizations can deploy with minimal configuration. These workflows need to be documented and modular such that any necessary changes dictated by the customerâ€™s technology stack can be easily implemented. These templates can either be developed directly by the vendor, or they can open the marketplace to allow members of the community to publish workflows. As an example,<a href=\"https://n8n.io/workflows/categories/ai/?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u> n8n offers a library of &gt;160 AI-based workflows</u></a>, many of which are free to use.&nbsp;</p><p><strong>Workflow designer</strong> - to support customers in writing new AI-based workflows, the vendor must provide adequate documentation that explains how the AI functionality is implemented in the automation logic. Further, it needs to provide development and staging environments, where customers can test the workflows before deploying these in production. Lastly, validation tools can identify any misconfigurations and security concerns in the playbooks, such as data which is not the expected format or calls to external services on unsecured channels.</p><h1 id=\"running-an-ai-workflow-tool-in-production\">Running an AI Workflow Tool in Production&nbsp;</h1><p>Once the AI-based automation workflows have been designed, you need to consider how the tool will behave in production with respect to inference engines, scalability, monitoring, troubleshooting, authorization, and access.</p><h2 id=\"inference-engine\">Inference Engine</h2><p>To run LLMs in production environments, you can choose between a range of inference engines. These run inference on trained machine learning or deep learning models across frameworks (PyTorch, Scikit-Learn,TensorFlow, etc.) or processors (GPU, CPU, etc). Most inference engines are open-source software that can standardize AI model deployment and execution.&nbsp;</p><p>Some of the most widely deployed inference engines include:</p><ul><li><a href=\"https://github.com/vllm-project/vllm?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>vLLM</u></a></li><li><a href=\"https://github.com/PygmalionAI/aphrodite-engine?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>Aphrodite-Engine</u></a></li><li><a href=\"https://github.com/triton-inference-server/server?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>triton-inference-server</u></a></li><li><a href=\"https://github.com/huggingface/text-generation-inference?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>Hugging Face TGI</u></a></li></ul><h2 id=\"scalability\">Scalability</h2><p>Scalability considerations for running AI-enhanced automation workflows includes both the AI modelâ€™s scalability and the workflow toolâ€™s.</p><p>For the workflow tool, scalability can consist of</p><ul><li>Dynamic scaling: This involves adding more resources dynamically as the amount of data ingested increases. While cloud-based solutions can provision these resources automatically, the self-hosted equivalent involves adding more instances for horizontal scalability with minimal disruption or additional configuration.</li><li>Absolute scale limits: These can be hard limits such as the number of concurrent workflows, maximum file sizes, maximum number of actions per playbook.</li><li>Support considerations: Smaller vendors with a scalable product may struggle to provide in-house enterprise-grade support for very large deployments, which includes support tickets and incident management. Furthermore, they may not have yet developed partnerships with third-party services providers that can help with large operations.</li><li>Pricing considerations: Some vendor pricing mechanisms may lock customers out of large or growing deployments, which forces organizations to compromise on the types of data they ingest, store, or analyze.</li></ul><p>For AI models hosted in the cloud by third parties (i.e. ChatGPT), main scalability concerns revolve around pricing, where the cost of large numbers of requests outweigh any efficiencies gained by implementing AI.</p><p>For self-hosted AI models scalability, considerations include maximum data input volume, maximum output, rate of errors, and mean time to get an output. Compute infrastructure is the main factor for scaling AI models, where the underlying hardware must have enough processing power to support requests and also support for horizontal scaling. When choosing to host the model in the cloud, horizontal scalability is not generally a concern, but with on-premises deployments there is a hard limit on the number of servers available at any given time. The scale of deployment directly correlates with the model's size and complexity, with smaller models (1-3 billion parameters) requiring substantially less compute infrastructure compared to large language models with 30-70 billion parameters.</p><p>Self-hosted models also have some control over response times. Model choice once again may have an impact, where small models can respond in 100-500 milliseconds, while larger models might require 1-5 seconds per inference. This can further be tuned with techniques such as model quantization, distributed inference, batch processing, intelligent caching, and hardware acceleration.&nbsp;</p><p>In instances such as n8nâ€™s AI Starter Kit, using Kubernetes to orchestrate the containers running the workflow tool and large language model can help deliver horizontal scalability based on request demand. Besides horizontal scalability, using Kubernetes can also help with load balancing and request handling.</p><h3 id=\"gpu-requirements-for-self-hosting\">GPU Requirements for Self-hosting</h3><p>To calculate how much GPU memory is required to run an LLM on-premises, you can use the following formula:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXd2O6INZEWhxEDKmagzxn_bXNWUOhlooQskVUbyDSeC4bjRDwmXi0f7jzzNHND5MyjBygDMgIm_Szn3A4qA0A4j49FJIZt_fMP_4jqCWAkdUTHyOewzcahtCyQBExFYDEVnDXw_oA?key=5Bu_i2rT_sZgQ-_nz7V586fH\" class=\"kg-image lightense-target\" alt=\"\" loading=\"lazy\" width=\"231\" height=\"86\"></figure><p><em>Source: https://ksingh7.medium.com/calculate-how-much-gpu-memory-you-need-to-serve-any-llm-67301a844f21</em></p><p>Where:</p><ul><li>M = GPU memory in GB</li><li>P = number of parameters</li><li>4B = 4 bytes per parameter</li><li>32 = 32 bits for the 4 bytes</li><li>Q = number of bits used for loading the model</li><li>1.2 = 20% overhead allocation</li></ul><p>As an example, applying this formula for Llama-3.1-70b, which has 70 Billion Parameters, we get a total of 168GB or GPU memory. This would require&nbsp; two NVIDIA A-100 80GB memory models to run.</p><p>In instances where computational power for hosting AI is limited, you can reduce the compute and memory costs using <a href=\"https://huggingface.co/docs/optimum/en/concept_guides/quantization?ref=blog.n8n.io#quantization\" target=\"_blank\" rel=\"noopener\"><u>quantization</u></a>. It is a technique that enables the AI model to operate with less memory storage, consume less energy (in theory), and perform operations like matrix multiplication faster. It does so by representing the weights and activations with low-precision data types like 8-bit integers instead of the usual 32-bit floating point.</p><p>Applying this quantization technique to the above calculation for Llama 70B, using float16 precision instead of float32 would cut the memory requirement in half.</p><h2 id=\"authorization-and-authentication\">Authorization and Authentication</h2><p>As AI-enhanced workflows can have multiple business owners and access various data resources, authorization and role based access controls can help segregate access to reduce privacy concerns and attack surface.</p><p><strong>User access controls</strong> refers to how permissions are granted and managed to administrators, developers, and non-developers that have access to the workflow designer.</p><p><strong>Service access controls</strong> refers to how the workflow automation tool can access other services and databases, such as providing API keys or JWT tokens, allowing access only through specific ports, requiring encryption, and the like.</p><h2 id=\"monitoring-and-error-handling\">Monitoring and Error Handling</h2><p>Workflow automation tools can implement monitoring capabilities for each step or action that takes place within the workflow, including any AI agents. These can report on execution times, errors, API codes, and generate logs.&nbsp;</p><p>To monitor the performance of AI agents, tools can track performance against:</p><ul><li>Latency - How quickly an LLM can provide a response after receiving an input. Faster response times enhance user satisfaction and engagement.</li><li>Throughput - The number of tasks or queries an LLM can handle within a given time frame, for assessing the model's capability to serve multiple requests simultaneously. This is important for scalability and performance in production environments.</li><li>Resource Utilization - How efficiently an LLM uses computational resources, such as CPU and GPU memory. Optimal resource utilization ensures that the model runs efficiently, enabling cost-effective scaling and sustainability in deployment.</li></ul><p>Based on the reported metrics, organizations can implement multiple layers of validation, including:</p><ul><li>Input preprocessing and sanitization</li><li>Confidence threshold monitoring</li><li>Fallback mechanisms for low-confidence predictions, such as implementing human agents in the loop</li></ul><p></p><h1 id=\"n8n-for-building-ai-based-workflows\">n8n for Building AI-based Workflows</h1><p>Secure and privacy-first AI is a core strategy for n8n, so we have developed a range of features that make use of AI. At n8n, we use AI-enhanced workflows internally for a variety of use cases, which include:</p><ul><li>Battlecard bot</li><li>Our own AI assistant for errors</li><li>Template reviews&nbsp;</li><li>Classifying and assigning bugs&nbsp;</li><li>Classifying forum posts</li></ul><p>We are also building a library of community-built AI content, which<a href=\"https://n8n.io/workflows/categories/ai/?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u> you can explore here</u></a>. We encourage members of the community to <a href=\"https://n8n.notion.site/n8n-Creator-hub-7bd2cbe0fce0449198ecb23ff4a2f76f?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>become creators</u></a> and submit their own templates.</p>\n\t\t<div class=\"newsletter-banner\">\n\t    <div class=\"newsletter-banner-content\">\n\t      <div class=\"section-header\">\n\t        <h2>Subscribe to <span>n8n newsletter</span></h2>\n\t        <div class=\"section-subheader--bottom\">\n\t          Get the best, coolest, and latest in automation and low-code delivered to your inbox each week.\n\t        </div>\n\t      </div>\n\t      <div class=\"newsletter-banner-form\">\n\t        <form autocomplete=\"off\" class=\"contact-form\" onsubmit=\"subscribeNewsletter(event)\">\n\t        \t<div id=\"recaptcha\" class=\"g-recaptcha\" data-sitekey=\"6LeAQeopAAAAAKlLsRb1weWm6T_vijoQBkGkbHzB\" data-callback=\"submitSubscription\" data-size=\"invisible\"><div class=\"grecaptcha-badge\" data-style=\"bottomright\" style=\"width: 256px; height: 60px; display: block; transition: right 0.3s ease 0s; position: fixed; bottom: 14px; right: -186px; box-shadow: gray 0px 0px 5px; border-radius: 2px; overflow: hidden;\"><div class=\"grecaptcha-logo\"><iframe title=\"reCAPTCHA\" width=\"256\" height=\"60\" role=\"presentation\" name=\"a-2x9qvenfheai\" frameborder=\"0\" scrolling=\"no\" sandbox=\"allow-forms allow-popups allow-same-origin allow-scripts allow-top-navigation allow-modals allow-popups-to-escape-sandbox allow-storage-access-by-user-activation\" src=\"https://www.google.com/recaptcha/api2/anchor?ar=1&amp;k=6LeAQeopAAAAAKlLsRb1weWm6T_vijoQBkGkbHzB&amp;co=aHR0cHM6Ly9ibG9nLm44bi5pbzo0NDM.&amp;hl=en&amp;v=jt8Oh2-Ue1u7nEbJQUIdocyd&amp;size=invisible&amp;cb=7nevwgcmutz1\"></iframe></div><div class=\"grecaptcha-error\"></div><textarea id=\"g-recaptcha-response\" name=\"g-recaptcha-response\" class=\"g-recaptcha-response\" style=\"width: 250px; height: 40px; border: 1px solid rgb(193, 193, 193); margin: 10px 25px; padding: 0px; resize: none; display: none;\"></textarea></div><iframe style=\"display: none;\"></iframe></div>\n\t          <div class=\"input-wrapper\">\n\t            <input placeholder=\"Email\" name=\"email\" type=\"email\" required=\"required\" class=\"\">\n\t            <div class=\"messages\">\n\t              <div class=\"message message--error\">Something went wrong. Please try again later.</div>\n\t              <div class=\"message message--success\">Subscribed!</div>\n\t            </div>\n\t          </div>\n\t          <button type=\"submit\" class=\"submit-btn\">Subscribe</button>\n\t        </form>\n\t      </div>\n\t    </div>\n    </div>\n\t\t<div class=\"post-share-section\">\n\t<div class=\"post-share-wrap\">\n\t\t<a href=\"https://twitter.com/intent/tweet?text=AI%20Workflows%20for%20the%20Cautious%20Enterprise&amp;url=https://blog.n8n.io/ai-workflows-for-the-cautious-enterprise/\" target=\"_blank\" rel=\"noopener\" aria-label=\"Twitter share icon\"><svg role=\"img\" viewBox=\"0 0 24 24\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M23.954 4.569c-.885.389-1.83.654-2.825.775 1.014-.611 1.794-1.574 2.163-2.723-.951.555-2.005.959-3.127 1.184-.896-.959-2.173-1.559-3.591-1.559-2.717 0-4.92 2.203-4.92 4.917 0 .39.045.765.127 1.124C7.691 8.094 4.066 6.13 1.64 3.161c-.427.722-.666 1.561-.666 2.475 0 1.71.87 3.213 2.188 4.096-.807-.026-1.566-.248-2.228-.616v.061c0 2.385 1.693 4.374 3.946 4.827-.413.111-.849.171-1.296.171-.314 0-.615-.03-.916-.086.631 1.953 2.445 3.377 4.604 3.417-1.68 1.319-3.809 2.105-6.102 2.105-.39 0-.779-.023-1.17-.067 2.189 1.394 4.768 2.209 7.557 2.209 9.054 0 13.999-7.496 13.999-13.986 0-.209 0-.42-.015-.63.961-.689 1.8-1.56 2.46-2.548l-.047-.02z\"></path></svg></a>\n\t\t<a href=\"https://www.facebook.com/sharer/sharer.php?u=https://blog.n8n.io/ai-workflows-for-the-cautious-enterprise/\" target=\"_blank\" rel=\"noopener\" aria-label=\"Facebook share icon\"><svg role=\"img\" viewBox=\"0 0 24 24\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M23.9981 11.9991C23.9981 5.37216 18.626 0 11.9991 0C5.37216 0 0 5.37216 0 11.9991C0 17.9882 4.38789 22.9522 10.1242 23.8524V15.4676H7.07758V11.9991H10.1242V9.35553C10.1242 6.34826 11.9156 4.68714 14.6564 4.68714C15.9692 4.68714 17.3424 4.92149 17.3424 4.92149V7.87439H15.8294C14.3388 7.87439 13.8739 8.79933 13.8739 9.74824V11.9991H17.2018L16.6698 15.4676H13.8739V23.8524C19.6103 22.9522 23.9981 17.9882 23.9981 11.9991Z\"></path></svg></a>\n\t\t<!-- <a href=\"javascript:\" class=\"post-share-link\" id=\"copy\" data-clipboard-target=\"#copy-link\" aria-label=\"Copy link icon\"><svg role=\"img\" viewBox=\"0 0 33 24\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M27.3999996,13.4004128 L21.7999996,13.4004128 L21.7999996,19 L18.9999996,19 L18.9999996,13.4004128 L13.3999996,13.4004128 L13.3999996,10.6006192 L18.9999996,10.6006192 L18.9999996,5 L21.7999996,5 L21.7999996,10.6006192 L27.3999996,10.6006192 L27.3999996,13.4004128 Z M12,20.87 C7.101,20.87 3.13,16.898 3.13,12 C3.13,7.102 7.101,3.13 12,3.13 C12.091,3.13 12.181,3.139 12.272,3.142 C9.866,5.336 8.347,8.487 8.347,12 C8.347,15.512 9.866,18.662 12.271,20.857 C12.18,20.859 12.091,20.87 12,20.87 Z M20.347,0 C18.882,0 17.484,0.276 16.186,0.756 C14.882,0.271 13.473,0 12,0 C5.372,0 0,5.373 0,12 C0,18.628 5.372,24 12,24 C13.471,24 14.878,23.726 16.181,23.242 C17.481,23.724 18.88,24 20.347,24 C26.975,24 32.347,18.628 32.347,12 C32.347,5.373 26.975,0 20.347,0 Z\"/></svg></a>\n\t\t<small class=\"share-link-info\">The link has been copied!</small> -->\n\t</div>\n\t<input type=\"text\" value=\"https://blog.n8n.io/ai-workflows-for-the-cautious-enterprise/\" id=\"copy-link\" aria-label=\"Copy link input\">\n</div>",
  "readme": "We designed this guide for the risk-sensitive enterprises that need to start considering their AI adoption strategy to remain competitive, but have little tolerance for data integrity or privacy-related risks. It outlines a range of techniques, such as optimizing LLM accuracy, adding guardrails, running AI models locally in the workflow automation tool, scalability, and other considerations that can bring AI to an enterprise-grade standard.\n\nSome workflow automation tools - like n8n - can integrate AI within the automation logic. Executing AI as part of a wider customizable workflow is the best way for enterprises to mitigate the inherent risks of large language models, computer vision, and other AI algorithms. \n\nRunning AI as part of an automation workflow has three main considerations:\n\n  1. The workflow automation tool can embed AI locally, or it can call external services.\n  2. AI agents are just one component of the workflow, so additional logic can be defined for both the AI input and output.\n  3. Automation logic can easily integrate AI with proprietary tools and legacy stack.\n\n\n\nWhen workflow-based automation tools act as a wrapper around AI workloads such as large language models, organizations can address some of the most pressing AI-related challenges, which include:\n\n  * **Data privacy** \\- the AI models can either be run locally or in a trusted location. Additional privacy and data loss prevention mechanisms can be implemented before data is sent to the AI agent and after the response is generated.\n  * **Insufficient talent and lack of skills to integrate AI** \\- workflow-based automation tools offer intuitive graphical user interfaces that allow both developer and non-developer audiences to build automation logic that implements AI.\n  * **Minimizing hallucinations** \\- these refer to non-factual or nonsensical LLM responses, which must be subject to output controls that can detect errors or noncompliant responses.\n  * **Sufficient and organized data for Small Language Models (SLM) or contextual responses** \\- workflow automation tools can access disparate data sets and normalize data before for the AI model to generate responses.\n\n\n\n## Types of AI and ML models\n\nSince 2023, the term AI has been used interchangeably with large language models (LLMs) and text-based generative AI, courtesy of the viral popularity of ChatGPT. However, itâ€™s important to distinguish between different types of production-ready AI workloads\n\n  * **Generative AI** \\- consists of large and small language models, image generation, and video generation, among others. \n    * **Large language models** \\- Neural networks trained on vast text datasets, capable of understanding context, generating human-like text, code, and performing complex reasoning tasks (e.g., GPT-4, Claude, PaLM). For example, you can create a [_Multi-Agent PDF-to-Blog Content Generation_](https://n8n.io/workflows/2457-multi-agent-pdf-to-blog-content-generation/?ref=blog.n8n.io)\n    * **Small/Specialized language models** \\- these models are trained for specific tasks like text completion, sentiment analysis, or domain-specific generation, with significantly fewer parameters than LLMs.\n    * **Image generation** \\- Models that create new images from text descriptions or modify existing images (e.g., DALL-E, Midjourney, Stable Diffusion). [_Here is one use case for AI-based image generation._](https://n8n.io/workflows/2417-flux-ai-image-generator/?ref=blog.n8n.io)\n    * **Video generation** \\- AI systems that can create or edit video content from text prompts or existing footage (e.g., Runway Gen-2, Google Image Video, Meta's Make-A-Video).\n    * **Speech generation** \\- A model that converts text to audio, such as [_this workflow that uses the OpenAI TTS mode_](https://n8n.io/workflows/2092-convert-text-to-speech-with-openai/?ref=blog.n8n.io)l.\n  * **Computer vision** \\- can be used for object detection and recognition in images/video, facial recognition and analysis. [_Here is one case for Automate Image Validation Tasks using AI Vision_](https://n8n.io/workflows/2420-automate-image-validation-tasks-using-ai-vision/?ref=blog.n8n.io).\n  * **Speech recognition** \\- can be used to convert audio to text in near real-time and across multiple languages.\n  * **Time series analysis -** can be used for analyzing and forecasting trends, detecting seasonal patterns, identifying anomalies, and making predictions based on sequential, time-dependent data.\n\n\n\nBased on the above, some achievable and production-ready use cases that implement AI today include agent support and Slack Bots, scheduling appointments, summarizing and chatting with internal PDFs, web scraping and webpage summarization, adding memory to AI agents, automating competitor research, image captioning, and customer support issue resolution using text classifier. A [_full library of AI templates can be found here._](https://n8n.io/workflows/categories/ai/?ref=blog.n8n.io)\n\n# Optimizing LLM Accuracy\n\nOut of all AI models available today, large language models have the highest potential of delivering value for enterprises in the short-term. As such, this section of the article will focus on optimizing these for specific use cases and proactively addressing hallucination-related challenges.\n\nLLM optimization typically consists of prompt engineering, retrieval-augmented generation (RAG), and model fine-tuning. These address different optimization use cases and are typically used together. We will also discuss guardrail mechanisms that protect against issues such as prompt injection and data leaks.\n\n![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXeVifFw4DR_FcFYImLi4HuTQPSQNpDtu2MFYjnV9P33_erhAJm58PqrAFWSiaCKnjhMOQZ87-v-sMPbccMrTfmGbdWuBetYxijQp7_SXXuWsYHiPof5eRHMUp27VmkpnINhPcWd?key=5Bu_i2rT_sZgQ-_nz7V586fH)\n\n_Source: https://platform.openai.com/docs/guides/optimizing-llm-accuracy/_\n\nIn the above graphic, context optimization refers to the model lacking contextual knowledge if data was missing from the training set, its knowledge is out of date, or if it requires knowledge of proprietary information. LLM optimization refers to the consistency of the behavior, and must be considered if the model - given suitable prompts - is producing inconsistent results with incorrect formatting, the tone or style of speech is not correct, or the reasoning is not being followed consistently. \n\n## Prompt Engineering\n\nPrompt engineering is the adjustment of the input request made to the LLM, and is often the only method needed for use cases like summarization, translation, and code generation where a zero-shot approach can reach production levels of accuracy and consistency.\n\nBasic prompt engineering techniques look at aligning the input prompt to produce a desired output. Some guidelines recommend starting with a simple prompt and an expected output in mind, and then optimizing the prompt by adding context, instructions, or examples.\n\nWe recommend this[ _OpenAI Guide on Prompt Engineering._](https://platform.openai.com/docs/guides/prompt-engineering?ref=blog.n8n.io) At a high level, the guide suggests the following strategies:\n\n  * Provide reference text\n  * Split complex tasks into simpler subtasks\n  * Give the model time to \"think\"\n  * Use external tools\n  * Test changes systematically\n\n\n\nYou can also follow prompt engineering guidelines from [_Anthropic_](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview?ref=blog.n8n.io) and [_Mistral_](https://docs.mistral.ai/guides/prompting_capabilities/?ref=blog.n8n.io).\n\n## Retrieval-Augmented Generation (RAG)\n\nRAG is the process of fetching relevant content to provide additional context for an LLM before it generates an answer. It is used to give the model access to domain-specific context to solve a task.\n\nFor example, if you need an LLM to produce answers that contain statistics, it can do so by retrieving information from a relevant database. When users ask a question, the prompt is embedded and used to retrieve the most relevant content from the knowledge base. This is presented to the model, which answers the question.\n\nWhile this is a great technique for referencing hard data, RAG also introduces a new dimension to consider - retrieval. If the LLM is supplied the wrong context, it cannot answer correctly, and if it is supplied too much irrelevant context, it may increase hallucinations.\n\nRetrieval itself must be optimized, which consists of tuning the search to return the right results, tuning the search to include less noise, and providing more information in each retrieved result. Libraries like LlamaIndex and LangChain are useful tools to optimize RAG. \n\n[_This presentation_](https://community.n8n.io/t/no-code-week-2024-from-gpts-to-custom-tailored-rag/48754?ref=blog.n8n.io) offers a good overview of retrieval-augmented generation.\n\n## Fine-Tuning\n\nFine-tuning is the process of continuing the training process of the LLM on a smaller, domain-specific dataset to optimize it for the specific task. Fine-tuning is typically performed to improve model accuracy on a specific task by providing many examples of that task being performed correctly, and to improve model efficiency by achieving the same accuracy for fewer tokens or by using a smaller model.\n\nThe most important step in the fine-tuning process is preparing a dataset of training examples, which contains clean, labeled, and unbiased data.\n\n## Guardrails\n\nGuardrails refer to controlling the LLM input and output controls that detect, quantify and mitigate the presence of specific types of risks, such as prompt injection. Thereâ€™s an [_eponymous open source_](https://github.com/guardrails-ai/guardrails?ref=blog.n8n.io) project that provides a Python framework for implementing these additional protection mechanisms. \n\nThese can be used to check whether the generated text is toxic, that responses are provided in a neutral or positive tone, that responses do not contain any financial advice in line with FINRA guidelines, to prevent user's personal data from being leaked in the response, to prevent mentions of competitors and replace with alternate phrasing, and other similar use cases.\n\n# Deployment Models for AI-Enhanced Workflow Automation\n\nThere are two options to run AI models within a workflow automation platform, namely to run the AI model natively in the tool, or to run the AI model as a standalone service and make requests over the network. \n\n**AI Natively Integrated in the workflow automation tool**\n\n  * **Pros** : No dependency on an external AI service, no data leaves the tool, no network-induced latency, and no per-request cost incurred.\n  * **Cons** : More complex setup and configuration, harder to change models.\n\n\n\n**Standalone AI model**\n\n  * **Pros** : Easy setup and configuration, wide choice of models.\n  * **Cons** : Privacy and data loss concerns as data leaves the automation tool, induces network latency, incurs per-request cost.\n\n\n\nBoth of these components - the AI model and the workflow tool - can either be deployed in an as-a-service model, where it is run and managed by the vendor or third party, or can be self-hosted.\n\n  \n| AI Natively Integrated in the Workflow automation tool| Standalone AI model  \n---|---|---  \nSelf-hosted| Enterprise manages and runs the whole platform in their environment, whether on-premises or in the cloud| Enterprise manages and runs the LLM in their environment (e.g., Ollama), whether on-premises or in the cloud  \nAs-a-service| Platform run by the vendor in the Cloud| LLM run in the Cloud and integrated via API requests (e.g., ChatGPT)  \n  \nOut of the deployment models described above, the one that is uncommon and is worth further exploration is the self-hosted version of a workflow automation tool with AI natively integrated. We describe this in further detail below.\n\n## Self-Hosting Workflow Automation Tools with Integrated AI\n\n![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdAZ-4mMp2B6v04knnnw3-GMGx5XTWjhiR9QUdlszogp1kQzlHEWbWpL2ySmNdU3xyPgUjLI1TLCy30LVMSjL61nBrm-CpUn8WyWqQwwcy3s5zNgKLkSZ-bHwlMmTX_XH2GoeN5?key=5Bu_i2rT_sZgQ-_nz7V586fH)\n\nThis model offers enterprises more control over how the AI model is run and managed, but entails a more hands-on approach compared to sending API requests to ChatGPT. Some of the difficulties in running AI locally include the choice of models, vector stores, and frameworks and configuring these components to work together. \n\nTo address this high entry barrier, n8n has released a [_Self-hosted AI Starter Kit_](https://blog.n8n.io/self-hosted-ai/)since August 2024. It consists of an open-source Docker Compose template that can initialize a local AI and low-code development environment. It includes the n8n workflow automation platform and a selection of best-in-class local AI tools, designed to be the fastest way to start building self-hosted AI workflows. As a virtual container-based appliance, it can be deployed both on-premises and in customer-managed cloud environments.\n\nThe kit uses Ollama as the API to interact with language models locally, Qdrant as a vector database, and PostgreSQL as a relational database. It also contains preconfigured AI workflow templates. \n\nThe starter kit also provides networking configurations to deploy locally or in cloud instances such as Digital Ocean and runpod.io. The starter kit is also designed to help organizations access local files by creating a shared folder which is mounted to the n8n container and allows n8n to access files on disk.\n\n# Integrating AI in Automation workflows\n\nAI-based workflows typically focus on the capabilities delivered by the AI model, such as interpreting natural language commands, object recognition and categorization, or unstructured data analysis. AI-based workflows have custom logic defined before the AI model, such as the conditions when the workflow is triggered and fetching the right data, and after the AI model, such as validating responses and preventing data leaks.\n\nWhen considering the time-to-value associated with automation tools that implement AI, organizations need to evaluate the vendorâ€™s portfolio of out-of-the-box workflows and the workflow designer itself.\n\n**Ready-built workflows** \\- the vendor can provide a marketplace or library of pre-built, pre-configured, and pre-validated AI templates that organizations can deploy with minimal configuration. These workflows need to be documented and modular such that any necessary changes dictated by the customerâ€™s technology stack can be easily implemented. These templates can either be developed directly by the vendor, or they can open the marketplace to allow members of the community to publish workflows. As an example,[_n8n offers a library of >160 AI-based workflows_](https://n8n.io/workflows/categories/ai/?ref=blog.n8n.io), many of which are free to use. \n\n**Workflow designer** \\- to support customers in writing new AI-based workflows, the vendor must provide adequate documentation that explains how the AI functionality is implemented in the automation logic. Further, it needs to provide development and staging environments, where customers can test the workflows before deploying these in production. Lastly, validation tools can identify any misconfigurations and security concerns in the playbooks, such as data which is not the expected format or calls to external services on unsecured channels.\n\n# Running an AI Workflow Tool in Production \n\nOnce the AI-based automation workflows have been designed, you need to consider how the tool will behave in production with respect to inference engines, scalability, monitoring, troubleshooting, authorization, and access.\n\n## Inference Engine\n\nTo run LLMs in production environments, you can choose between a range of inference engines. These run inference on trained machine learning or deep learning models across frameworks (PyTorch, Scikit-Learn,TensorFlow, etc.) or processors (GPU, CPU, etc). Most inference engines are open-source software that can standardize AI model deployment and execution. \n\nSome of the most widely deployed inference engines include:\n\n  * [_vLLM_](https://github.com/vllm-project/vllm?ref=blog.n8n.io)\n  * [ _Aphrodite-Engine_](https://github.com/PygmalionAI/aphrodite-engine?ref=blog.n8n.io)\n  * [ _triton-inference-server_](https://github.com/triton-inference-server/server?ref=blog.n8n.io)\n  * [ _Hugging Face TGI_](https://github.com/huggingface/text-generation-inference?ref=blog.n8n.io)\n\n\n\n## Scalability\n\nScalability considerations for running AI-enhanced automation workflows includes both the AI modelâ€™s scalability and the workflow toolâ€™s.\n\nFor the workflow tool, scalability can consist of\n\n  * Dynamic scaling: This involves adding more resources dynamically as the amount of data ingested increases. While cloud-based solutions can provision these resources automatically, the self-hosted equivalent involves adding more instances for horizontal scalability with minimal disruption or additional configuration.\n  * Absolute scale limits: These can be hard limits such as the number of concurrent workflows, maximum file sizes, maximum number of actions per playbook.\n  * Support considerations: Smaller vendors with a scalable product may struggle to provide in-house enterprise-grade support for very large deployments, which includes support tickets and incident management. Furthermore, they may not have yet developed partnerships with third-party services providers that can help with large operations.\n  * Pricing considerations: Some vendor pricing mechanisms may lock customers out of large or growing deployments, which forces organizations to compromise on the types of data they ingest, store, or analyze.\n\n\n\nFor AI models hosted in the cloud by third parties (i.e. ChatGPT), main scalability concerns revolve around pricing, where the cost of large numbers of requests outweigh any efficiencies gained by implementing AI.\n\nFor self-hosted AI models scalability, considerations include maximum data input volume, maximum output, rate of errors, and mean time to get an output. Compute infrastructure is the main factor for scaling AI models, where the underlying hardware must have enough processing power to support requests and also support for horizontal scaling. When choosing to host the model in the cloud, horizontal scalability is not generally a concern, but with on-premises deployments there is a hard limit on the number of servers available at any given time. The scale of deployment directly correlates with the model's size and complexity, with smaller models (1-3 billion parameters) requiring substantially less compute infrastructure compared to large language models with 30-70 billion parameters.\n\nSelf-hosted models also have some control over response times. Model choice once again may have an impact, where small models can respond in 100-500 milliseconds, while larger models might require 1-5 seconds per inference. This can further be tuned with techniques such as model quantization, distributed inference, batch processing, intelligent caching, and hardware acceleration. \n\nIn instances such as n8nâ€™s AI Starter Kit, using Kubernetes to orchestrate the containers running the workflow tool and large language model can help deliver horizontal scalability based on request demand. Besides horizontal scalability, using Kubernetes can also help with load balancing and request handling.\n\n### GPU Requirements for Self-hosting\n\nTo calculate how much GPU memory is required to run an LLM on-premises, you can use the following formula:\n\n![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXd2O6INZEWhxEDKmagzxn_bXNWUOhlooQskVUbyDSeC4bjRDwmXi0f7jzzNHND5MyjBygDMgIm_Szn3A4qA0A4j49FJIZt_fMP_4jqCWAkdUTHyOewzcahtCyQBExFYDEVnDXw_oA?key=5Bu_i2rT_sZgQ-_nz7V586fH)\n\n_Source: https://ksingh7.medium.com/calculate-how-much-gpu-memory-you-need-to-serve-any-llm-67301a844f21_\n\nWhere:\n\n  * M = GPU memory in GB\n  * P = number of parameters\n  * 4B = 4 bytes per parameter\n  * 32 = 32 bits for the 4 bytes\n  * Q = number of bits used for loading the model\n  * 1.2 = 20% overhead allocation\n\n\n\nAs an example, applying this formula for Llama-3.1-70b, which has 70 Billion Parameters, we get a total of 168GB or GPU memory. This would require  two NVIDIA A-100 80GB memory models to run.\n\nIn instances where computational power for hosting AI is limited, you can reduce the compute and memory costs using [_quantization_](https://huggingface.co/docs/optimum/en/concept_guides/quantization?ref=blog.n8n.io#quantization). It is a technique that enables the AI model to operate with less memory storage, consume less energy (in theory), and perform operations like matrix multiplication faster. It does so by representing the weights and activations with low-precision data types like 8-bit integers instead of the usual 32-bit floating point.\n\nApplying this quantization technique to the above calculation for Llama 70B, using float16 precision instead of float32 would cut the memory requirement in half.\n\n## Authorization and Authentication\n\nAs AI-enhanced workflows can have multiple business owners and access various data resources, authorization and role based access controls can help segregate access to reduce privacy concerns and attack surface.\n\n**User access controls** refers to how permissions are granted and managed to administrators, developers, and non-developers that have access to the workflow designer.\n\n**Service access controls** refers to how the workflow automation tool can access other services and databases, such as providing API keys or JWT tokens, allowing access only through specific ports, requiring encryption, and the like.\n\n## Monitoring and Error Handling\n\nWorkflow automation tools can implement monitoring capabilities for each step or action that takes place within the workflow, including any AI agents. These can report on execution times, errors, API codes, and generate logs. \n\nTo monitor the performance of AI agents, tools can track performance against:\n\n  * Latency - How quickly an LLM can provide a response after receiving an input. Faster response times enhance user satisfaction and engagement.\n  * Throughput - The number of tasks or queries an LLM can handle within a given time frame, for assessing the model's capability to serve multiple requests simultaneously. This is important for scalability and performance in production environments.\n  * Resource Utilization - How efficiently an LLM uses computational resources, such as CPU and GPU memory. Optimal resource utilization ensures that the model runs efficiently, enabling cost-effective scaling and sustainability in deployment.\n\n\n\nBased on the reported metrics, organizations can implement multiple layers of validation, including:\n\n  * Input preprocessing and sanitization\n  * Confidence threshold monitoring\n  * Fallback mechanisms for low-confidence predictions, such as implementing human agents in the loop\n\n\n\n# n8n for Building AI-based Workflows\n\nSecure and privacy-first AI is a core strategy for n8n, so we have developed a range of features that make use of AI. At n8n, we use AI-enhanced workflows internally for a variety of use cases, which include:\n\n  * Battlecard bot\n  * Our own AI assistant for errors\n  * Template reviews \n  * Classifying and assigning bugs \n  * Classifying forum posts\n\n\n\nWe are also building a library of community-built AI content, which[ _you can explore here_](https://n8n.io/workflows/categories/ai/?ref=blog.n8n.io). We encourage members of the community to [_become creators_](https://n8n.notion.site/n8n-Creator-hub-7bd2cbe0fce0449198ecb23ff4a2f76f?ref=blog.n8n.io) and submit their own templates.\n\n## Subscribe to n8n newsletter\n\nGet the best, coolest, and latest in automation and low-code delivered to your inbox each week. \n\nSomething went wrong. Please try again later.\n\nSubscribed!\n\nSubscribe\n\n[](https://twitter.com/intent/tweet?text=AI%20Workflows%20for%20the%20Cautious%20Enterprise&url=https://blog.n8n.io/ai-workflows-for-the-cautious-enterprise/) [](https://www.facebook.com/sharer/sharer.php?u=https://blog.n8n.io/ai-workflows-for-the-cautious-enterprise/)\n",
  "crawled_at": "2025-05-28T10:46:36.706408"
}