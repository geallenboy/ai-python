{
  "url": "https://blog.n8n.io/llamaindex-vs-langchain/",
  "title": "LlamaIndex vs LangChain: Which RAG tool is right for you?",
  "excerpt": "LlamaIndex vs LangChain: Which is best for your LLM application? This guide compares these frameworks, highlighting their strengths and limitations for RAG use cases. We also introduce n8n as a low-code alternative that combines LangChain's flexibility with a user-friendly interface.",
  "thumbnail": "https://blog.n8n.io/content/images/size/w1200/2025/03/llamaindex-vs-langchain2-8--1-.png",
  "tags": [
    "AI",
    "Guide"
  ],
  "html": "<p>Retrieval-Augmented Generation (RAG) is essential for building LLM applications that can access and reason over up-to-date, proprietary, or domain-specific information. By using RAG, LLMs can overcome the limitations of relying solely on their pre-trained knowledge.</p><p>Two popular frameworks for building RAG chatbots are LlamaIndex and LangChain.</p><p>This article provides a comparative analysis of LlamaIndex and LangChain, highlighting their core strengths, key differences, and ideal use cases. We will then introduce <a href=\"https://n8n.io/ai/?ref=blog.n8n.io\" rel=\"noopener\" target=\"_blank\">n8n</a> as <a href=\"https://blog.n8n.io/langchain-alternatives/\" rel=\"noreferrer\">a LangChain alternative</a>. n8n is a powerful, low-code automation software, particularly well-suited for RAG workflows that require extensive integrations and visual workflow design.</p><p>So let's get straight into it!</p><h2 id=\"llamaindex-vs-langchain-a-comparative-analysis\">LlamaIndex vs LangChain: A comparative analysis</h2><p>Here's a comparative table to summarize the key differences between LlamaIndex and LangChain:</p><table>\n    <tbody><tr>\n        <td>Criteria</td>\n        <td><strong>LlamaIndex</strong></td>\n        <td><strong>LangChain</strong></td>\n    </tr>\n    <tr>\n        <td><strong>Primary focus</strong></td>\n        <td>Data connection, indexing, <br>and querying for RAG.</td>\n        <td>Building and orchestrating <br>complex LLM workflows, <br>including agents and chains.</td>\n    </tr>\n    <tr>\n        <td><strong>Ease of use</strong></td>\n        <td>Easier to learn and use, <br>especially for beginners. <br>High-level API simplifies <br>common tasks.</td>\n        <td>Steeper learning curve. <br>Requires a deeper understanding <br>of LLM concepts.</td>\n    </tr>\n    <tr>\n        <td><strong>Data ingestion</strong></td>\n        <td>Extensive data connectors <br>through LlamaHub (APIs, PDFs, <br>databases, etc.). Streamlined <br>data loading and indexing.</td>\n        <td>Supports data loading, <br>but focus is more on <br>data transformation within <br>the pipeline.</td>\n    </tr>\n    <tr>\n        <td><strong>Querying</strong></td>\n        <td>Sophisticated querying <br>capabilities, including <br>subqueries and multi-document <br>summarization. Optimized <br>for retrieval.</td>\n        <td>Flexible querying, <br>but often requires more <br>manual configuration.</td>\n    </tr>\n    <tr>\n        <td><strong>Flexibility</strong></td>\n        <td>Less flexible, more <br>opinionated. Good for <br>standard RAG use cases.</td>\n        <td>Highly flexible and modular. <br>Allows swapping LLMs, <br>customizing prompts, <br>and building complex chains <br>using various LangChain <br>chain types.</td>\n    </tr>\n    <tr>\n        <td><strong>Extensibility</strong></td>\n        <td>Primarily through LlamaHub <br>and custom data connectors.</td>\n        <td>Highly extensible through <br>custom chains, agents, <br>and tools.</td>\n    </tr>\n    <tr>\n        <td><strong>Customization</strong></td>\n        <td>Some customization options, <br>but less than LangChain.</td>\n        <td>Highly customisable and <br>great degree of control.</td>\n    </tr>\n    <tr>\n        <td><strong>Free for commercial use?</strong></td>\n        <td>Yes</td>\n        <td>Yes</td>\n    </tr>\n    <tr>\n        <td><strong>Use cases</strong></td>\n        <td>RAG chatbots, document <br>Q&amp;A, knowledge base <br>querying, data augmentation.</td>\n        <td>Complex reasoning systems, <br>multi-agent applications, <br>applications requiring <br>integration with multiple <br>tools and APIs. You can <br>also use LangChain for <br>RAG workflows.</td>\n    </tr>\n    <tr>\n        <td><strong>Repository</strong></td>\n        <td><a href=\"https://github.com/run-llama/llama_index?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\">LlamaIndex GitHub</a></td>\n        <td><a href=\"https://github.com/langchain-ai/langchain?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\">LangChain GitHub</a></td>\n    </tr>\n</tbody></table><h2 id=\"which-is-better-llamaindex-or-langchain\">Which is better: LlamaIndex or LangChain?</h2><p>Both LlamaIndex and LangChain are powerful frameworks for building LLM-powered applications, particularly those leveraging RAG.</p><p>However, when considering LangChain vs LlamaIndex, they have distinct strengths that make them better suited for different use cases.</p><h3 id=\"ease-of-use\">Ease of use</h3><p>LlamaIndex generally has a gentler learning curve. Its high-level API and focus on data connection and querying make it easier to get started, especially for developers new to LLMs. For example, if you need to quickly build a RAG chatbot that answers questions over a collection of PDF documents, LlamaIndex's data loaders and index structures simplify this process considerably.</p><p>LangChain, while more powerful, has a steeper learning curve. Its modularity and flexibility require a deeper understanding of LLM concepts and the various components involved.</p><h3 id=\"data-handling-and-indexing\">Data handling and indexing</h3><p>LlamaIndex excels in this area. It provides various indexing strategies optimized for different types of data and retrieval needs. For instance, you can easily load data from APIs, databases, and local files, and choose between vector, tree, or keyword-based indexes.</p><p>LlamaIndex has a user-friendly approach to data ingestion, primarily through <a href=\"https://llamahub.ai/?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>LlamaHub</u></a>. This central repository offers a wide array of data connectors for common sources like APIs, PDFs, documents, and databases. This extensive and easily accessible collection of connectors significantly simplifies and speeds up the process of integrating diverse data sources into your RAG pipeline.</p><p>LangChain, in contrast, does not enforce a specific indexing approach but instead allows users to structure their own pipelines based on their preferred tools. This flexibility makes it suitable for developers who want more control over their retrieval strategies, though it may require additional setup compared to LlamaIndexâ€™s built-in indexing mechanisms.</p><p>LangChain also supports data loading with its own set of document loaders. While it might not have a single, unified hub like LlamaHub, LangChain's data loaders are flexible and can be customized. This approach provides greater control over the data loading process, which is advantageous for developers who need to implement highly specific or custom data ingestion logic.</p><h3 id=\"flexibility\">Flexibility</h3><p>LangChain offers significantly more flexibility and control. Its modular architecture allows you to swap out different LLMs, customize prompt templates, and chain together multiple tools and agents. This is crucial if you're building complex applications, such as a multi-step reasoning system or an application that needs to interact with multiple external services.</p><p>LlamaIndex, while offering some customization, is more opinionated in its approach, prioritizing ease of use over fine-grained control.</p><h3 id=\"querying-capabilities\">Querying capabilities</h3><p>LlamaIndex is optimized for sophisticated querying within RAG systems. It supports advanced querying techniques like subqueries (querying across multiple documents or indexes) and multi-document summarization.</p><p>LangChain offers flexible querying options, but often requires more manual configuration to achieve advanced querying patterns. You have the building blocks to create complex query chains, but you need to assemble them yourself.</p><h3 id=\"memory-management\">Memory management</h3><p>LlamaIndex offers basic context retention capabilities, allowing for simple conversational interactions. This is sufficient for straightforward RAG chatbots where maintaining short-term conversation history is needed for context.</p><p>LangChain's advanced memory management is crucial for building sophisticated conversational AI applications that require extensive context retention, understanding of conversation history, and complex multi-turn reasoning.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">ðŸ’¡</div><div class=\"kg-callout-text\">If your primary need is to quickly connect LLMs to your data and build RAG applications with minimal complexity, LlamaIndex is an excellent choice. If you need maximum flexibility, control, and the ability to build complex, multi-step LLM workflows, LangChain is the more powerful option.</div></div><h2 id=\"can-i-use-langchain-and-llamaindex-together\">Can I use LangChain and LlamaIndex together?</h2><p>Yes, you can absolutely use LangChain and LlamaIndex together! In fact, combining them can be a powerful way to leverage the strengths of each framework. Here's why and how:</p><h3 id=\"use-llamaindex-for-data-management\">Use LlamaIndex for data management</h3><p>You can use LlamaIndex's powerful data connectors and indexing capabilities to efficiently load, structure, and index your data from various sources. This creates a robust knowledge base for your RAG system. As we mentioned earlier, while LangChain connectors are also available, LlamaIndex excels in this area.</p><h3 id=\"use-langchain-for-orchestration\">Use LangChain for orchestration</h3><p>Utilize LangChain's chains, agents, and LangChain tools to build the overall logic and workflow of your application. Integrate LlamaIndex's query engine as a tool within your LangChain workflow, allowing you to retrieve relevant information from your indexed data.</p><h2 id=\"what-are-the-limitations-of-llamaindex-and-langchain\">What are the limitations of LlamaIndex and LangChain?</h2><h3 id=\"llamaindex-limitations\">LlamaIndex limitations</h3><ul><li>Primarily focused on data retrieval, making it less suitable for:<ul><li>Highly complex LLM applications with intricate, multi-step workflows</li><li>Applications requiring interactions with numerous external services</li></ul></li><li>Supports only basic context retention, which may be insufficient for:<ul><li>Applications needing extensive conversational memory</li><li>Complex reasoning across multiple turns.</li></ul></li></ul><h3 id=\"langchain-limitations\">LangChain limitations</h3><ul><li>High flexibility comes with:<ul><li>A steep learning curve, especially for developers new to LLMs</li><li>More intricate initial setup and configuration</li></ul></li><li>Increased debugging and maintenance overhead, particularly for sophisticated applications</li><li>Frequent breaking changes between versions, often requiring ongoing code adjustments.</li></ul><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">ðŸ’¡</div><div class=\"kg-callout-text\">Unlike LlamaIndexâ€™s focus on data retrieval and LangChainâ€™s steep learning curve, n8n provides <b><strong style=\"white-space: pre-wrap;\">scalable orchestration</strong></b> with built-in connectors and <b><strong style=\"white-space: pre-wrap;\">easier maintenance</strong></b> across evolving LLM applications.</div></div><h2 id=\"an-alternative-to-llamaindex-or-langchain-n8n\">An alternative to LlamaIndex or LangChain: n8n</h2><p>While LangChain and LlamaIndex are powerful tools for building LLM applications, they primarily focus on a code-centric approach.</p><p>n8n offers a compelling alternative by providing a low-code environment that seamlessly integrates with LangChain. This means you can get the power and flexibility of LangChain without the complexity of managing its underlying code directly.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">ðŸ’¡</div><div class=\"kg-callout-text\">By providing an abstraction layer over LangChain, n8n simplifies the development process while retaining the core flexibility that LangChain offers.</div></div><h3 id=\"why-choose-n8n-over-llamaindex-and-langchain\">Why choose n8n over LlamaIndex and LangChain?</h3><ul><li>You want LangChain's power, but prefer <strong>low-code</strong>! <a href=\"https://n8n.io/integrations/agent/?ref=blog.n8n.io\" rel=\"noopener\" target=\"_blank\">n8n's AI Agent node</a> allows you to leverage LangChain's features, such as agents and memory, without writing complex Python code.</li><li><strong>Extensive integrations:</strong> <a href=\"https://n8n.io/integrations/?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>n8n boasts 400+ integrations with various apps and services</u></a> such as databases, CRMs, marketing platforms, communication tools, and virtually any service with an API. This is a significant advantage over LangChain and LlamaIndex, which primarily focus on LLM interactions. With n8n itâ€™s also very easy to integrate <a href=\"https://n8n.io/integrations/?q=vector&amp;ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>vector databases</u></a>, such as Pinecone, with LangChain.</li><li><strong>Visual workflow builder:</strong> n8n provides a user-friendly, <a href=\"https://n8n.io/features/?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>drag-and-drop interface for building workflows</u></a>, making it accessible to both technical and non-technical users. This can be a significant advantage over the code-heavy approach of LangChain and LlamaIndex.</li><li><strong>Native AI capabilities:</strong> <a href=\"https://docs.n8n.io/advanced-ai/langchain/langchain-n8n/?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>n8n has built-in support for LangChain</u></a>, including integrations with tools like <a href=\"https://blog.n8n.io/local-llm/\"><u>LangChain Ollama for local LLM usage</u></a>, allowing you to <a href=\"https://n8n.io/ai/?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>incorporate LLM interactions into your workflows</u></a>. You configure your LangChain chains and agents within the node, and n8n handles execution and integration with the rest of your workflow. This way, you leverage LangChain's advanced features (agents, memory, custom chains) without writing intricate Python.</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://blog.n8n.io/content/images/2025/02/n8n_ai_agent.webp\" class=\"kg-image lightense-target\" alt=\"n8n example of AI Agent workflow\" loading=\"lazy\" width=\"1985\" height=\"1200\" srcset=\"https://blog.n8n.io/content/images/size/w600/2025/02/n8n_ai_agent.webp 600w, https://blog.n8n.io/content/images/size/w1000/2025/02/n8n_ai_agent.webp 1000w, https://blog.n8n.io/content/images/size/w1600/2025/02/n8n_ai_agent.webp 1600w, https://blog.n8n.io/content/images/2025/02/n8n_ai_agent.webp 1985w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">n8n example of AI Agent workflow</span></figcaption></figure><ul><li><strong>Rapid prototyping and iteration:</strong> you can quickly experiment with different RAG approaches, connect to various data sources, and test different LLM configurations.</li><li><strong>Simplified configuration:</strong> While you can use advanced LangChain features, the visual interface simplifies configuration using the built-in LangChain templates. You set up prompts, input variables, and output handling within the LangChain node's settings.</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://blog.n8n.io/content/images/2025/02/n8n_ai_node_configuration.webp\" class=\"kg-image lightense-target\" alt=\"Configuring the AI Agent n8n node\" loading=\"lazy\" width=\"1154\" height=\"1292\" srcset=\"https://blog.n8n.io/content/images/size/w600/2025/02/n8n_ai_node_configuration.webp 600w, https://blog.n8n.io/content/images/size/w1000/2025/02/n8n_ai_node_configuration.webp 1000w, https://blog.n8n.io/content/images/2025/02/n8n_ai_node_configuration.webp 1154w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Configuring the AI Agent n8n node</span></figcaption></figure><ul><li><strong>Hybrid approach:</strong> you can combine the ease of visual workflow design with the power of custom code using <a href=\"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.code/?ref=blog.n8n.io\" rel=\"noopener\" target=\"_blank\">n8n's LangChain code node</a>.</li></ul><h2 id=\"examples-of-rag-workflows-built-with-n8n\">Examples of RAG workflows built with n8n</h2><p>Let's explore some real-world workflow examples that illustrate the versatility of n8n for RAG and Agentic AI!</p><h3 id=\"ai-powered-rag-workflow-for-stock-earnings-report-analysis\">AI-Powered RAG workflow for stock earnings report analysis</h3>\n<!--kg-card-begin: html-->\n<script>\n  workflowBanner(2741, document.currentScript);\n</script><div class=\"workflow\">\n      <div class=\"workflow-content\">\n        <div class=\"workflow-nodes\">\n          <img src=\"data:image/svg+xml;base64,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\" alt=\"Pinecone Vector Store\" title=\"Pinecone Vector Store\"><img src=\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB2aWV3Qm94PSIwIDAgNDggNDgiPjxkZWZzPjxwYXRoIGlkPSJhIiBkPSJNNDQuNSAyMEgyNHY4LjVoMTEuOEMzNC43IDMzLjkgMzAuMSAzNyAyNCAzN2MtNy4yIDAtMTMtNS44LTEzLTEzczUuOC0xMyAxMy0xM2MzLjEgMCA1LjkgMS4xIDguMSAyLjlsNi40LTYuNEMzNC42IDQuMSAyOS42IDIgMjQgMiAxMS44IDIgMiAxMS44IDIgMjRzOS44IDIyIDIyIDIyYzExIDAgMjEtOCAyMS0yMiAwLTEuMy0uMi0yLjctLjUtNCIvPjwvZGVmcz48Y2xpcFBhdGggaWQ9ImIiPjx1c2UgeGxpbms6aHJlZj0iI2EiIG92ZXJmbG93PSJ2aXNpYmxlIi8+PC9jbGlwUGF0aD48cGF0aCBmaWxsPSIjRkJCQzA1IiBkPSJNMCAzN1YxMWwxNyAxM3oiIGNsaXAtcGF0aD0idXJsKCNiKSIvPjxwYXRoIGZpbGw9IiNFQTQzMzUiIGQ9Im0wIDExIDE3IDEzIDctNi4xTDQ4IDE0VjBIMHoiIGNsaXAtcGF0aD0idXJsKCNiKSIvPjxwYXRoIGZpbGw9IiMzNEE4NTMiIGQ9Im0wIDM3IDMwLTIzIDcuOSAxTDQ4IDB2NDhIMHoiIGNsaXAtcGF0aD0idXJsKCNiKSIvPjxwYXRoIGZpbGw9IiM0Mjg1RjQiIGQ9Ik00OCA0OCAxNyAyNGwtNC0zIDM1LTEweiIgY2xpcC1wYXRoPSJ1cmwoI2IpIi8+PC9zdmc+\" alt=\"Embeddings Google Gemini\" title=\"Embeddings Google Gemini\"><img src=\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI3NjgiIGhlaWdodD0iMTAyNCI+PHBhdGggZmlsbD0iIzdEN0Q4NyIgZD0iTTAgOTYwVjY0aDU3NmwxOTIgMTkydjcwNHptNzA0LTY0MEw1MTIgMTI4SDY0djc2OGg2NDB6TTMyMCA1MTJIMTI4VjI1NmgxOTJ6bS02NC0xOTJoLTY0djEyOGg2NHptMCA0NDhoNjR2NjRIMTI4di02NGg2NFY2NDBoLTY0di02NGgxMjh6bTI1Ni0zMjBoNjR2NjRIMzg0di02NGg2NFYzMjBoLTY0di02NGgxMjh6bTY0IDM4NEgzODRWNTc2aDE5MnptLTY0LTE5MmgtNjR2MTI4aDY0eiIvPjwvc3ZnPg==\" alt=\"Default Data Loader\" title=\"Default Data Loader\">\n          <i class=\"fa fa-grip-lines-vertical\" aria-hidden=\"true\"></i>\n        \n          <i class=\"fa fa-sync\" aria-hidden=\"true\" style=\"color:#007755\"></i>\n        \n          <span>+5</span>\n        </div>\n        <div class=\"workflow-details\">\n          <p class=\"workflow-details-name\">\n            <a href=\"https://n8n.io/workflows/2741-ai-powered-rag-workflow-for-stock-earnings-report-analysis/\" class=\"blog-banner-workflow\">AI-Powered RAG Workflow For Stock Earnings Report Analysis</a>\n          </p>\n          <p class=\"workflow-details-stats\">by mihailtd</p>\n        </div>\n      </div>\n      <a href=\"https://n8n.io/workflows/2741-ai-powered-rag-workflow-for-stock-earnings-report-analysis/\" class=\"global-button blog-banner-workflow\">\n        Use this workflow\n      </a>\n    </div>\n<!--kg-card-end: html-->\n<p>This n8n workflow creates a financial analysis tool that generates reports on a company's quarterly earnings using <a href=\"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatopenai/?ref=blog.n8n.io\" rel=\"noopener\" target=\"_blank\">OpenAI GPT-4o-mini</a>, <a href=\"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatgooglegemini/?ref=blog.n8n.io\" rel=\"noopener\" target=\"_blank\">Google's Gemini AI</a>, and <a href=\"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstorepinecone/?ref=blog.n8n.io\" rel=\"noopener\" target=\"_blank\">Pinecone's vector search</a>. The workflow fetches earnings PDFs, parses them, generates embeddings, and stores them in Pinecone. An AI agent orchestrates the process, using Pinecone and LLMs to analyze data and generate the report, which is then saved to <a href=\"https://docs.n8n.io/integrations/builtin/app-nodes/n8n-nodes-base.googledrive/?ref=blog.n8n.io\" rel=\"noopener\" target=\"_blank\">Google Drive</a>.</p><h3 id=\"complete-business-whatsapp-ai-powered-rag-chatbot-using-openai\">Complete business WhatsApp AI-powered RAG chatbot using OpenAI</h3>\n<!--kg-card-begin: html-->\n<script>\n  workflowBanner(2845, document.currentScript);\n</script><div class=\"workflow\">\n      <div class=\"workflow-content\">\n        <div class=\"workflow-nodes\">\n          <img src=\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCI+PHBhdGggZmlsbD0iIzM3NDc0ZiIgZD0iTTM1IDM3Yy0yLjIgMC00LTEuOC00LTRzMS44LTQgNC00IDQgMS44IDQgNC0xLjggNC00IDQiLz48cGF0aCBmaWxsPSIjMzc0NzRmIiBkPSJNMzUgNDNjLTMgMC01LjktMS40LTcuOC0zLjdsMy4xLTIuNWMxLjEgMS40IDIuOSAyLjMgNC43IDIuMyAzLjMgMCA2LTIuNyA2LTZzLTIuNy02LTYtNmMtMSAwLTIgLjMtMi45LjdsLTEuNyAxTDIzLjMgMTZsMy41LTEuOSA1LjMgOS40YzEtLjMgMi0uNSAzLS41IDUuNSAwIDEwIDQuNSAxMCAxMFM0MC41IDQzIDM1IDQzIi8+PHBhdGggZmlsbD0iIzM3NDc0ZiIgZD0iTTE0IDQzQzguNSA0MyA0IDM4LjUgNCAzM2MwLTQuNiAzLjEtOC41IDcuNS05LjdsMSAzLjlDOS45IDI3LjkgOCAzMC4zIDggMzNjMCAzLjMgMi43IDYgNiA2czYtMi43IDYtNnYtMmgxNXY0SDIzLjhjLS45IDQuNi01IDgtOS44IDgiLz48cGF0aCBmaWxsPSIjZTkxZTYzIiBkPSJNMTQgMzdjLTIuMiAwLTQtMS44LTQtNHMxLjgtNCA0LTQgNCAxLjggNCA0LTEuOCA0LTQgNCIvPjxwYXRoIGZpbGw9IiMzNzQ3NGYiIGQ9Ik0yNSAxOWMtMi4yIDAtNC0xLjgtNC00czEuOC00IDQtNCA0IDEuOCA0IDQtMS44IDQtNCA0Ii8+PHBhdGggZmlsbD0iI2U5MWU2MyIgZD0ibTE1LjcgMzQtMy40LTIgNS45LTkuN2MtMi0xLjktMy4yLTQuNS0zLjItNy4zIDAtNS41IDQuNS0xMCAxMC0xMHMxMCA0LjUgMTAgMTBjMCAuOS0uMSAxLjctLjMgMi41bC0zLjktMWMuMS0uNS4yLTEgLjItMS41IDAtMy4zLTIuNy02LTYtNnMtNiAyLjctNiA2YzAgMi4xIDEuMSA0IDIuOSA1LjFsMS43IDF6Ii8+PC9zdmc+\" alt=\"Respond to Webhook\" title=\"Respond to Webhook\">\n          <i class=\"fa fa-robot\" aria-hidden=\"true\" style=\"color:#404040\"></i>\n        \n          <i class=\"fa fa-sticky-note\" aria-hidden=\"true\" style=\"color:#FFD233\"></i>\n        <img src=\"data:image/svg+xml;base64,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\" alt=\"OpenAI Chat Model\" title=\"OpenAI Chat Model\">\n          <i class=\"fa fa-mouse-pointer\" aria-hidden=\"true\" style=\"color:#909298\"></i>\n        \n          <span>+5</span>\n        </div>\n        <div class=\"workflow-details\">\n          <p class=\"workflow-details-name\">\n            <a href=\"https://n8n.io/workflows/2845-complete-business-whatsapp-ai-powered-rag-chatbot-using-openai/\" class=\"blog-banner-workflow\">Complete business WhatsApp AI-Powered RAG Chatbot using OpenAI</a>\n          </p>\n          <p class=\"workflow-details-stats\">by n3witalia</p>\n        </div>\n      </div>\n      <a href=\"https://n8n.io/workflows/2845-complete-business-whatsapp-ai-powered-rag-chatbot-using-openai/\" class=\"global-button blog-banner-workflow\">\n        Use this workflow\n      </a>\n    </div>\n<!--kg-card-end: html-->\n<p>This workflow allows you to create an AI-powered chatbot for WhatsApp Business that uses RAG to provide accurate and relevant information to customers. The workflow sets up <a href=\"https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.webhook/?ref=blog.n8n.io\" rel=\"noopener\" target=\"_blank\">webhooks </a>to handle incoming messages, processes them, and utilizes an AI agent with a predefined system message to ensure appropriate responses. It accesses a knowledge base stored in <a href=\"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstoreqdrant/?ref=blog.n8n.io\" rel=\"noopener\" target=\"_blank\">Qdrant </a>(a vector database like Pinecone), generates responses using OpenAI's GPT model, and sends them back to the user.</p><h3 id=\"effortless-email-management-with-ai-powered-summarization-review\">Effortless email management with AI-powered summarization &amp; review</h3>\n<!--kg-card-begin: html-->\n<script>\n  workflowBanner(2862, document.currentScript);\n</script><div class=\"workflow\">\n      <div class=\"workflow-content\">\n        <div class=\"workflow-nodes\">\n          \n          <i class=\"fa fa-inbox\" aria-hidden=\"true\" style=\"color:#44AA22\"></i>\n        <img src=\"data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNDAiIGhlaWdodD0iNDAiIHZpZXdCb3g9IjAgMCA0MCA0MCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTM3LjExNjQgMzIuNjE4N0gyLjg4MzU3QzIuNTA0OSAzMi42MTg3IDIuMTI5OTMgMzIuNTQ0MSAxLjc4MDA4IDMyLjM5OTJDMS40MzAyMyAzMi4yNTQzIDEuMTEyMzQgMzIuMDQxOSAwLjg0NDU3OSAzMS43NzQxQzAuNTc2ODE1IDMxLjUwNjQgMC4zNjQ0MTIgMzEuMTg4NSAwLjIxOTQ5OSAzMC44Mzg2QzAuMDc0NTg1OCAzMC40ODg4IDAgMzAuMTEzOCAwIDI5LjczNTFWMTAuODgzNkMwIDEwLjExODggMC4zMDM4MDQgOS4zODUzNSAwLjg0NDU3OSA4Ljg0NDU4QzEuMzg1MzUgOC4zMDM4IDIuMTE4OCA4IDIuODgzNTcgOEgzNy4xMTY0QzM3LjQ5NTEgOCAzNy44NzAxIDguMDc0NTkgMzguMjE5OSA4LjIxOTVDMzguNTY5OCA4LjM2NDQxIDM4Ljg4NzcgOC41NzY4MSAzOS4xNTU0IDguODQ0NThDMzkuNDIzMiA5LjExMjM0IDM5LjYzNTYgOS40MzAyMyAzOS43ODA1IDkuNzgwMDhDMzkuOTI1NCAxMC4xMjk5IDQwIDEwLjUwNDkgNDAgMTAuODgzNlYyOS43MzE4QzQwLjAwMDIgMzAuMTEwNiAzOS45MjU4IDMwLjQ4NTggMzkuNzgxIDMwLjgzNThDMzkuNjM2MiAzMS4xODU5IDM5LjQyMzggMzEuNTAzOSAzOS4xNTYgMzEuNzcxOUMzOC44ODgyIDMyLjAzOTggMzguNTcwMyAzMi4yNTI0IDM4LjIyMDMgMzIuMzk3NEMzNy44NzAzIDMyLjU0MjQgMzcuNDk1MiAzMi42MTg3IDM3LjExNjQgMzIuNjE4N1pNOS42MTQxMyAyNi44NDgyVjE5LjM0NzZMMTMuNDYxMSAyNC4xNTYzTDE3LjMwNjQgMTkuMzQ3NlYyNi44NDgySDIxLjE1MzRWMTMuNzcyMUgxNy4zMDY0TDEzLjQ2MTEgMTguNTgwOUw5LjYxNDEzIDEzLjc3MjFINS43NjcxNVYyNi44NTE2TDkuNjE0MTMgMjYuODQ4MlpNMzUuMzg2MyAyMC4zMDk0SDMxLjUzOTNWMTMuNzcwNUgyNy42OTRWMjAuMzA5NEgyMy44NDdMMjkuNjE1OCAyNy4wNDE2TDM1LjM4NjMgMjAuMzA5NFoiIGZpbGw9IiMzODM4MzkiLz4KPC9zdmc+Cg==\" alt=\"Markdown\" title=\"Markdown\">\n          <i class=\"fa fa-envelope\" aria-hidden=\"true\" style=\"color:#00bb88\"></i>\n        <img src=\"data:image/svg+xml;base64,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\" alt=\"Qdrant Vector Store\" title=\"Qdrant Vector Store\"><img src=\"data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNDAiIGhlaWdodD0iNDAiIHZpZXdCb3g9IjAgMCA0MCA0MCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTM2Ljg2NzEgMTYuMzcxOEMzNy43NzQ2IDEzLjY0OCAzNy40NjIxIDEwLjY2NDIgMzYuMDEwOCA4LjE4NjYxQzMzLjgyODIgNC4zODY1MyAyOS40NDA3IDIuNDMxNDkgMjUuMTU1NiAzLjM1MTUxQzIzLjI0OTMgMS4yMDM5NiAyMC41MTA1IC0wLjAxNzMxNDggMTcuNjM5MiAwLjAwMDE4NTUzM0MxMy4yNTkxIC0wLjAwOTgxNDY4IDkuMzcyNzMgMi44MTAyNSA4LjAyNTIgNi45Nzc4M0M1LjIxMTM5IDcuNTU0MSAyLjc4MjU4IDkuMzE1MzggMS4zNjEzIDExLjgxMTdDLTAuODM3NDkzIDE1LjYwMTggLTAuMzM2MjMyIDIwLjM3OTQgMi42MDEzMyAyMy42Mjk0QzEuNjkzODEgMjYuMzUzMiAyLjAwNjMyIDI5LjMzNzEgMy40NTc2IDMxLjgxNDZDNS42NDAxNSAzNS42MTQ3IDEwLjAyNzcgMzcuNTY5NyAxNC4zMTI4IDM2LjY0OTdDMTYuMjE3OSAzOC43OTczIDE4Ljk1NzkgNDAuMDE4NSAyMS44MjkyIDM5Ljk5OThDMjYuMjExOCA0MC4wMTEgMzAuMDk5NCAzNy4xODg1IDMxLjQ0NjkgMzMuMDE3MUMzNC4yNjA4IDMyLjQ0MDkgMzYuNjg5NiAzMC42Nzk2IDM4LjExMDggMjguMTgzM0M0MC4zMDcxIDI0LjM5MzIgMzkuODA0NiAxOS42MTk0IDM2Ljg2ODMgMTYuMzY5M0wzNi44NjcxIDE2LjM3MThaTTIxLjgzMTcgMzcuMzg2QzIwLjA3OCAzNy4zODg1IDE4LjM3OTIgMzYuNzc0NyAxNy4wMzI5IDM1LjY1MDlDMTcuMDk0MSAzNS42MTg0IDE3LjIwMDQgMzUuNTU5NyAxNy4yNjkxIDM1LjUxNzJMMjUuMjM0MyAzMC45MTcxQzI1LjY0MTggMzAuNjg1OCAyNS44OTE4IDMwLjI1MjEgMjUuODg5MyAyOS43ODMzVjE4LjU1NDNMMjkuMjU1NyAyMC40OTgxQzI5LjI5MTkgMjAuNTE1NiAyOS4zMTU3IDIwLjU1MDYgMjkuMzIwNyAyMC41OTA2VjI5Ljg4OTZDMjkuMzE1NyAzNC4wMjQ3IDI1Ljk2NjggMzcuMzc3MiAyMS44MzE3IDM3LjM4NlpNNS43MjY0IDMwLjUwNzFDNC44NDc2MyAyOC45ODk2IDQuNTMxMzcgMjcuMjEwOCA0LjgzMjYzIDI1LjQ4NDVDNC44OTEzOCAyNS41MTk1IDQuOTk1MTMgMjUuNTgzMiA1LjA2ODg4IDI1LjYyNTdMMTMuMDM0MSAzMC4yMjU4QzEzLjQzNzggMzAuNDYyMSAxMy45Mzc4IDMwLjQ2MjEgMTQuMzQyOCAzMC4yMjU4TDI0LjA2NjggMjQuNjEwN1YyOC40OTgzQzI0LjA2OTMgMjguNTM4MyAyNC4wNTA1IDI4LjU3NyAyNC4wMTkzIDI4LjYwMkwxNS45Njc5IDMzLjI1MDlDMTIuMzgxNSAzNS4zMTU5IDcuODAxNDQgMzQuMDg4NCA1LjcyNzY1IDMwLjUwNzFINS43MjY0Wk0zLjYzMDEgMTMuMTIwNUM0LjUwNTEyIDExLjYwMDQgNS44ODY0IDEwLjQzNzkgNy41MzE0NCA5LjgzNDE1QzcuNTMxNDQgOS45MDI5IDcuNTI3NjkgMTAuMDI0MiA3LjUyNzY5IDEwLjEwOTJWMTkuMzEwNkM3LjUyNTE5IDE5Ljc3ODEgNy43NzUxOSAyMC4yMTE5IDguMTgxNDUgMjAuNDQzMUwxNy45MDU0IDI2LjA1N0wxNC41MzkxIDI4LjAwMDhDMTQuNTA1MyAyOC4wMjMzIDE0LjQ2MjggMjguMDI3IDE0LjQyNTMgMjguMDEwOEw2LjM3MjY2IDIzLjM1ODJDMi43OTM4MyAyMS4yODU2IDEuNTY2MzEgMTYuNzA2OCAzLjYyODg1IDEzLjEyMTdMMy42MzAxIDEzLjEyMDVaTTMxLjI4ODIgMTkuNTU2OUwyMS41NjQyIDEzLjk0MTdMMjQuOTMwNiAxMS45OTkyQzI0Ljk2NDMgMTEuOTc2NyAyNS4wMDY4IDExLjk3MjkgMjUuMDQ0MyAxMS45ODkyTDMzLjA5NyAxNi42MzhDMzYuNjgyMSAxOC43MDkzIDM3LjkxMDggMjMuMjk1NyAzNS44Mzk1IDI2Ljg4MDhDMzQuOTYzMyAyOC4zOTgzIDMzLjU4MzIgMjkuNTYwOCAzMS45Mzk1IDMwLjE2NThWMjAuNjg5NEMzMS45NDMyIDIwLjIyMTkgMzEuNjk0NSAxOS43ODk0IDMxLjI4OTQgMTkuNTU2OUgzMS4yODgyWk0zNC42MzgzIDE0LjUxNDJDMzQuNTc5NSAxNC40NzggMzQuNDc1OCAxNC40MTU1IDM0LjQwMiAxNC4zNzNMMjYuNDM2OCA5Ljc3Mjg5QzI2LjAzMzEgOS41MzY2NCAyNS41MzMxIDkuNTM2NjQgMjUuMTI4MSA5Ljc3Mjg5TDE1LjQwNDEgMTUuMzg4VjExLjUwMDRDMTUuNDAxNiAxMS40NjA0IDE1LjQyMDQgMTEuNDIxNyAxNS40NTE2IDExLjM5NjdMMjMuNTAzIDYuNzUxNThDMjcuMDg5NCA0LjY4Mjc5IDMxLjY3NDUgNS45MTQwNiAzMy43NDIgOS41MDE2NEMzNC42MTU4IDExLjAxNjcgMzQuOTMyIDEyLjc5MDUgMzQuNjM1OCAxNC41MTQySDM0LjYzODNaTTEzLjU3NDEgMjEuNDQzMUwxMC4yMDY1IDE5LjQ5OTRDMTAuMTcwMiAxOS40ODE5IDEwLjE0NjUgMTkuNDQ2OCAxMC4xNDE1IDE5LjQwNjhWMTAuMTA3OUMxMC4xNDQgNS45Njc4MSAxMy41MDI4IDIuNjEyNzQgMTcuNjQyOSAyLjYxNTI0QzE5LjM5NDIgMi42MTUyNCAyMS4wODkyIDMuMjMwMjUgMjIuNDM1NSA0LjM1MDI4QzIyLjM3NDMgNC4zODI3OCAyMi4yNjkzIDQuNDQxNTMgMjIuMTk5MiA0LjQ4NDAzTDE0LjIzNDEgOS4wODQxM0MxMy44MjY2IDkuMzE1MzggMTMuNTc2NiA5Ljc0Nzg5IDEzLjU3OTEgMTAuMjE2N0wxMy41NzQxIDIxLjQ0MDZWMjEuNDQzMVpNMTUuNDAyOSAxNy41MDA2TDE5LjczNDIgMTQuOTk5M0wyNC4wNjU1IDE3LjQ5OTNWMjIuNTAwN0wxOS43MzQyIDI1LjAwMDdMMTUuNDAyOSAyMi41MDA3VjE3LjUwMDZaIiBmaWxsPSIjN0Q3RDg3Ii8+Cjwvc3ZnPgo=\" alt=\"Embeddings OpenAI\" title=\"Embeddings OpenAI\">\n          <span>+5</span>\n        </div>\n        <div class=\"workflow-details\">\n          <p class=\"workflow-details-name\">\n            <a href=\"https://n8n.io/workflows/2862-effortless-email-management-with-ai-powered-summarization-and-review/\" class=\"blog-banner-workflow\">Effortless Email Management with AI-Powered Summarization &amp; Review</a>\n          </p>\n          <p class=\"workflow-details-stats\">by n3witalia</p>\n        </div>\n      </div>\n      <a href=\"https://n8n.io/workflows/2862-effortless-email-management-with-ai-powered-summarization-and-review/\" class=\"global-button blog-banner-workflow\">\n        Use this workflow\n      </a>\n    </div>\n<!--kg-card-end: html-->\n<p>This workflow automates the handling of incoming emails, summarizes their content, generates responses using RAG, and obtains approval before sending replies. It listens for new emails, summarizes them, generates responses, and sends drafts for human review via <a href=\"https://docs.n8n.io/integrations/builtin/app-nodes/n8n-nodes-base.gmail/?ref=blog.n8n.io\" rel=\"noopener\" target=\"_blank\">Gmail</a>. If approved, the email is sent; otherwise, it's edited or handled manually. A text classifier categorizes feedback to guide the process.</p><h3 id=\"rag-chatbot-for-company-documents-using-google-drive-and-gemini\">RAG chatbot for company documents using Google Drive and Gemini</h3>\n<!--kg-card-begin: html-->\n<script>\n  workflowBanner(2753, document.currentScript);\n</script><div class=\"workflow\">\n      <div class=\"workflow-content\">\n        <div class=\"workflow-nodes\">\n          <img src=\"data:image/svg+xml;base64,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\" alt=\"Pinecone Vector Store\" title=\"Pinecone Vector Store\"><img src=\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB2aWV3Qm94PSIwIDAgNDggNDgiPjxkZWZzPjxwYXRoIGlkPSJhIiBkPSJNNDQuNSAyMEgyNHY4LjVoMTEuOEMzNC43IDMzLjkgMzAuMSAzNyAyNCAzN2MtNy4yIDAtMTMtNS44LTEzLTEzczUuOC0xMyAxMy0xM2MzLjEgMCA1LjkgMS4xIDguMSAyLjlsNi40LTYuNEMzNC42IDQuMSAyOS42IDIgMjQgMiAxMS44IDIgMiAxMS44IDIgMjRzOS44IDIyIDIyIDIyYzExIDAgMjEtOCAyMS0yMiAwLTEuMy0uMi0yLjctLjUtNCIvPjwvZGVmcz48Y2xpcFBhdGggaWQ9ImIiPjx1c2UgeGxpbms6aHJlZj0iI2EiIG92ZXJmbG93PSJ2aXNpYmxlIi8+PC9jbGlwUGF0aD48cGF0aCBmaWxsPSIjRkJCQzA1IiBkPSJNMCAzN1YxMWwxNyAxM3oiIGNsaXAtcGF0aD0idXJsKCNiKSIvPjxwYXRoIGZpbGw9IiNFQTQzMzUiIGQ9Im0wIDExIDE3IDEzIDctNi4xTDQ4IDE0VjBIMHoiIGNsaXAtcGF0aD0idXJsKCNiKSIvPjxwYXRoIGZpbGw9IiMzNEE4NTMiIGQ9Im0wIDM3IDMwLTIzIDcuOSAxTDQ4IDB2NDhIMHoiIGNsaXAtcGF0aD0idXJsKCNiKSIvPjxwYXRoIGZpbGw9IiM0Mjg1RjQiIGQ9Ik00OCA0OCAxNyAyNGwtNC0zIDM1LTEweiIgY2xpcC1wYXRoPSJ1cmwoI2IpIi8+PC9zdmc+\" alt=\"Embeddings Google Gemini\" title=\"Embeddings Google Gemini\"><img src=\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI3NjgiIGhlaWdodD0iMTAyNCI+PHBhdGggZmlsbD0iIzdEN0Q4NyIgZD0iTTAgOTYwVjY0aDU3NmwxOTIgMTkydjcwNHptNzA0LTY0MEw1MTIgMTI4SDY0djc2OGg2NDB6TTMyMCA1MTJIMTI4VjI1NmgxOTJ6bS02NC0xOTJoLTY0djEyOGg2NHptMCA0NDhoNjR2NjRIMTI4di02NGg2NFY2NDBoLTY0di02NGgxMjh6bTI1Ni0zMjBoNjR2NjRIMzg0di02NGg2NFYzMjBoLTY0di02NGgxMjh6bTY0IDM4NEgzODRWNTc2aDE5MnptLTY0LTE5MmgtNjR2MTI4aDY0eiIvPjwvc3ZnPg==\" alt=\"Default Data Loader\" title=\"Default Data Loader\">\n          <i class=\"fa fa-grip-lines-vertical\" aria-hidden=\"true\"></i>\n        \n          <i class=\"fa fa-robot\" aria-hidden=\"true\" style=\"color:#404040\"></i>\n        \n          <span>+5</span>\n        </div>\n        <div class=\"workflow-details\">\n          <p class=\"workflow-details-name\">\n            <a href=\"https://n8n.io/workflows/2753-rag-chatbot-for-company-documents-using-google-drive-and-gemini/\" class=\"blog-banner-workflow\">RAG Chatbot for Company Documents using Google Drive and Gemini</a>\n          </p>\n          <p class=\"workflow-details-stats\">by mihailtd</p>\n        </div>\n      </div>\n      <a href=\"https://n8n.io/workflows/2753-rag-chatbot-for-company-documents-using-google-drive-and-gemini/\" class=\"global-button blog-banner-workflow\">\n        Use this workflow\n      </a>\n    </div>\n<!--kg-card-end: html-->\n<p>This workflow implements a Retrieval Augmented Generation (RAG) chatbot that answers employee questions based on company documents stored in <a href=\"https://docs.n8n.io/integrations/builtin/app-nodes/n8n-nodes-base.googledrive/?ref=blog.n8n.io\" rel=\"noopener\" target=\"_blank\">Google Drive</a>. It automatically indexes new or updated documents in a <a href=\"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstorepinecone/?ref=blog.n8n.io\" rel=\"noopener\" target=\"_blank\">Pinecone vector database</a>, allowing the chatbot to provide accurate and up-to-date information. The workflow uses <a href=\"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatgooglegemini/?ref=blog.n8n.io\" rel=\"noopener\" target=\"_blank\">Google's Gemini AI</a> for both embeddings and response generation. When a new or updated document is detected, the workflow downloads it, splits it into chunks, embeds the chunks using Gemini, and stores them in Pinecone. The chatbot receives questions, retrieves relevant information from Pinecone based on the question, and generates answers using the Gemini chat model.</p><h2 id=\"langchain-vs-llamaindex-vs-n8n-faq\">LangChain vs LlamaIndex vs n8n FAQ</h2><div class=\"kg-card kg-toggle-card\" data-kg-toggle-state=\"close\">\n            <div class=\"kg-toggle-heading\">\n                <h4 class=\"kg-toggle-heading-text\"><span style=\"white-space: pre-wrap;\">Which is better for building autonomous AI agents, Auto-GPT or LangChain?</span></h4>\n                <button class=\"kg-toggle-card-icon\" aria-label=\"Expand toggle to read content\">\n                    <svg id=\"Regular\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                        <path class=\"cls-1\" d=\"M23.25,7.311,12.53,18.03a.749.749,0,0,1-1.06,0L.75,7.311\"></path>\n                    </svg>\n                </button>\n            </div>\n            <div class=\"kg-toggle-content\"><p><span style=\"white-space: pre-wrap;\">Auto-GPT is better for fully autonomous agents that operate with minimal human input, while LangChain offers greater flexibility for building custom workflows and integrating multiple external tools and APIs.</span></p></div>\n        </div><div class=\"kg-card kg-toggle-card\" data-kg-toggle-state=\"close\">\n            <div class=\"kg-toggle-heading\">\n                <h4 class=\"kg-toggle-heading-text\"><b><strong style=\"white-space: pre-wrap;\">Which tool is best for retrieval-augmented generation (RAG): LangChain, LlamaIndex, or Haystack?</strong></b></h4>\n                <button class=\"kg-toggle-card-icon\" aria-label=\"Expand toggle to read content\">\n                    <svg id=\"Regular\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                        <path class=\"cls-1\" d=\"M23.25,7.311,12.53,18.03a.749.749,0,0,1-1.06,0L.75,7.311\"></path>\n                    </svg>\n                </button>\n            </div>\n            <div class=\"kg-toggle-content\"><p><span style=\"white-space: pre-wrap;\">Haystack is the best choice for search-heavy RAG applications, LlamaIndex excels at indexing and querying large datasets, and LangChain is ideal for orchestrating complex LLM workflows that involve both retrieval and external integrations.</span></p></div>\n        </div><div class=\"kg-card kg-toggle-card\" data-kg-toggle-state=\"close\">\n            <div class=\"kg-toggle-heading\">\n                <h4 class=\"kg-toggle-heading-text\"><b><strong style=\"white-space: pre-wrap;\">When should I use LangChain, LlamaIndex, or Hugging Face for an LLM project?</strong></b></h4>\n                <button class=\"kg-toggle-card-icon\" aria-label=\"Expand toggle to read content\">\n                    <svg id=\"Regular\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                        <path class=\"cls-1\" d=\"M23.25,7.311,12.53,18.03a.749.749,0,0,1-1.06,0L.75,7.311\"></path>\n                    </svg>\n                </button>\n            </div>\n            <div class=\"kg-toggle-content\"><p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">Use LangChain for complex workflows and multi-step logic, LlamaIndex for efficient data retrieval, and Hugging Face for accessing and fine-tuning pre-trained LLMs across a wide range of tasks.</span></p></div>\n        </div><div class=\"kg-card kg-toggle-card\" data-kg-toggle-state=\"close\">\n            <div class=\"kg-toggle-heading\">\n                <h4 class=\"kg-toggle-heading-text\"><span style=\"white-space: pre-wrap;\">Can LangChain be used with Python only?</span></h4>\n                <button class=\"kg-toggle-card-icon\" aria-label=\"Expand toggle to read content\">\n                    <svg id=\"Regular\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                        <path class=\"cls-1\" d=\"M23.25,7.311,12.53,18.03a.749.749,0,0,1-1.06,0L.75,7.311\"></path>\n                    </svg>\n                </button>\n            </div>\n            <div class=\"kg-toggle-content\"><p><span style=\"white-space: pre-wrap;\">LangChain offers official libraries for both Python and JavaScript. While the Python library was initially more mature, both libraries are now actively developed and maintained, offering comparable features and functionality. You can choose either language based on your preference and project requirements.</span></p><p><span style=\"white-space: pre-wrap;\">There are community-driven adaptations of LangChain for Java and Golang. Additionally, platforms like n8n provide a visual interface and pre-built nodes for LangChain.</span></p></div>\n        </div><div class=\"kg-card kg-toggle-card\" data-kg-toggle-state=\"close\">\n            <div class=\"kg-toggle-heading\">\n                <h4 class=\"kg-toggle-heading-text\"><span style=\"white-space: pre-wrap;\">Can you use LangChain with Ollama local LLMs?</span></h4>\n                <button class=\"kg-toggle-card-icon\" aria-label=\"Expand toggle to read content\">\n                    <svg id=\"Regular\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                        <path class=\"cls-1\" d=\"M23.25,7.311,12.53,18.03a.749.749,0,0,1-1.06,0L.75,7.311\"></path>\n                    </svg>\n                </button>\n            </div>\n            <div class=\"kg-toggle-content\"><p><span style=\"white-space: pre-wrap;\">Yes, LangChain provides official components for integrating with Ollama. This can be beneficial for various reasons, including privacy, security, and cost-effectiveness.</span></p></div>\n        </div><div class=\"kg-card kg-toggle-card\" data-kg-toggle-state=\"close\">\n            <div class=\"kg-toggle-heading\">\n                <h4 class=\"kg-toggle-heading-text\"><span style=\"white-space: pre-wrap;\">Can you use LangChain with Pinecone or other vector databases?</span></h4>\n                <button class=\"kg-toggle-card-icon\" aria-label=\"Expand toggle to read content\">\n                    <svg id=\"Regular\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                        <path class=\"cls-1\" d=\"M23.25,7.311,12.53,18.03a.749.749,0,0,1-1.06,0L.75,7.311\"></path>\n                    </svg>\n                </button>\n            </div>\n            <div class=\"kg-toggle-content\"><p><span style=\"white-space: pre-wrap;\">LangChain is designed to work seamlessly with Pinecone and other vector databases. Integrating a vector database like Pinecone with LangChain is a common practice for building efficient and scalable RAG applications.</span></p></div>\n        </div><div class=\"kg-card kg-toggle-card\" data-kg-toggle-state=\"close\">\n            <div class=\"kg-toggle-heading\">\n                <h4 class=\"kg-toggle-heading-text\"><span style=\"white-space: pre-wrap;\">What is LangSmith?</span></h4>\n                <button class=\"kg-toggle-card-icon\" aria-label=\"Expand toggle to read content\">\n                    <svg id=\"Regular\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                        <path class=\"cls-1\" d=\"M23.25,7.311,12.53,18.03a.749.749,0,0,1-1.06,0L.75,7.311\"></path>\n                    </svg>\n                </button>\n            </div>\n            <div class=\"kg-toggle-content\"><p><span style=\"white-space: pre-wrap;\">LangSmith is a platform designed specifically for debugging, testing, and evaluating LLM applications. It helps developers improve the performance and reliability of their LangChain projects by providing tools to trace workflow execution and evaluate accuracy.</span></p></div>\n        </div><h2 id=\"wrap-up\">Wrap up</h2><p>In this article, we provided a comparative analysis of LlamaIndex and LangChain, two powerful frameworks for building RAG systems. We highlighted their strengths, limitations, and use cases, and also introduced n8n as a compelling alternative for those seeking a low-code, integration-heavy solution.</p><p>Choosing the right framework for your LLM application depends on your specific needs and priorities.</p><p>If you need a simple and efficient solution for data-centric tasks, LlamaIndex is a great choice. If you require greater flexibility and control for complex workflows, LangChain might be a better fit. And if you're looking for a broader automation platform that seamlessly integrates with LLMs, n8n offers a compelling alternative.</p>\n<!--kg-card-begin: html-->\n<div class=\"content-banner\">\n  <div>\n    <h3>Create your RAG workflows with n8n</h3>\n    <p>Get started today and unlock flexible, scalable AI automation!</p>\n  </div>\n  <a href=\"https://app.n8n.cloud/register?ref=blog.n8n.io\" class=\"global-button blog-banner-signup\" target=\"_blank\" rel=\"noopener\">Try n8n now</a>\n</div>\n<!--kg-card-end: html-->\n<h2 id=\"what%E2%80%99s-next\">Whatâ€™s next?</h2><p>To further enhance your understanding of RAG and LLM application development, check out these resources on the n8n blog:</p><ul><li><a href=\"https://blog.n8n.io/llm-agents/\"><strong><u>LLM agents in 2025</u></strong></a><strong>:</strong> Learn about the latest advances in LLM agents and how they are being used in enterprise environments.</li><li><a href=\"https://blog.n8n.io/rag-chatbot/\"><strong><u>RAG chatbots</u></strong></a><strong>:</strong> Learn how to build a RAG chatbot that can access and process information from your documents or knowledge base using n8n.</li><li><a href=\"https://blog.n8n.io/local-llm/\"><strong><u>Use n8n to integrate LangChain with local LLMs</u></strong></a><strong>:</strong> Learn how to run LLMs like Deepseek locally on your own machine using n8n and Ollama.</li><li><a href=\"https://blog.n8n.io/ai-agentic-workflows/\"><strong><u>AI-agentic workflows</u></strong></a><strong>:</strong> Learn how AI agents can be used to automate tasks and make decisions in n8n workflows.</li><li><a href=\"https://blog.n8n.io/mlops-tools/\"><strong><u>MLOps tools</u></strong></a><strong>:</strong> Explore the different MLOps tools available and how they can be used to manage the entire lifecycle of your ML models.</li></ul>\n\t\t<div class=\"newsletter-banner\">\n\t    <div class=\"newsletter-banner-content\">\n\t      <div class=\"section-header\">\n\t        <h2>Subscribe to <span>n8n newsletter</span></h2>\n\t        <div class=\"section-subheader--bottom\">\n\t          Get the best, coolest, and latest in automation and low-code delivered to your inbox each week.\n\t        </div>\n\t      </div>\n\t      <div class=\"newsletter-banner-form\">\n\t        <form autocomplete=\"off\" class=\"contact-form\" onsubmit=\"subscribeNewsletter(event)\">\n\t        \t<div id=\"recaptcha\" class=\"g-recaptcha\" data-sitekey=\"6LeAQeopAAAAAKlLsRb1weWm6T_vijoQBkGkbHzB\" data-callback=\"submitSubscription\" data-size=\"invisible\"><div class=\"grecaptcha-badge\" data-style=\"bottomright\" style=\"width: 256px; height: 60px; display: block; transition: right 0.3s ease 0s; position: fixed; bottom: 14px; right: -186px; box-shadow: gray 0px 0px 5px; border-radius: 2px; overflow: hidden;\"><div class=\"grecaptcha-logo\"><iframe title=\"reCAPTCHA\" width=\"256\" height=\"60\" role=\"presentation\" name=\"a-ge91smnfdx6v\" frameborder=\"0\" scrolling=\"no\" sandbox=\"allow-forms allow-popups allow-same-origin allow-scripts allow-top-navigation allow-modals allow-popups-to-escape-sandbox allow-storage-access-by-user-activation\" src=\"https://www.google.com/recaptcha/api2/anchor?ar=1&amp;k=6LeAQeopAAAAAKlLsRb1weWm6T_vijoQBkGkbHzB&amp;co=aHR0cHM6Ly9ibG9nLm44bi5pbzo0NDM.&amp;hl=en&amp;v=jt8Oh2-Ue1u7nEbJQUIdocyd&amp;size=invisible&amp;cb=ofkqf0i1m17r\"></iframe></div><div class=\"grecaptcha-error\"></div><textarea id=\"g-recaptcha-response\" name=\"g-recaptcha-response\" class=\"g-recaptcha-response\" style=\"width: 250px; height: 40px; border: 1px solid rgb(193, 193, 193); margin: 10px 25px; padding: 0px; resize: none; display: none;\"></textarea></div><iframe style=\"display: none;\"></iframe></div>\n\t          <div class=\"input-wrapper\">\n\t            <input placeholder=\"Email\" name=\"email\" type=\"email\" required=\"required\" class=\"\">\n\t            <div class=\"messages\">\n\t              <div class=\"message message--error\">Something went wrong. Please try again later.</div>\n\t              <div class=\"message message--success\">Subscribed!</div>\n\t            </div>\n\t          </div>\n\t          <button type=\"submit\" class=\"submit-btn\">Subscribe</button>\n\t        </form>\n\t      </div>\n\t    </div>\n    </div>\n\t\t<div class=\"post-share-section\">\n\t<div class=\"post-share-wrap\">\n\t\t<a href=\"https://twitter.com/intent/tweet?text=LlamaIndex%20vs%20LangChain%3A%20Which%20RAG%20tool%20is%20right%20for%20you%3F&amp;url=https://blog.n8n.io/llamaindex-vs-langchain/\" target=\"_blank\" rel=\"noopener\" aria-label=\"Twitter share icon\"><svg role=\"img\" viewBox=\"0 0 24 24\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M23.954 4.569c-.885.389-1.83.654-2.825.775 1.014-.611 1.794-1.574 2.163-2.723-.951.555-2.005.959-3.127 1.184-.896-.959-2.173-1.559-3.591-1.559-2.717 0-4.92 2.203-4.92 4.917 0 .39.045.765.127 1.124C7.691 8.094 4.066 6.13 1.64 3.161c-.427.722-.666 1.561-.666 2.475 0 1.71.87 3.213 2.188 4.096-.807-.026-1.566-.248-2.228-.616v.061c0 2.385 1.693 4.374 3.946 4.827-.413.111-.849.171-1.296.171-.314 0-.615-.03-.916-.086.631 1.953 2.445 3.377 4.604 3.417-1.68 1.319-3.809 2.105-6.102 2.105-.39 0-.779-.023-1.17-.067 2.189 1.394 4.768 2.209 7.557 2.209 9.054 0 13.999-7.496 13.999-13.986 0-.209 0-.42-.015-.63.961-.689 1.8-1.56 2.46-2.548l-.047-.02z\"></path></svg></a>\n\t\t<a href=\"https://www.facebook.com/sharer/sharer.php?u=https://blog.n8n.io/llamaindex-vs-langchain/\" target=\"_blank\" rel=\"noopener\" aria-label=\"Facebook share icon\"><svg role=\"img\" viewBox=\"0 0 24 24\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M23.9981 11.9991C23.9981 5.37216 18.626 0 11.9991 0C5.37216 0 0 5.37216 0 11.9991C0 17.9882 4.38789 22.9522 10.1242 23.8524V15.4676H7.07758V11.9991H10.1242V9.35553C10.1242 6.34826 11.9156 4.68714 14.6564 4.68714C15.9692 4.68714 17.3424 4.92149 17.3424 4.92149V7.87439H15.8294C14.3388 7.87439 13.8739 8.79933 13.8739 9.74824V11.9991H17.2018L16.6698 15.4676H13.8739V23.8524C19.6103 22.9522 23.9981 17.9882 23.9981 11.9991Z\"></path></svg></a>\n\t\t<!-- <a href=\"javascript:\" class=\"post-share-link\" id=\"copy\" data-clipboard-target=\"#copy-link\" aria-label=\"Copy link icon\"><svg role=\"img\" viewBox=\"0 0 33 24\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M27.3999996,13.4004128 L21.7999996,13.4004128 L21.7999996,19 L18.9999996,19 L18.9999996,13.4004128 L13.3999996,13.4004128 L13.3999996,10.6006192 L18.9999996,10.6006192 L18.9999996,5 L21.7999996,5 L21.7999996,10.6006192 L27.3999996,10.6006192 L27.3999996,13.4004128 Z M12,20.87 C7.101,20.87 3.13,16.898 3.13,12 C3.13,7.102 7.101,3.13 12,3.13 C12.091,3.13 12.181,3.139 12.272,3.142 C9.866,5.336 8.347,8.487 8.347,12 C8.347,15.512 9.866,18.662 12.271,20.857 C12.18,20.859 12.091,20.87 12,20.87 Z M20.347,0 C18.882,0 17.484,0.276 16.186,0.756 C14.882,0.271 13.473,0 12,0 C5.372,0 0,5.373 0,12 C0,18.628 5.372,24 12,24 C13.471,24 14.878,23.726 16.181,23.242 C17.481,23.724 18.88,24 20.347,24 C26.975,24 32.347,18.628 32.347,12 C32.347,5.373 26.975,0 20.347,0 Z\"/></svg></a>\n\t\t<small class=\"share-link-info\">The link has been copied!</small> -->\n\t</div>\n\t<input type=\"text\" value=\"https://blog.n8n.io/llamaindex-vs-langchain/\" id=\"copy-link\" aria-label=\"Copy link input\">\n</div>",
  "readme": "Retrieval-Augmented Generation (RAG) is essential for building LLM applications that can access and reason over up-to-date, proprietary, or domain-specific information. By using RAG, LLMs can overcome the limitations of relying solely on their pre-trained knowledge.\n\nTwo popular frameworks for building RAG chatbots are LlamaIndex and LangChain.\n\nThis article provides a comparative analysis of LlamaIndex and LangChain, highlighting their core strengths, key differences, and ideal use cases. We will then introduce [n8n](https://n8n.io/ai/?ref=blog.n8n.io) as [a LangChain alternative](https://blog.n8n.io/langchain-alternatives/). n8n is a powerful, low-code automation software, particularly well-suited for RAG workflows that require extensive integrations and visual workflow design.\n\nSo let's get straight into it!\n\n## LlamaIndex vs LangChain: A comparative analysis\n\nHere's a comparative table to summarize the key differences between LlamaIndex and LangChain:\n\nCriteria | **LlamaIndex** | **LangChain**  \n---|---|---  \n**Primary focus** | Data connection, indexing,   \nand querying for RAG. | Building and orchestrating   \ncomplex LLM workflows,   \nincluding agents and chains.  \n**Ease of use** | Easier to learn and use,   \nespecially for beginners.   \nHigh-level API simplifies   \ncommon tasks. | Steeper learning curve.   \nRequires a deeper understanding   \nof LLM concepts.  \n**Data ingestion** | Extensive data connectors   \nthrough LlamaHub (APIs, PDFs,   \ndatabases, etc.). Streamlined   \ndata loading and indexing. | Supports data loading,   \nbut focus is more on   \ndata transformation within   \nthe pipeline.  \n**Querying** | Sophisticated querying   \ncapabilities, including   \nsubqueries and multi-document   \nsummarization. Optimized   \nfor retrieval. | Flexible querying,   \nbut often requires more   \nmanual configuration.  \n**Flexibility** | Less flexible, more   \nopinionated. Good for   \nstandard RAG use cases. | Highly flexible and modular.   \nAllows swapping LLMs,   \ncustomizing prompts,   \nand building complex chains   \nusing various LangChain   \nchain types.  \n**Extensibility** | Primarily through LlamaHub   \nand custom data connectors. | Highly extensible through   \ncustom chains, agents,   \nand tools.  \n**Customization** | Some customization options,   \nbut less than LangChain. | Highly customisable and   \ngreat degree of control.  \n**Free for commercial use?** | Yes | Yes  \n**Use cases** | RAG chatbots, document   \nQ&A, knowledge base   \nquerying, data augmentation. | Complex reasoning systems,   \nmulti-agent applications,   \napplications requiring   \nintegration with multiple   \ntools and APIs. You can   \nalso use LangChain for   \nRAG workflows.  \n**Repository** | [LlamaIndex GitHub](https://github.com/run-llama/llama_index?ref=blog.n8n.io) | [LangChain GitHub](https://github.com/langchain-ai/langchain?ref=blog.n8n.io)  \n  \n## Which is better: LlamaIndex or LangChain?\n\nBoth LlamaIndex and LangChain are powerful frameworks for building LLM-powered applications, particularly those leveraging RAG.\n\nHowever, when considering LangChain vs LlamaIndex, they have distinct strengths that make them better suited for different use cases.\n\n### Ease of use\n\nLlamaIndex generally has a gentler learning curve. Its high-level API and focus on data connection and querying make it easier to get started, especially for developers new to LLMs. For example, if you need to quickly build a RAG chatbot that answers questions over a collection of PDF documents, LlamaIndex's data loaders and index structures simplify this process considerably.\n\nLangChain, while more powerful, has a steeper learning curve. Its modularity and flexibility require a deeper understanding of LLM concepts and the various components involved.\n\n### Data handling and indexing\n\nLlamaIndex excels in this area. It provides various indexing strategies optimized for different types of data and retrieval needs. For instance, you can easily load data from APIs, databases, and local files, and choose between vector, tree, or keyword-based indexes.\n\nLlamaIndex has a user-friendly approach to data ingestion, primarily through [_LlamaHub_](https://llamahub.ai/?ref=blog.n8n.io). This central repository offers a wide array of data connectors for common sources like APIs, PDFs, documents, and databases. This extensive and easily accessible collection of connectors significantly simplifies and speeds up the process of integrating diverse data sources into your RAG pipeline.\n\nLangChain, in contrast, does not enforce a specific indexing approach but instead allows users to structure their own pipelines based on their preferred tools. This flexibility makes it suitable for developers who want more control over their retrieval strategies, though it may require additional setup compared to LlamaIndexâ€™s built-in indexing mechanisms.\n\nLangChain also supports data loading with its own set of document loaders. While it might not have a single, unified hub like LlamaHub, LangChain's data loaders are flexible and can be customized. This approach provides greater control over the data loading process, which is advantageous for developers who need to implement highly specific or custom data ingestion logic.\n\n### Flexibility\n\nLangChain offers significantly more flexibility and control. Its modular architecture allows you to swap out different LLMs, customize prompt templates, and chain together multiple tools and agents. This is crucial if you're building complex applications, such as a multi-step reasoning system or an application that needs to interact with multiple external services.\n\nLlamaIndex, while offering some customization, is more opinionated in its approach, prioritizing ease of use over fine-grained control.\n\n### Querying capabilities\n\nLlamaIndex is optimized for sophisticated querying within RAG systems. It supports advanced querying techniques like subqueries (querying across multiple documents or indexes) and multi-document summarization.\n\nLangChain offers flexible querying options, but often requires more manual configuration to achieve advanced querying patterns. You have the building blocks to create complex query chains, but you need to assemble them yourself.\n\n### Memory management\n\nLlamaIndex offers basic context retention capabilities, allowing for simple conversational interactions. This is sufficient for straightforward RAG chatbots where maintaining short-term conversation history is needed for context.\n\nLangChain's advanced memory management is crucial for building sophisticated conversational AI applications that require extensive context retention, understanding of conversation history, and complex multi-turn reasoning.\n\nðŸ’¡\n\nIf your primary need is to quickly connect LLMs to your data and build RAG applications with minimal complexity, LlamaIndex is an excellent choice. If you need maximum flexibility, control, and the ability to build complex, multi-step LLM workflows, LangChain is the more powerful option.\n\n## Can I use LangChain and LlamaIndex together?\n\nYes, you can absolutely use LangChain and LlamaIndex together! In fact, combining them can be a powerful way to leverage the strengths of each framework. Here's why and how:\n\n### Use LlamaIndex for data management\n\nYou can use LlamaIndex's powerful data connectors and indexing capabilities to efficiently load, structure, and index your data from various sources. This creates a robust knowledge base for your RAG system. As we mentioned earlier, while LangChain connectors are also available, LlamaIndex excels in this area.\n\n### Use LangChain for orchestration\n\nUtilize LangChain's chains, agents, and LangChain tools to build the overall logic and workflow of your application. Integrate LlamaIndex's query engine as a tool within your LangChain workflow, allowing you to retrieve relevant information from your indexed data.\n\n## What are the limitations of LlamaIndex and LangChain?\n\n### LlamaIndex limitations\n\n  * Primarily focused on data retrieval, making it less suitable for:\n    * Highly complex LLM applications with intricate, multi-step workflows\n    * Applications requiring interactions with numerous external services\n  * Supports only basic context retention, which may be insufficient for:\n    * Applications needing extensive conversational memory\n    * Complex reasoning across multiple turns.\n\n\n\n### LangChain limitations\n\n  * High flexibility comes with:\n    * A steep learning curve, especially for developers new to LLMs\n    * More intricate initial setup and configuration\n  * Increased debugging and maintenance overhead, particularly for sophisticated applications\n  * Frequent breaking changes between versions, often requiring ongoing code adjustments.\n\n\n\nðŸ’¡\n\nUnlike LlamaIndexâ€™s focus on data retrieval and LangChainâ€™s steep learning curve, n8n provides ****scalable orchestration**** with built-in connectors and ****easier maintenance**** across evolving LLM applications.\n\n## An alternative to LlamaIndex or LangChain: n8n\n\nWhile LangChain and LlamaIndex are powerful tools for building LLM applications, they primarily focus on a code-centric approach.\n\nn8n offers a compelling alternative by providing a low-code environment that seamlessly integrates with LangChain. This means you can get the power and flexibility of LangChain without the complexity of managing its underlying code directly.\n\nðŸ’¡\n\nBy providing an abstraction layer over LangChain, n8n simplifies the development process while retaining the core flexibility that LangChain offers.\n\n### Why choose n8n over LlamaIndex and LangChain?\n\n  * You want LangChain's power, but prefer **low-code**! [n8n's AI Agent node](https://n8n.io/integrations/agent/?ref=blog.n8n.io) allows you to leverage LangChain's features, such as agents and memory, without writing complex Python code.\n  * **Extensive integrations:** [_n8n boasts 400+ integrations with various apps and services_](https://n8n.io/integrations/?ref=blog.n8n.io) such as databases, CRMs, marketing platforms, communication tools, and virtually any service with an API. This is a significant advantage over LangChain and LlamaIndex, which primarily focus on LLM interactions. With n8n itâ€™s also very easy to integrate [_vector databases_](https://n8n.io/integrations/?q=vector&ref=blog.n8n.io), such as Pinecone, with LangChain.\n  * **Visual workflow builder:** n8n provides a user-friendly, [_drag-and-drop interface for building workflows_](https://n8n.io/features/?ref=blog.n8n.io), making it accessible to both technical and non-technical users. This can be a significant advantage over the code-heavy approach of LangChain and LlamaIndex.\n  * **Native AI capabilities:** [_n8n has built-in support for LangChain_](https://docs.n8n.io/advanced-ai/langchain/langchain-n8n/?ref=blog.n8n.io), including integrations with tools like [_LangChain Ollama for local LLM usage_](https://blog.n8n.io/local-llm/), allowing you to [_incorporate LLM interactions into your workflows_](https://n8n.io/ai/?ref=blog.n8n.io). You configure your LangChain chains and agents within the node, and n8n handles execution and integration with the rest of your workflow. This way, you leverage LangChain's advanced features (agents, memory, custom chains) without writing intricate Python.\n\n![n8n example of AI Agent workflow](https://blog.n8n.io/content/images/2025/02/n8n_ai_agent.webp)n8n example of AI Agent workflow\n\n  * **Rapid prototyping and iteration:** you can quickly experiment with different RAG approaches, connect to various data sources, and test different LLM configurations.\n  * **Simplified configuration:** While you can use advanced LangChain features, the visual interface simplifies configuration using the built-in LangChain templates. You set up prompts, input variables, and output handling within the LangChain node's settings.\n\n![Configuring the AI Agent n8n node](https://blog.n8n.io/content/images/2025/02/n8n_ai_node_configuration.webp)Configuring the AI Agent n8n node\n\n  * **Hybrid approach:** you can combine the ease of visual workflow design with the power of custom code using [n8n's LangChain code node](https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.code/?ref=blog.n8n.io).\n\n\n\n## Examples of RAG workflows built with n8n\n\nLet's explore some real-world workflow examples that illustrate the versatility of n8n for RAG and Agentic AI!\n\n### AI-Powered RAG workflow for stock earnings report analysis\n\n![Pinecone Vector Store](data:image/svg+xml;base64,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)![Embeddings Google Gemini](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB2aWV3Qm94PSIwIDAgNDggNDgiPjxkZWZzPjxwYXRoIGlkPSJhIiBkPSJNNDQuNSAyMEgyNHY4LjVoMTEuOEMzNC43IDMzLjkgMzAuMSAzNyAyNCAzN2MtNy4yIDAtMTMtNS44LTEzLTEzczUuOC0xMyAxMy0xM2MzLjEgMCA1LjkgMS4xIDguMSAyLjlsNi40LTYuNEMzNC42IDQuMSAyOS42IDIgMjQgMiAxMS44IDIgMiAxMS44IDIgMjRzOS44IDIyIDIyIDIyYzExIDAgMjEtOCAyMS0yMiAwLTEuMy0uMi0yLjctLjUtNCIvPjwvZGVmcz48Y2xpcFBhdGggaWQ9ImIiPjx1c2UgeGxpbms6aHJlZj0iI2EiIG92ZXJmbG93PSJ2aXNpYmxlIi8+PC9jbGlwUGF0aD48cGF0aCBmaWxsPSIjRkJCQzA1IiBkPSJNMCAzN1YxMWwxNyAxM3oiIGNsaXAtcGF0aD0idXJsKCNiKSIvPjxwYXRoIGZpbGw9IiNFQTQzMzUiIGQ9Im0wIDExIDE3IDEzIDctNi4xTDQ4IDE0VjBIMHoiIGNsaXAtcGF0aD0idXJsKCNiKSIvPjxwYXRoIGZpbGw9IiMzNEE4NTMiIGQ9Im0wIDM3IDMwLTIzIDcuOSAxTDQ4IDB2NDhIMHoiIGNsaXAtcGF0aD0idXJsKCNiKSIvPjxwYXRoIGZpbGw9IiM0Mjg1RjQiIGQ9Ik00OCA0OCAxNyAyNGwtNC0zIDM1LTEweiIgY2xpcC1wYXRoPSJ1cmwoI2IpIi8+PC9zdmc+)![Default Data Loader](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI3NjgiIGhlaWdodD0iMTAyNCI+PHBhdGggZmlsbD0iIzdEN0Q4NyIgZD0iTTAgOTYwVjY0aDU3NmwxOTIgMTkydjcwNHptNzA0LTY0MEw1MTIgMTI4SDY0djc2OGg2NDB6TTMyMCA1MTJIMTI4VjI1NmgxOTJ6bS02NC0xOTJoLTY0djEyOGg2NHptMCA0NDhoNjR2NjRIMTI4di02NGg2NFY2NDBoLTY0di02NGgxMjh6bTI1Ni0zMjBoNjR2NjRIMzg0di02NGg2NFYzMjBoLTY0di02NGgxMjh6bTY0IDM4NEgzODRWNTc2aDE5MnptLTY0LTE5MmgtNjR2MTI4aDY0eiIvPjwvc3ZnPg==) ____ +5\n\n[AI-Powered RAG Workflow For Stock Earnings Report Analysis](https://n8n.io/workflows/2741-ai-powered-rag-workflow-for-stock-earnings-report-analysis/)\n\nby mihailtd\n\n[ Use this workflow ](https://n8n.io/workflows/2741-ai-powered-rag-workflow-for-stock-earnings-report-analysis/)\n\nThis n8n workflow creates a financial analysis tool that generates reports on a company's quarterly earnings using [OpenAI GPT-4o-mini](https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatopenai/?ref=blog.n8n.io), [Google's Gemini AI](https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatgooglegemini/?ref=blog.n8n.io), and [Pinecone's vector search](https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstorepinecone/?ref=blog.n8n.io). The workflow fetches earnings PDFs, parses them, generates embeddings, and stores them in Pinecone. An AI agent orchestrates the process, using Pinecone and LLMs to analyze data and generate the report, which is then saved to [Google Drive](https://docs.n8n.io/integrations/builtin/app-nodes/n8n-nodes-base.googledrive/?ref=blog.n8n.io).\n\n### Complete business WhatsApp AI-powered RAG chatbot using OpenAI\n\n![Respond to Webhook](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCI+PHBhdGggZmlsbD0iIzM3NDc0ZiIgZD0iTTM1IDM3Yy0yLjIgMC00LTEuOC00LTRzMS44LTQgNC00IDQgMS44IDQgNC0xLjggNC00IDQiLz48cGF0aCBmaWxsPSIjMzc0NzRmIiBkPSJNMzUgNDNjLTMgMC01LjktMS40LTcuOC0zLjdsMy4xLTIuNWMxLjEgMS40IDIuOSAyLjMgNC43IDIuMyAzLjMgMCA2LTIuNyA2LTZzLTIuNy02LTYtNmMtMSAwLTIgLjMtMi45LjdsLTEuNyAxTDIzLjMgMTZsMy41LTEuOSA1LjMgOS40YzEtLjMgMi0uNSAzLS41IDUuNSAwIDEwIDQuNSAxMCAxMFM0MC41IDQzIDM1IDQzIi8+PHBhdGggZmlsbD0iIzM3NDc0ZiIgZD0iTTE0IDQzQzguNSA0MyA0IDM4LjUgNCAzM2MwLTQuNiAzLjEtOC41IDcuNS05LjdsMSAzLjlDOS45IDI3LjkgOCAzMC4zIDggMzNjMCAzLjMgMi43IDYgNiA2czYtMi43IDYtNnYtMmgxNXY0SDIzLjhjLS45IDQuNi01IDgtOS44IDgiLz48cGF0aCBmaWxsPSIjZTkxZTYzIiBkPSJNMTQgMzdjLTIuMiAwLTQtMS44LTQtNHMxLjgtNCA0LTQgNCAxLjggNCA0LTEuOCA0LTQgNCIvPjxwYXRoIGZpbGw9IiMzNzQ3NGYiIGQ9Ik0yNSAxOWMtMi4yIDAtNC0xLjgtNC00czEuOC00IDQtNCA0IDEuOCA0IDQtMS44IDQtNCA0Ii8+PHBhdGggZmlsbD0iI2U5MWU2MyIgZD0ibTE1LjcgMzQtMy40LTIgNS45LTkuN2MtMi0xLjktMy4yLTQuNS0zLjItNy4zIDAtNS41IDQuNS0xMCAxMC0xMHMxMCA0LjUgMTAgMTBjMCAuOS0uMSAxLjctLjMgMi41bC0zLjktMWMuMS0uNS4yLTEgLjItMS41IDAtMy4zLTIuNy02LTYtNnMtNiAyLjctNiA2YzAgMi4xIDEuMSA0IDIuOSA1LjFsMS43IDF6Ii8+PC9zdmc+) ____![OpenAI Chat Model](data:image/svg+xml;base64,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) __ +5\n\n[Complete business WhatsApp AI-Powered RAG Chatbot using OpenAI](https://n8n.io/workflows/2845-complete-business-whatsapp-ai-powered-rag-chatbot-using-openai/)\n\nby n3witalia\n\n[ Use this workflow ](https://n8n.io/workflows/2845-complete-business-whatsapp-ai-powered-rag-chatbot-using-openai/)\n\nThis workflow allows you to create an AI-powered chatbot for WhatsApp Business that uses RAG to provide accurate and relevant information to customers. The workflow sets up [webhooks ](https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.webhook/?ref=blog.n8n.io)to handle incoming messages, processes them, and utilizes an AI agent with a predefined system message to ensure appropriate responses. It accesses a knowledge base stored in [Qdrant ](https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstoreqdrant/?ref=blog.n8n.io)(a vector database like Pinecone), generates responses using OpenAI's GPT model, and sends them back to the user.\n\n### Effortless email management with AI-powered summarization & review\n\n__![Markdown](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNDAiIGhlaWdodD0iNDAiIHZpZXdCb3g9IjAgMCA0MCA0MCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTM3LjExNjQgMzIuNjE4N0gyLjg4MzU3QzIuNTA0OSAzMi42MTg3IDIuMTI5OTMgMzIuNTQ0MSAxLjc4MDA4IDMyLjM5OTJDMS40MzAyMyAzMi4yNTQzIDEuMTEyMzQgMzIuMDQxOSAwLjg0NDU3OSAzMS43NzQxQzAuNTc2ODE1IDMxLjUwNjQgMC4zNjQ0MTIgMzEuMTg4NSAwLjIxOTQ5OSAzMC44Mzg2QzAuMDc0NTg1OCAzMC40ODg4IDAgMzAuMTEzOCAwIDI5LjczNTFWMTAuODgzNkMwIDEwLjExODggMC4zMDM4MDQgOS4zODUzNSAwLjg0NDU3OSA4Ljg0NDU4QzEuMzg1MzUgOC4zMDM4IDIuMTE4OCA4IDIuODgzNTcgOEgzNy4xMTY0QzM3LjQ5NTEgOCAzNy44NzAxIDguMDc0NTkgMzguMjE5OSA4LjIxOTVDMzguNTY5OCA4LjM2NDQxIDM4Ljg4NzcgOC41NzY4MSAzOS4xNTU0IDguODQ0NThDMzkuNDIzMiA5LjExMjM0IDM5LjYzNTYgOS40MzAyMyAzOS43ODA1IDkuNzgwMDhDMzkuOTI1NCAxMC4xMjk5IDQwIDEwLjUwNDkgNDAgMTAuODgzNlYyOS43MzE4QzQwLjAwMDIgMzAuMTEwNiAzOS45MjU4IDMwLjQ4NTggMzkuNzgxIDMwLjgzNThDMzkuNjM2MiAzMS4xODU5IDM5LjQyMzggMzEuNTAzOSAzOS4xNTYgMzEuNzcxOUMzOC44ODgyIDMyLjAzOTggMzguNTcwMyAzMi4yNTI0IDM4LjIyMDMgMzIuMzk3NEMzNy44NzAzIDMyLjU0MjQgMzcuNDk1MiAzMi42MTg3IDM3LjExNjQgMzIuNjE4N1pNOS42MTQxMyAyNi44NDgyVjE5LjM0NzZMMTMuNDYxMSAyNC4xNTYzTDE3LjMwNjQgMTkuMzQ3NlYyNi44NDgySDIxLjE1MzRWMTMuNzcyMUgxNy4zMDY0TDEzLjQ2MTEgMTguNTgwOUw5LjYxNDEzIDEzLjc3MjFINS43NjcxNVYyNi44NTE2TDkuNjE0MTMgMjYuODQ4MlpNMzUuMzg2MyAyMC4zMDk0SDMxLjUzOTNWMTMuNzcwNUgyNy42OTRWMjAuMzA5NEgyMy44NDdMMjkuNjE1OCAyNy4wNDE2TDM1LjM4NjMgMjAuMzA5NFoiIGZpbGw9IiMzODM4MzkiLz4KPC9zdmc+Cg==) __![Qdrant Vector Store](data:image/svg+xml;base64,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)![Embeddings OpenAI](data:image/svg+xml;base64,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) +5\n\n[Effortless Email Management with AI-Powered Summarization & Review](https://n8n.io/workflows/2862-effortless-email-management-with-ai-powered-summarization-and-review/)\n\nby n3witalia\n\n[ Use this workflow ](https://n8n.io/workflows/2862-effortless-email-management-with-ai-powered-summarization-and-review/)\n\nThis workflow automates the handling of incoming emails, summarizes their content, generates responses using RAG, and obtains approval before sending replies. It listens for new emails, summarizes them, generates responses, and sends drafts for human review via [Gmail](https://docs.n8n.io/integrations/builtin/app-nodes/n8n-nodes-base.gmail/?ref=blog.n8n.io). If approved, the email is sent; otherwise, it's edited or handled manually. A text classifier categorizes feedback to guide the process.\n\n### RAG chatbot for company documents using Google Drive and Gemini\n\n![Pinecone Vector Store](data:image/svg+xml;base64,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)![Embeddings Google Gemini](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB2aWV3Qm94PSIwIDAgNDggNDgiPjxkZWZzPjxwYXRoIGlkPSJhIiBkPSJNNDQuNSAyMEgyNHY4LjVoMTEuOEMzNC43IDMzLjkgMzAuMSAzNyAyNCAzN2MtNy4yIDAtMTMtNS44LTEzLTEzczUuOC0xMyAxMy0xM2MzLjEgMCA1LjkgMS4xIDguMSAyLjlsNi40LTYuNEMzNC42IDQuMSAyOS42IDIgMjQgMiAxMS44IDIgMiAxMS44IDIgMjRzOS44IDIyIDIyIDIyYzExIDAgMjEtOCAyMS0yMiAwLTEuMy0uMi0yLjctLjUtNCIvPjwvZGVmcz48Y2xpcFBhdGggaWQ9ImIiPjx1c2UgeGxpbms6aHJlZj0iI2EiIG92ZXJmbG93PSJ2aXNpYmxlIi8+PC9jbGlwUGF0aD48cGF0aCBmaWxsPSIjRkJCQzA1IiBkPSJNMCAzN1YxMWwxNyAxM3oiIGNsaXAtcGF0aD0idXJsKCNiKSIvPjxwYXRoIGZpbGw9IiNFQTQzMzUiIGQ9Im0wIDExIDE3IDEzIDctNi4xTDQ4IDE0VjBIMHoiIGNsaXAtcGF0aD0idXJsKCNiKSIvPjxwYXRoIGZpbGw9IiMzNEE4NTMiIGQ9Im0wIDM3IDMwLTIzIDcuOSAxTDQ4IDB2NDhIMHoiIGNsaXAtcGF0aD0idXJsKCNiKSIvPjxwYXRoIGZpbGw9IiM0Mjg1RjQiIGQ9Ik00OCA0OCAxNyAyNGwtNC0zIDM1LTEweiIgY2xpcC1wYXRoPSJ1cmwoI2IpIi8+PC9zdmc+)![Default Data Loader](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI3NjgiIGhlaWdodD0iMTAyNCI+PHBhdGggZmlsbD0iIzdEN0Q4NyIgZD0iTTAgOTYwVjY0aDU3NmwxOTIgMTkydjcwNHptNzA0LTY0MEw1MTIgMTI4SDY0djc2OGg2NDB6TTMyMCA1MTJIMTI4VjI1NmgxOTJ6bS02NC0xOTJoLTY0djEyOGg2NHptMCA0NDhoNjR2NjRIMTI4di02NGg2NFY2NDBoLTY0di02NGgxMjh6bTI1Ni0zMjBoNjR2NjRIMzg0di02NGg2NFYzMjBoLTY0di02NGgxMjh6bTY0IDM4NEgzODRWNTc2aDE5MnptLTY0LTE5MmgtNjR2MTI4aDY0eiIvPjwvc3ZnPg==) ____ +5\n\n[RAG Chatbot for Company Documents using Google Drive and Gemini](https://n8n.io/workflows/2753-rag-chatbot-for-company-documents-using-google-drive-and-gemini/)\n\nby mihailtd\n\n[ Use this workflow ](https://n8n.io/workflows/2753-rag-chatbot-for-company-documents-using-google-drive-and-gemini/)\n\nThis workflow implements a Retrieval Augmented Generation (RAG) chatbot that answers employee questions based on company documents stored in [Google Drive](https://docs.n8n.io/integrations/builtin/app-nodes/n8n-nodes-base.googledrive/?ref=blog.n8n.io). It automatically indexes new or updated documents in a [Pinecone vector database](https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstorepinecone/?ref=blog.n8n.io), allowing the chatbot to provide accurate and up-to-date information. The workflow uses [Google's Gemini AI](https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatgooglegemini/?ref=blog.n8n.io) for both embeddings and response generation. When a new or updated document is detected, the workflow downloads it, splits it into chunks, embeds the chunks using Gemini, and stores them in Pinecone. The chatbot receives questions, retrieves relevant information from Pinecone based on the question, and generates answers using the Gemini chat model.\n\n## LangChain vs LlamaIndex vs n8n FAQ\n\n#### Which is better for building autonomous AI agents, Auto-GPT or LangChain?\n\nAuto-GPT is better for fully autonomous agents that operate with minimal human input, while LangChain offers greater flexibility for building custom workflows and integrating multiple external tools and APIs.\n\n#### ****Which tool is best for retrieval-augmented generation (RAG): LangChain, LlamaIndex, or Haystack?****\n\nHaystack is the best choice for search-heavy RAG applications, LlamaIndex excels at indexing and querying large datasets, and LangChain is ideal for orchestrating complex LLM workflows that involve both retrieval and external integrations.\n\n#### ****When should I use LangChain, LlamaIndex, or Hugging Face for an LLM project?****\n\nUse LangChain for complex workflows and multi-step logic, LlamaIndex for efficient data retrieval, and Hugging Face for accessing and fine-tuning pre-trained LLMs across a wide range of tasks.\n\n#### Can LangChain be used with Python only?\n\nLangChain offers official libraries for both Python and JavaScript. While the Python library was initially more mature, both libraries are now actively developed and maintained, offering comparable features and functionality. You can choose either language based on your preference and project requirements.\n\nThere are community-driven adaptations of LangChain for Java and Golang. Additionally, platforms like n8n provide a visual interface and pre-built nodes for LangChain.\n\n#### Can you use LangChain with Ollama local LLMs?\n\nYes, LangChain provides official components for integrating with Ollama. This can be beneficial for various reasons, including privacy, security, and cost-effectiveness.\n\n#### Can you use LangChain with Pinecone or other vector databases?\n\nLangChain is designed to work seamlessly with Pinecone and other vector databases. Integrating a vector database like Pinecone with LangChain is a common practice for building efficient and scalable RAG applications.\n\n#### What is LangSmith?\n\nLangSmith is a platform designed specifically for debugging, testing, and evaluating LLM applications. It helps developers improve the performance and reliability of their LangChain projects by providing tools to trace workflow execution and evaluate accuracy.\n\n## Wrap up\n\nIn this article, we provided a comparative analysis of LlamaIndex and LangChain, two powerful frameworks for building RAG systems. We highlighted their strengths, limitations, and use cases, and also introduced n8n as a compelling alternative for those seeking a low-code, integration-heavy solution.\n\nChoosing the right framework for your LLM application depends on your specific needs and priorities.\n\nIf you need a simple and efficient solution for data-centric tasks, LlamaIndex is a great choice. If you require greater flexibility and control for complex workflows, LangChain might be a better fit. And if you're looking for a broader automation platform that seamlessly integrates with LLMs, n8n offers a compelling alternative.\n\n### Create your RAG workflows with n8n\n\nGet started today and unlock flexible, scalable AI automation!\n\n[Try n8n now](https://app.n8n.cloud/register?ref=blog.n8n.io)\n\n## Whatâ€™s next?\n\nTo further enhance your understanding of RAG and LLM application development, check out these resources on the n8n blog:\n\n  * [**_LLM agents in 2025_**](https://blog.n8n.io/llm-agents/)**:** Learn about the latest advances in LLM agents and how they are being used in enterprise environments.\n  * [**_RAG chatbots_**](https://blog.n8n.io/rag-chatbot/)**:** Learn how to build a RAG chatbot that can access and process information from your documents or knowledge base using n8n.\n  * [**_Use n8n to integrate LangChain with local LLMs_**](https://blog.n8n.io/local-llm/)**:** Learn how to run LLMs like Deepseek locally on your own machine using n8n and Ollama.\n  * [**_AI-agentic workflows_**](https://blog.n8n.io/ai-agentic-workflows/)**:** Learn how AI agents can be used to automate tasks and make decisions in n8n workflows.\n  * [**_MLOps tools_**](https://blog.n8n.io/mlops-tools/)**:** Explore the different MLOps tools available and how they can be used to manage the entire lifecycle of your ML models.\n\n\n\n## Subscribe to n8n newsletter\n\nGet the best, coolest, and latest in automation and low-code delivered to your inbox each week. \n\nSomething went wrong. Please try again later.\n\nSubscribed!\n\nSubscribe\n\n[](https://twitter.com/intent/tweet?text=LlamaIndex%20vs%20LangChain%3A%20Which%20RAG%20tool%20is%20right%20for%20you%3F&url=https://blog.n8n.io/llamaindex-vs-langchain/) [](https://www.facebook.com/sharer/sharer.php?u=https://blog.n8n.io/llamaindex-vs-langchain/)\n",
  "crawled_at": "2025-05-28T10:46:28.822691"
}