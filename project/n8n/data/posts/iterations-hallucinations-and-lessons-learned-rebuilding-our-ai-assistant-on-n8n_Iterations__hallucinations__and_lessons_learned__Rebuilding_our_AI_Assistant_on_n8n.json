{
  "url": "https://blog.n8n.io/iterations-hallucinations-and-lessons-learned-rebuilding-our-ai-assistant-on-n8n/",
  "title": "Iterations, hallucinations, and lessons learned: Rebuilding our AI Assistant on n8n",
  "excerpt": "",
  "thumbnail": "https://blog.n8n.io/content/images/size/w1200/2025/02/rebuilding-ai-assistant1.png",
  "tags": [
    "AI"
  ],
  "html": "<p>We like drinking our own champagne at n8n, so when it came to rebuilding our internal AI assistant, we decided to see if we could do it using our own tooling. And as engineers who think in code, it was an enticing challenge to step away from the command line and experiment on building with workflows. It took us a few months, but we did it, and pretty successfully too! Plus, we learned some valuable lessons along the way. If you‚Äôre planning on building AI tools, we hope our endeavors will prove just as useful to you as they have for us.&nbsp;</p><h3 id=\"hard-code-hard-iterations\">Hard-code, hard iterations</h3><p>We‚Äôd been running a hard-coded internal AI Assistant for some time, but it was tricky to iterate on and far too complex for our PM colleagues to engage with. If you wanted to tweak the AI logic, improve a prompt, or just saw a potential efficiency gain, you needed to dive deep into the code. All-in-all, it was a solid piece of engineering but inaccessible to the folks who could benefit from it the most.&nbsp;</p><p>We liked the tooling we‚Äôd used for the original AI Assistant, so we knew we‚Äôd keep LangChain for orchestration and have everything running through GPT-4. But ultimately, we wanted to see if such a complicated AI use case could be built purely on n8n.&nbsp;</p><h3 id=\"the-ai-assistant\">The AI Assistant</h3><p>n8n‚Äôs Ai Assistant has three use cases:</p><ol><li>Debugging user errors</li><li>Answering natural language questions in a chat format</li><li>Helping users set up credentials&nbsp;</li></ol><p>At the backend, we run two huge vector sources that make up an internal Knowledge Base (KB) ‚Äì one is our documentation, the other is n8n‚Äôs Forum. We instruct our assistant to read the documentation first, to prevent hallucinations, and then turn to the Forum for further insights.&nbsp;</p><p>We set up our data in chunks. Each chunk is saved with context so the assistant can understand what part of a document it's reading, and the wider context surrounding it. Of course, we automated n8n workflows to scrape the documentation three times a week to update the database. At the same time, we also scrape the Forum for questions that have corresponding answers, and couple them together in the KB. Drinking our own champagne ;-)</p><p>Both the AI Service and our internal instance where the workflows live have development and production environments, so we can work on them without changing the production versions directly.</p><p>Here‚Äôs an overview of the main assistant components:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXc_O_ATF-L1_OMa2uhbmSleibIbGf87Swg9jp5tjRAjsGVdZJztFXwVjEqODpS0X1aLZFiEH1JUJi35FU6wl3mygDDHTe8kqtWQHRrMAEcF3s-FdIDqaGCSVMw9XzpPTnCyDhpQ?key=jkRWBvulaU0mlb-0kab9IZfI\" class=\"kg-image lightense-target\" alt=\"\" loading=\"lazy\" width=\"624\" height=\"191\"></figure><p></p><ol><li>n8n Front end: All messages sent by users from the assistant chat sidebar are first sent to our AI Service, which is a separate web service hosted internally.&nbsp;</li><li>AI Service: Handles authentication for the incoming requests and calls n8n webhook also with authentication so we make sure workflow accepts only requests from the AI Service</li><li>Assistant Workflows: Hosted on our internal instance. There is one main (Gateway) workflow that accepts webhook calls and routes to a specific agent, based on user mode.&nbsp;</li></ol><p>Under the hood we have four distinct agents that handle the Assistant‚Äôs use cases. We chose this setup because each use case requires different input context and tooling. When a user initiates the Assistant, we route the request using n8n's Switch node to one of the four agents. The only downside of this approach is that once an agent has started a user chat, it cannot switch to another agent while in-session, so the user needs to start again.&nbsp;</p><p><strong>Debugging user errors</strong></p><p>Generic error helper: Initiated when users click Ask Assistant button from node output panel when there is an error in the node. This agent is specialized in debugging node errors, has context about the error and access to n8n documentation and forum answers</p><p>Code node error helper: Specialized in debugging errors in the Code Node. This agent is initiated when users click Ask Assistant button from node output panel when there is an error in the Code Node. It has context about user‚Äôs code and access to n8n documentation. Beside answering questions this agent also can suggest code changes and apply them to currently open node if user chooses so.</p><p><strong>Answering natural language questions in a chat format</strong></p><p>Support agent: Responds to user requests when they just open up a chat and start asking questions. This agent has context about what users are currently seeing (workflow and nodes) and has access to n8n documentation and forum answers</p><p><strong>Helping users set up credentials&nbsp;</strong></p><p>Credentials helper: Initiated when users click on Ask Assistant button from the credentials modal. This agent has context about users‚Äô current node and credentials they want to set up and has access to n8n documentation</p><h3 id=\"implementation\">Implementation&nbsp;</h3><p>One of the trickiest things about working with AI is that the responses it produces can be completely unexpected. Ask a question one way, and you‚Äôll get a correct answer. Switch something as small as a name or number in the prompt, and you can end up with a wildly different response.&nbsp;</p><p>To mitigate this we started small, testing an agent on a user prompt like ‚Äúwhy am I seeing this?‚Äù. We quickly realized that we needed to provide more context because the assistant would not necessarily digest what was on the user‚Äôs screen before searching the KB. So we created a ‚Äúworkflow info‚Äù tool, which enables the Assistant to gather information about a specific workflow.</p><p>Now when users ask something about their workflow, or why they're seeing something specifically without explaining it, the assistant can use the ‚Äúworkflow info‚Äù tool to pull the error or the process from the context that is on the user‚Äôs screen. Today our support chat is delivering accurate answers by looking at the schemas the user is viewing, and using this as part of its search. We‚Äôve included an example below, so you can see how the agent has access to different tools to debug user problems.&nbsp;</p><figure class=\"kg-card kg-image-card\"><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdcsobtdG4fsXiwAFIjm0TGSCKaxSFe7_nY6qzsTUBAoKdDODrgXsStieQ4ff4_5u2O1p5CIWIexce-NdeSnktxVOqaCrx9-mTSJK-RVW8fr1c8mmOTLXr3UnSIQjLh0DQp8QL7wg?key=jkRWBvulaU0mlb-0kab9IZfI\" class=\"kg-image lightense-target\" alt=\"\" loading=\"lazy\" width=\"523\" height=\"372\"></figure><h4 id=\"ai-to-evaluate-ai\">AI to evaluate AI</h4><p>We save executions, so all our traces are available directly in n8n, which means we have a fantastic set of internal traces for evaluating tests and prompt changes. Since we have a rich dataset of traces, we thought ‚Äòwhy not use AI to fast-track the iteration process?‚Äô and rolled out an LLM to judge the responses our LLM was producing.</p><p>However, we made a rookie error in giving the same instructions to both the Assistant and the Judge:<em> make responses helpful, actionable, and brief</em>. This resulted in the Judge scoring every answer perfectly. So we iterated a bit on the framework.</p><p>Internally, we have a custom validation project set up in LangSmith where we can run the Assistant against different sample requests from our traces and get a score on answer quality. This enables us to test different prompts and models quickly. It took a lot of experimentation to arrive at a framework that works reliably, and of course, every time we change something we have to test it. But now we have almost 50 use cases and we can more accurately test how our changes are improving or decreasing the quality of responses.</p><p>Today, the LLM Judge receives a user prompt and the output from the Assistant, and it judges and scores the output against a set of instructions, such as ‚Äòshould X be included, is this a high-priority, is this output actionable?‚Äô etc. So far, the quality seems to be a bit better than our old model!</p><h3 id=\"lessons-learned\">Lessons learned&nbsp;</h3><p><strong>Time lag is ok</strong></p><p>Something we realized early on is that users don‚Äôt mind waiting a few seconds for useful responses. We assumed that an agent taking 10 seconds to stream a response would be a barrier to adoption, but it's not at all. So even though we‚Äôve noticed a slight increase in response times, we haven‚Äôt seen a decrease in error resolution.&nbsp;&nbsp;</p><p><strong>Iterate, iterate, iterate!</strong></p><p>As engineers, we often lean on our gut feelings in code ‚Äì you can see where to make tweaks and see how the program responds. But with AI, you can try asking the same question in a different way and end up with a completely different answer. And sometimes you change a prompt to find that it proves one use case, but it makes three others worse!</p><p>AI is so much less deterministic, and the way you evaluate and evolve the responses you‚Äôre getting has to be approached with a different mindset. Trial and error became the new normal for us as we iterated on every aspect of our assistant to see what would work. While this took some time investment, ultimately it paid off because we now have high-quality responses, and response times just keep improving.&nbsp;</p><p><strong>Be open beyond code&nbsp;</strong></p><p>We‚Äôre engineers and it was admittedly a challenge to switch our engineering outlook to move from working in code, to working in workflows. This project completely changed our minds on low-code approaches ‚Äì it was so much easier building and migrating our AI assistant than we‚Äôd envisaged and we‚Äôre really impressed with how successful this project has been on such a large-scale dataset.</p><p>&nbsp;&nbsp;&nbsp;</p><h3 id=\"what%E2%80%99s-next\">What‚Äôs next?</h3><p>Word got out pretty quickly about our new AI Assistant, and n8n‚Äôs Support Team is already leveraging the KB and AI workflows we built to see if they can improve the quality and speed of their responses.</p><p>We‚Äôre now looking at how we can increase the abilities of our AI Assistant over time ‚Äì like being able to build a workflow from a prompt. And because we‚Äôve set everything up in n8n, we can also easily experiment with different LLMs for auxiliary actions. We‚Äôre also considering introducing another AI Agent to the four that we currently run, so that users will be able to switch agents while in-session.</p><h4 id=\"give-it-a-spin\">Give it a spin</h4><p>We‚Äôre open source enthusiasts ‚Äì so of course we‚Äôve published AI Assistant workflows! Have a look at some of our popular templates below, and let us know what you think.&nbsp;</p><p><a href=\"https://n8n.io/workflows/2753-rag-chatbot-for-company-documents-using-google-drive-and-gemini/?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>RAG Chatbot for Company Documents using Google Drive and Gemini</u></a><br>üëâ This workflow implements a Retrieval Augmented Generation (RAG) chatbot that answers employee questions based on company documents stored in Google Drive. </p><p><a href=\"https://n8n.io/workflows/2850-bamboohr-ai-powered-company-policies-and-benefits-chatbot/?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>BambooHR AI-Powered Company Policies and Benefits Chatbot</u></a><br>üëâ This workflow enables companies to provide instant HR support by automating responses to employee queries about policies and benefits.</p><p>You can also connect with <a href=\"https://www.linkedin.com/in/niklashatje/?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\">Niklas</a> and <a href=\"https://www.linkedin.com/in/milorad-filipovi%C4%87-47188882/?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\">Milorad</a> on LinkedIn and learn directly from them. </p>\n\t\t<div class=\"newsletter-banner\">\n\t    <div class=\"newsletter-banner-content\">\n\t      <div class=\"section-header\">\n\t        <h2>Subscribe to <span>n8n newsletter</span></h2>\n\t        <div class=\"section-subheader--bottom\">\n\t          Get the best, coolest, and latest in automation and low-code delivered to your inbox each week.\n\t        </div>\n\t      </div>\n\t      <div class=\"newsletter-banner-form\">\n\t        <form autocomplete=\"off\" class=\"contact-form\" onsubmit=\"subscribeNewsletter(event)\">\n\t        \t<div id=\"recaptcha\" class=\"g-recaptcha\" data-sitekey=\"6LeAQeopAAAAAKlLsRb1weWm6T_vijoQBkGkbHzB\" data-callback=\"submitSubscription\" data-size=\"invisible\"><div class=\"grecaptcha-badge\" data-style=\"bottomright\" style=\"width: 256px; height: 60px; display: block; transition: right 0.3s ease 0s; position: fixed; bottom: 14px; right: -186px; box-shadow: gray 0px 0px 5px; border-radius: 2px; overflow: hidden;\"><div class=\"grecaptcha-logo\"><iframe title=\"reCAPTCHA\" width=\"256\" height=\"60\" role=\"presentation\" name=\"a-3nznlddoiieq\" frameborder=\"0\" scrolling=\"no\" sandbox=\"allow-forms allow-popups allow-same-origin allow-scripts allow-top-navigation allow-modals allow-popups-to-escape-sandbox allow-storage-access-by-user-activation\" src=\"https://www.google.com/recaptcha/api2/anchor?ar=1&amp;k=6LeAQeopAAAAAKlLsRb1weWm6T_vijoQBkGkbHzB&amp;co=aHR0cHM6Ly9ibG9nLm44bi5pbzo0NDM.&amp;hl=en&amp;v=jt8Oh2-Ue1u7nEbJQUIdocyd&amp;size=invisible&amp;cb=e07bpyymptc0\"></iframe></div><div class=\"grecaptcha-error\"></div><textarea id=\"g-recaptcha-response\" name=\"g-recaptcha-response\" class=\"g-recaptcha-response\" style=\"width: 250px; height: 40px; border: 1px solid rgb(193, 193, 193); margin: 10px 25px; padding: 0px; resize: none; display: none;\"></textarea></div><iframe style=\"display: none;\"></iframe></div>\n\t          <div class=\"input-wrapper\">\n\t            <input placeholder=\"Email\" name=\"email\" type=\"email\" required=\"required\" class=\"\">\n\t            <div class=\"messages\">\n\t              <div class=\"message message--error\">Something went wrong. Please try again later.</div>\n\t              <div class=\"message message--success\">Subscribed!</div>\n\t            </div>\n\t          </div>\n\t          <button type=\"submit\" class=\"submit-btn\">Subscribe</button>\n\t        </form>\n\t      </div>\n\t    </div>\n    </div>\n\t\t<div class=\"post-share-section\">\n\t<div class=\"post-share-wrap\">\n\t\t<a href=\"https://twitter.com/intent/tweet?text=Iterations%2C%20hallucinations%2C%20and%20lessons%20learned%3A%20Rebuilding%20our%20AI%20Assistant%20on%20n8n&amp;url=https://blog.n8n.io/iterations-hallucinations-and-lessons-learned-rebuilding-our-ai-assistant-on-n8n/\" target=\"_blank\" rel=\"noopener\" aria-label=\"Twitter share icon\"><svg role=\"img\" viewBox=\"0 0 24 24\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M23.954 4.569c-.885.389-1.83.654-2.825.775 1.014-.611 1.794-1.574 2.163-2.723-.951.555-2.005.959-3.127 1.184-.896-.959-2.173-1.559-3.591-1.559-2.717 0-4.92 2.203-4.92 4.917 0 .39.045.765.127 1.124C7.691 8.094 4.066 6.13 1.64 3.161c-.427.722-.666 1.561-.666 2.475 0 1.71.87 3.213 2.188 4.096-.807-.026-1.566-.248-2.228-.616v.061c0 2.385 1.693 4.374 3.946 4.827-.413.111-.849.171-1.296.171-.314 0-.615-.03-.916-.086.631 1.953 2.445 3.377 4.604 3.417-1.68 1.319-3.809 2.105-6.102 2.105-.39 0-.779-.023-1.17-.067 2.189 1.394 4.768 2.209 7.557 2.209 9.054 0 13.999-7.496 13.999-13.986 0-.209 0-.42-.015-.63.961-.689 1.8-1.56 2.46-2.548l-.047-.02z\"></path></svg></a>\n\t\t<a href=\"https://www.facebook.com/sharer/sharer.php?u=https://blog.n8n.io/iterations-hallucinations-and-lessons-learned-rebuilding-our-ai-assistant-on-n8n/\" target=\"_blank\" rel=\"noopener\" aria-label=\"Facebook share icon\"><svg role=\"img\" viewBox=\"0 0 24 24\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M23.9981 11.9991C23.9981 5.37216 18.626 0 11.9991 0C5.37216 0 0 5.37216 0 11.9991C0 17.9882 4.38789 22.9522 10.1242 23.8524V15.4676H7.07758V11.9991H10.1242V9.35553C10.1242 6.34826 11.9156 4.68714 14.6564 4.68714C15.9692 4.68714 17.3424 4.92149 17.3424 4.92149V7.87439H15.8294C14.3388 7.87439 13.8739 8.79933 13.8739 9.74824V11.9991H17.2018L16.6698 15.4676H13.8739V23.8524C19.6103 22.9522 23.9981 17.9882 23.9981 11.9991Z\"></path></svg></a>\n\t\t<!-- <a href=\"javascript:\" class=\"post-share-link\" id=\"copy\" data-clipboard-target=\"#copy-link\" aria-label=\"Copy link icon\"><svg role=\"img\" viewBox=\"0 0 33 24\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M27.3999996,13.4004128 L21.7999996,13.4004128 L21.7999996,19 L18.9999996,19 L18.9999996,13.4004128 L13.3999996,13.4004128 L13.3999996,10.6006192 L18.9999996,10.6006192 L18.9999996,5 L21.7999996,5 L21.7999996,10.6006192 L27.3999996,10.6006192 L27.3999996,13.4004128 Z M12,20.87 C7.101,20.87 3.13,16.898 3.13,12 C3.13,7.102 7.101,3.13 12,3.13 C12.091,3.13 12.181,3.139 12.272,3.142 C9.866,5.336 8.347,8.487 8.347,12 C8.347,15.512 9.866,18.662 12.271,20.857 C12.18,20.859 12.091,20.87 12,20.87 Z M20.347,0 C18.882,0 17.484,0.276 16.186,0.756 C14.882,0.271 13.473,0 12,0 C5.372,0 0,5.373 0,12 C0,18.628 5.372,24 12,24 C13.471,24 14.878,23.726 16.181,23.242 C17.481,23.724 18.88,24 20.347,24 C26.975,24 32.347,18.628 32.347,12 C32.347,5.373 26.975,0 20.347,0 Z\"/></svg></a>\n\t\t<small class=\"share-link-info\">The link has been copied!</small> -->\n\t</div>\n\t<input type=\"text\" value=\"https://blog.n8n.io/iterations-hallucinations-and-lessons-learned-rebuilding-our-ai-assistant-on-n8n/\" id=\"copy-link\" aria-label=\"Copy link input\">\n</div>",
  "readme": "We like drinking our own champagne at n8n, so when it came to rebuilding our internal AI assistant, we decided to see if we could do it using our own tooling. And as engineers who think in code, it was an enticing challenge to step away from the command line and experiment on building with workflows. It took us a few months, but we did it, and pretty successfully too! Plus, we learned some valuable lessons along the way. If you‚Äôre planning on building AI tools, we hope our endeavors will prove just as useful to you as they have for us. \n\n### Hard-code, hard iterations\n\nWe‚Äôd been running a hard-coded internal AI Assistant for some time, but it was tricky to iterate on and far too complex for our PM colleagues to engage with. If you wanted to tweak the AI logic, improve a prompt, or just saw a potential efficiency gain, you needed to dive deep into the code. All-in-all, it was a solid piece of engineering but inaccessible to the folks who could benefit from it the most. \n\nWe liked the tooling we‚Äôd used for the original AI Assistant, so we knew we‚Äôd keep LangChain for orchestration and have everything running through GPT-4. But ultimately, we wanted to see if such a complicated AI use case could be built purely on n8n. \n\n### The AI Assistant\n\nn8n‚Äôs Ai Assistant has three use cases:\n\n  1. Debugging user errors\n  2. Answering natural language questions in a chat format\n  3. Helping users set up credentials \n\n\n\nAt the backend, we run two huge vector sources that make up an internal Knowledge Base (KB) ‚Äì one is our documentation, the other is n8n‚Äôs Forum. We instruct our assistant to read the documentation first, to prevent hallucinations, and then turn to the Forum for further insights. \n\nWe set up our data in chunks. Each chunk is saved with context so the assistant can understand what part of a document it's reading, and the wider context surrounding it. Of course, we automated n8n workflows to scrape the documentation three times a week to update the database. At the same time, we also scrape the Forum for questions that have corresponding answers, and couple them together in the KB. Drinking our own champagne ;-)\n\nBoth the AI Service and our internal instance where the workflows live have development and production environments, so we can work on them without changing the production versions directly.\n\nHere‚Äôs an overview of the main assistant components:\n\n![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXc_O_ATF-L1_OMa2uhbmSleibIbGf87Swg9jp5tjRAjsGVdZJztFXwVjEqODpS0X1aLZFiEH1JUJi35FU6wl3mygDDHTe8kqtWQHRrMAEcF3s-FdIDqaGCSVMw9XzpPTnCyDhpQ?key=jkRWBvulaU0mlb-0kab9IZfI)\n\n  1. n8n Front end: All messages sent by users from the assistant chat sidebar are first sent to our AI Service, which is a separate web service hosted internally. \n  2. AI Service: Handles authentication for the incoming requests and calls n8n webhook also with authentication so we make sure workflow accepts only requests from the AI Service\n  3. Assistant Workflows: Hosted on our internal instance. There is one main (Gateway) workflow that accepts webhook calls and routes to a specific agent, based on user mode. \n\n\n\nUnder the hood we have four distinct agents that handle the Assistant‚Äôs use cases. We chose this setup because each use case requires different input context and tooling. When a user initiates the Assistant, we route the request using n8n's Switch node to one of the four agents. The only downside of this approach is that once an agent has started a user chat, it cannot switch to another agent while in-session, so the user needs to start again. \n\n**Debugging user errors**\n\nGeneric error helper: Initiated when users click Ask Assistant button from node output panel when there is an error in the node. This agent is specialized in debugging node errors, has context about the error and access to n8n documentation and forum answers\n\nCode node error helper: Specialized in debugging errors in the Code Node. This agent is initiated when users click Ask Assistant button from node output panel when there is an error in the Code Node. It has context about user‚Äôs code and access to n8n documentation. Beside answering questions this agent also can suggest code changes and apply them to currently open node if user chooses so.\n\n**Answering natural language questions in a chat format**\n\nSupport agent: Responds to user requests when they just open up a chat and start asking questions. This agent has context about what users are currently seeing (workflow and nodes) and has access to n8n documentation and forum answers\n\n**Helping users set up credentials  **\n\nCredentials helper: Initiated when users click on Ask Assistant button from the credentials modal. This agent has context about users‚Äô current node and credentials they want to set up and has access to n8n documentation\n\n### Implementation \n\nOne of the trickiest things about working with AI is that the responses it produces can be completely unexpected. Ask a question one way, and you‚Äôll get a correct answer. Switch something as small as a name or number in the prompt, and you can end up with a wildly different response. \n\nTo mitigate this we started small, testing an agent on a user prompt like ‚Äúwhy am I seeing this?‚Äù. We quickly realized that we needed to provide more context because the assistant would not necessarily digest what was on the user‚Äôs screen before searching the KB. So we created a ‚Äúworkflow info‚Äù tool, which enables the Assistant to gather information about a specific workflow.\n\nNow when users ask something about their workflow, or why they're seeing something specifically without explaining it, the assistant can use the ‚Äúworkflow info‚Äù tool to pull the error or the process from the context that is on the user‚Äôs screen. Today our support chat is delivering accurate answers by looking at the schemas the user is viewing, and using this as part of its search. We‚Äôve included an example below, so you can see how the agent has access to different tools to debug user problems. \n\n![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdcsobtdG4fsXiwAFIjm0TGSCKaxSFe7_nY6qzsTUBAoKdDODrgXsStieQ4ff4_5u2O1p5CIWIexce-NdeSnktxVOqaCrx9-mTSJK-RVW8fr1c8mmOTLXr3UnSIQjLh0DQp8QL7wg?key=jkRWBvulaU0mlb-0kab9IZfI)\n\n#### AI to evaluate AI\n\nWe save executions, so all our traces are available directly in n8n, which means we have a fantastic set of internal traces for evaluating tests and prompt changes. Since we have a rich dataset of traces, we thought ‚Äòwhy not use AI to fast-track the iteration process?‚Äô and rolled out an LLM to judge the responses our LLM was producing.\n\nHowever, we made a rookie error in giving the same instructions to both the Assistant and the Judge:_make responses helpful, actionable, and brief_. This resulted in the Judge scoring every answer perfectly. So we iterated a bit on the framework.\n\nInternally, we have a custom validation project set up in LangSmith where we can run the Assistant against different sample requests from our traces and get a score on answer quality. This enables us to test different prompts and models quickly. It took a lot of experimentation to arrive at a framework that works reliably, and of course, every time we change something we have to test it. But now we have almost 50 use cases and we can more accurately test how our changes are improving or decreasing the quality of responses.\n\nToday, the LLM Judge receives a user prompt and the output from the Assistant, and it judges and scores the output against a set of instructions, such as ‚Äòshould X be included, is this a high-priority, is this output actionable?‚Äô etc. So far, the quality seems to be a bit better than our old model!\n\n### Lessons learned \n\n**Time lag is ok**\n\nSomething we realized early on is that users don‚Äôt mind waiting a few seconds for useful responses. We assumed that an agent taking 10 seconds to stream a response would be a barrier to adoption, but it's not at all. So even though we‚Äôve noticed a slight increase in response times, we haven‚Äôt seen a decrease in error resolution.  \n\n**Iterate, iterate, iterate!**\n\nAs engineers, we often lean on our gut feelings in code ‚Äì you can see where to make tweaks and see how the program responds. But with AI, you can try asking the same question in a different way and end up with a completely different answer. And sometimes you change a prompt to find that it proves one use case, but it makes three others worse!\n\nAI is so much less deterministic, and the way you evaluate and evolve the responses you‚Äôre getting has to be approached with a different mindset. Trial and error became the new normal for us as we iterated on every aspect of our assistant to see what would work. While this took some time investment, ultimately it paid off because we now have high-quality responses, and response times just keep improving. \n\n**Be open beyond code  **\n\nWe‚Äôre engineers and it was admittedly a challenge to switch our engineering outlook to move from working in code, to working in workflows. This project completely changed our minds on low-code approaches ‚Äì it was so much easier building and migrating our AI assistant than we‚Äôd envisaged and we‚Äôre really impressed with how successful this project has been on such a large-scale dataset.\n\n   \n\n### What‚Äôs next?\n\nWord got out pretty quickly about our new AI Assistant, and n8n‚Äôs Support Team is already leveraging the KB and AI workflows we built to see if they can improve the quality and speed of their responses.\n\nWe‚Äôre now looking at how we can increase the abilities of our AI Assistant over time ‚Äì like being able to build a workflow from a prompt. And because we‚Äôve set everything up in n8n, we can also easily experiment with different LLMs for auxiliary actions. We‚Äôre also considering introducing another AI Agent to the four that we currently run, so that users will be able to switch agents while in-session.\n\n#### Give it a spin\n\nWe‚Äôre open source enthusiasts ‚Äì so of course we‚Äôve published AI Assistant workflows! Have a look at some of our popular templates below, and let us know what you think. \n\n[_RAG Chatbot for Company Documents using Google Drive and Gemini_](https://n8n.io/workflows/2753-rag-chatbot-for-company-documents-using-google-drive-and-gemini/?ref=blog.n8n.io)  \nüëâ This workflow implements a Retrieval Augmented Generation (RAG) chatbot that answers employee questions based on company documents stored in Google Drive. \n\n[_BambooHR AI-Powered Company Policies and Benefits Chatbot_](https://n8n.io/workflows/2850-bamboohr-ai-powered-company-policies-and-benefits-chatbot/?ref=blog.n8n.io)  \nüëâ This workflow enables companies to provide instant HR support by automating responses to employee queries about policies and benefits.\n\nYou can also connect with [Niklas](https://www.linkedin.com/in/niklashatje/?ref=blog.n8n.io) and [Milorad](https://www.linkedin.com/in/milorad-filipovi%C4%87-47188882/?ref=blog.n8n.io) on LinkedIn and learn directly from them. \n\n## Subscribe to n8n newsletter\n\nGet the best, coolest, and latest in automation and low-code delivered to your inbox each week. \n\nSomething went wrong. Please try again later.\n\nSubscribed!\n\nSubscribe\n\n[](https://twitter.com/intent/tweet?text=Iterations%2C%20hallucinations%2C%20and%20lessons%20learned%3A%20Rebuilding%20our%20AI%20Assistant%20on%20n8n&url=https://blog.n8n.io/iterations-hallucinations-and-lessons-learned-rebuilding-our-ai-assistant-on-n8n/) [](https://www.facebook.com/sharer/sharer.php?u=https://blog.n8n.io/iterations-hallucinations-and-lessons-learned-rebuilding-our-ai-assistant-on-n8n/)\n",
  "crawled_at": "2025-05-28T10:46:51.053283"
}