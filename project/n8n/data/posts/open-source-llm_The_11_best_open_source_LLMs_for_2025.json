{
  "url": "https://blog.n8n.io/open-source-llm/",
  "title": "The 11 best open-source LLMs for 2025",
  "excerpt": "Discover these top 11 open-source LLMs and build advanced AI workflows with n8n LangChain integration.",
  "thumbnail": "https://blog.n8n.io/content/images/size/w1200/2025/01/11-os-llm--1-.jpg",
  "tags": [
    "AI",
    "Guide"
  ],
  "html": "<p>Open-source models are changing the LLM landscape, promising better security, cost-efficiency, and customization for AI deployments. While <a href=\"https://nerdynav.com/chatgpt-statistics/?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>ChatGPT has over 180 million users</u></a>, on-premises solutions already control more than half of the LLM market, with <a href=\"https://market.us/report/large-language-model-llm-market/?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>projections indicating continued growth</u></a> in the coming years.</p><p>The trend is clear: since early 2023, new open-source model releases have nearly doubled compared to their closed-source counterparts.</p><figure class=\"kg-card kg-image-card kg-width-wide kg-card-hascaption\"><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdk1JmwPe5tLC6-XHppDpkBwOXd1t-WL4Pu871KVEdheZm03QYtJPD4WU1OXmG_ZiQDp-zHiN9BU5CruVQslAXf4QfqZd5mYPY4oUGbONPORbmVy9AZcy0hUh3QzsiBZ5ca46m1uw?key=INQ3nGlG9V9oPWqw4SkZT1dt\" class=\"kg-image lightense-target\" alt=\"LLM releases by year: blue cards = pre-trained models, orange cards = instruction-tuned. Top half shows open-source models, bottom half contains closed-source ones. Source: https://arxiv.org/abs/2307.06435\" loading=\"lazy\" width=\"624\" height=\"215\"><figcaption><span style=\"white-space: pre-wrap;\">LLM releases by year: blue cards = pre-trained models, orange cards = instruction-tuned. Top half shows open-source models, bottom half contains closed-source ones. Source: </span><a href=\"https://arxiv.org/abs/2307.06435?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u><span class=\"underline\" style=\"white-space: pre-wrap;\">https://arxiv.org/abs/2307.06435</span></u></a></figcaption></figure><p>Today, we‚Äôll dive into the world of open-source LLMs and:</p><ul><li>discuss the reasons behind the surge in open-source LLM deployments;</li><li>recognize potential pitfalls and challenges;</li><li>review the 11 best open-source LLMs on the market;</li><li>show you how to easily access these powerful open-source AI models;</li><li>guide you on how to get started with open-source LLMs using <a href=\"https://n8n.io/integrations/categories/ai/?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>Ollama and LangChain in n8n</u></a>.</li></ul><p>Read on to find out!</p><h2 id=\"are-there-any-open-source-llms\">Are there any open-source LLMs?</h2><p>For this article, we‚Äôve selected 11 popular open-source LLM models, focusing on both widely used and available in <a href=\"https://ollama.com/library?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>Ollama</u></a>.</p><p>Our review covers a range of pre-trained ‚Äúbase‚Äù models and their fine-tuned variants. These models come in various sizes, and you can either use them directly or opt for fine-tuned versions from original developers or third-party sources.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">While pre-trained models provide a strong foundation, fine-tuned versions are typically necessary for practical, task-specific applications. Many vendors offer pre-fine-tuned models, but users can create their own datasets to further fine-tune for more specialized use cases.</div></div><p>Here's our open-source LLM leaderboard:</p><table>\n<thead>\n<tr>\n<th>Model<br>Family</th>\n<th>Developer</th>\n<th>Params</th>\n<th>Context<br>window</th>\n<th>Use-cases</th>\n<th>License</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"#llama3\">Llama 3</a></td>\n<td>Meta</td>\n<td>1B, 3B, 8B,<br>70B, 405B</td>\n<td>8k, 128k</td>\n<td>- General text generation<br>- Multilingual tasks<br>- Code generation<br>- Long-form content<br>- Fine-tuning for specific domains</td>\n<td>Llama Community License</td>\n</tr>\n<tr>\n<td><a href=\"#mistral\">Mistral</a></td>\n<td>Mistral AI</td>\n<td>3B-124B</td>\n<td>32k-128k</td>\n<td>- High-complexity tasks<br>- Multilingual processing<br>- Code generation<br>- Image understanding<br>- Edge computing<br>- On-device AI<br>- Function calling<br>- Efficient large-scale processing</td>\n<td>Apache 2.0<br>Mistral Research License<br>Commercial License</td>\n</tr>\n<tr>\n<td><a href=\"#falcon-3\">Falcon 3</a></td>\n<td>TII</td>\n<td>1B, 3B,<br>7B, 10B</td>\n<td>8k-32k</td>\n<td>- General text generation<br>- Code generation<br>- Mathematical tasks<br>- Scientific knowledge<br>- Multilingual applications<br>- Fine-tuning for specific domains</td>\n<td>TII Falcon License</td>\n</tr>\n<tr>\n<td><a href=\"#gemma-2\">Gemma 2</a></td>\n<td>Google</td>\n<td>2B, 9B, 27B</td>\n<td>8k</td>\n<td>- General text generation<br>- Question answering<br>- Summarization<br>- Code generation<br>- Fine-tuning for specific domains</td>\n<td>Gemma license</td>\n</tr>\n<tr>\n<td><a href=\"#phi-3x-4\">Phi-3.x / 4</a></td>\n<td>Microsoft</td>\n<td>3.8B (mini)<br>7B (small)<br>14B (medium)<br>42B (MoE)</td>\n<td>4k, 8k, 128k<br>16k (Phi-4)</td>\n<td>- General text generation<br>- Multi-lingual tasks<br>- Code understanding<br>- Math reasoning<br>- Image understanding (vision model)<br>- On-device inference</td>\n<td>Microsoft Research<br>License</td>\n</tr>\n<tr>\n<td><a href=\"#command-r\">Command R</a></td>\n<td>Cohere</td>\n<td>7B, 35B, 104B</td>\n<td>128k</td>\n<td>- Conversational AI<br>- RAG<br>- Tool use<br>- Multilingual tasks<br>- Long-form content generation</td>\n<td>CC-BY-NC 4.0</td>\n</tr>\n<tr>\n<td><a href=\"#stablelm\">StableLM 2</a></td>\n<td>Stability AI</td>\n<td>1.6B, 3B, 12B</td>\n<td>Up to 16k</td>\n<td>- Multilingual text generation<br>- Code generation and understanding<br>- Fine-tuning for specific tasks<br>- Research and commercial applications</td>\n<td>Stability AI Community<br>and Enterprise licenses</td>\n</tr>\n<tr>\n<td><a href=\"#starcoder\">StarCoder2</a></td>\n<td>BigCode</td>\n<td>3B, 7B, 15B</td>\n<td>16k</td>\n<td>- Code completion<br>- Multi-language programming<br>- Code understanding<br>- Fine-tuning for specific tasks</td>\n<td>Apache 2.0</td>\n</tr>\n<tr>\n<td><a href=\"#yi\">Yi</a></td>\n<td>01.AI</td>\n<td>6B, 9B, 34B</td>\n<td>4k, 8k, 200k</td>\n<td>- Bilingual text generation<br>- Code understanding and generation<br>- Math and reasoning tasks<br>- Fine-tuning for specific domains</td>\n<td>Apache 2.0</td>\n</tr>\n<tr>\n<td><a href=\"#qwen25\">Qwen2.5</a></td>\n<td>Alibaba</td>\n<td>0.5B to 72B</td>\n<td>128K</td>\n<td>- General text generation<br>- Multilingual tasks<br>- Code generation<br>- Mathematical reasoning<br>- Structured data processing</td>\n<td>Qwen license<br>(3B and 72B size models)<br>Apache 2.0 (others)</td>\n</tr>\n<tr>\n<td><a href=\"#deepseek-2x-3\">DeepSeek-V2.x/V3</a></td>\n<td>DeepSeek AI</td>\n<td>16B, 236B,<br>671B for V3<br>(2.4B-37B<br>activated)</td>\n<td>32k-128k</td>\n<td>- General text generation<br>- Multilingual tasks<br>- Code generation<br>- Fine-tuning<br>- Advanced reasoning (V3)</td>\n<td>DeepSeek License</td>\n</tr>\n</tbody>\n</table>\n<p>For a comprehensive list of available LLMs beyond our selection, you can explore the <a href=\"https://github.com/Hannibal046/Awesome-LLM?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>Awesome-LLM GitHub repository</u></a>, which provides an extensive catalog of language models and related resources.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Did you know that in <a href=\"https://n8n.io/integrations/?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>n8n ‚Äì a workflow automation tool</u></a> ‚Äì you can use open-source LLMs in several ways. <br><br><b><strong style=\"white-space: pre-wrap;\">First</strong></b>, there is a dedicated node to connect to Ollama models ‚Äì the easiest way to start working with locally deployed LLMs.<br><br><b><strong style=\"white-space: pre-wrap;\">Second</strong></b>, an OpenAI node allows you to specify a custom endpoint. This way you can swap between OpenAI and open-source LLMs ‚Äì a perfect solution for working with OpenRouter.<br><br><b><strong style=\"white-space: pre-wrap;\">Finally</strong></b>, there are several nodes to other providers, such as HuggingFace, and even a custom HTTP Request node. Thanks to the straightforward user interface in n8n, your LLM-powered workflow automations and AI-agents remain the same when you switch between models. To easily deploy a local model, begin with a <a href=\"https://github.com/n8n-io/self-hosted-ai-starter-kit?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>n8n‚Äôs self-hosted AI starter kit with Ollama</u></a> integration.</div></div><h2 id=\"what-are-the-advantages-and-disadvantages-of-open-source-llms\">What are the advantages and disadvantages of open-source LLMs?</h2><p>Open-source LLMs offer several advantages beyond publicly available model weights and increased transparency:</p><ul><li>Full ownership ensures complete control over the model, additional training data, and practical applications.</li><li>Better fine-tuning accuracy is possible due to flexible customization of local model parameters, supported by community contributions.</li><li>Longevity is guaranteed as self-hosted models don‚Äôt become obsolete, unlike closed-source providers who may ‚Äúretire‚Äù older models.</li><li>Better cost estimation is possible as expenses shift from potentially volatile usage-based pricing to infrastructure costs. However, total costs may exceed subscription-based services, depending on usage patterns and infrastructure choices.</li><li>Flexibility in choosing software and hardware combinations allows for optimal resource allocation based on specific needs.</li><li>Community contributions enable model optimization through techniques like quantization and pruning, as well as the development of efficient deployment strategies and supporting tools.</li></ul><p>Despite their benefits, open-source LLMs come with some potential drawbacks:</p><ul><li>Quality may not match solutions offered by large corporations due to limited resources.</li><li>Vulnerability to attacks is a concern, as bad actors can potentially manipulate input data and interfere with the model‚Äôs behavior in open-source environments.</li><li>License requirements vary widely. Some models use permissive licenses (like Apache 2.0), others have non-commercial restrictions, and some (like Meta Llama 3) include specific terms for commercial usage.</li></ul><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üîó</div><div class=\"kg-callout-text\">LLMs are commonly used for <a href=\"https://blog.n8n.io/open-source-chatbot/\"><u>chatbots</u></a>, <a href=\"https://blog.n8n.io/llm-agents/\"><u>AI agents</u></a> and <a href=\"https://blog.n8n.io/ai-agentic-workflows/\"><u>workflow automations</u></a>. Check out our earlier blog articles.</div></div><h2 id=\"what-is-the-best-open-source-llm\">What is the best open-source LLM?</h2><p>There is no single best open-source LLM.&nbsp;</p><p>And here‚Äôs why.</p><p>There are many benchmarks for rating the models, and various research groups decide which benchmarks are suitable. This makes objective comparison rather non-trivial.</p><p>Thanks to the Hugging Face, there is a <a href=\"https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>public leaderboard for the open-source LLMs</u></a>.</p><p>It <a href=\"https://huggingface.co/docs/leaderboards/open_llm_leaderboard/about?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>performs tests on 6 key benchmarks</u></a> using the Eleuther AI Language Model Evaluation Harness. The results are aggregated and each model receives a final score.</p><p>The leaderboard has several quick filters for consumer-grade, edge device models and so on. Several adjustable columns such as model size, quantization method, etc. are also available.</p><p>The leaderboard is an open competition and anyone can submit their model for evaluation.</p><p>Let‚Äôs take open-source LLMs one by one and have a closer look at them!</p><h3 id=\"llama3\">Llama3</h3><p><strong>Best for</strong>: general-purpose applications with scalability needs</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXcNs-EXXhwbBahLP_uGL0DEP8hPIEj81LWKRqUr13CS4XlEExOU3GMPZvDpZSn_mOMi9eXPjQxJqW9WxqcXU6Y56okQgEHgl-Da2YBdBomvAhusoiYvMnzyxvM-NcpbFXRpXsmA?key=INQ3nGlG9V9oPWqw4SkZT1dt\" class=\"kg-image lightense-target\" alt=\"Llama3 is great for general-purpose applications with scalability needs\" loading=\"lazy\" width=\"624\" height=\"312\"><figcaption><span style=\"white-space: pre-wrap;\">Llama3 is great for general-purpose applications with scalability needs</span></figcaption></figure><p><a href=\"https://ai.meta.com/blog/meta-llama-3/?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>Llama 3</u></a> is Meta‚Äôs latest generation of open-source large language models, offering high performance across a wide range of tasks. The latest Llama 3.3 70B model offers performance comparable to the 405B parameter model at a fraction of the computational cost, making it an attractive option for developers and researchers.</p><div class=\"kg-card kg-callout-card kg-callout-card-red\"><div class=\"kg-callout-emoji\">‚öôÔ∏è</div><div class=\"kg-callout-text\"><b><strong style=\"white-space: pre-wrap;\">Llama 3 key features</strong></b></div></div><ul><li>Multiple model sizes: 1B, 3B, 8B, 70B, and 405B parameters</li><li>Multilingual and multimodal capabilities</li><li><a href=\"https://www.ibm.com/think/topics/grouped-query-attention?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>Grouped Query Attention</u></a> (GQA) for improved inference efficiency</li><li>Context windows of 8k tokens for smaller models, up to 128k tokens for larger models</li><li>Responsible AI development with tools like Llama Guard 2 and Code Shield</li></ul><div class=\"kg-card kg-callout-card kg-callout-card-accent\"><div class=\"kg-callout-emoji\">ü¶æ</div><div class=\"kg-callout-text\"><b><strong style=\"white-space: pre-wrap;\">Llama 3 use cases</strong></b></div></div><ul><li>General-purpose text generation and understanding</li><li>Multilingual applications across various languages</li><li>Code generation and understanding</li><li>Long-form content creation and analysis</li><li>Fine-tuning for specific domains or tasks</li><li>Assistant-like interactions in chatbots and AI applications</li></ul><h3 id=\"mistral\">Mistral</h3><p><strong>Best for</strong>: on-device AI with function calling</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXeDPDSDS0Zu7yMbO_1NciZn8EUNfBbbvtlsb4LXxyiUOCYfLHtuzjubjNBwh9Qjt2IUVTpBuLKPgYM8lhZuctOdQug0ZwbkvMjg2-dqerMV_ggpO6eTMTMqWr-Tng7mm1yXNk3L?key=INQ3nGlG9V9oPWqw4SkZT1dt\" class=\"kg-image lightense-target\" alt=\"Mistral models are best for on-device AI with function calling\" loading=\"lazy\" width=\"624\" height=\"312\"><figcaption><span style=\"white-space: pre-wrap;\">Mistral models are best for on-device AI with function calling</span></figcaption></figure><p><a href=\"https://mistral.ai/technology/?ref=blog.n8n.io#models\" target=\"_blank\" rel=\"noopener\"><u>Mistral AI</u></a>, a French startup, has rapidly become a major player in the open-source LLM space. Mistral‚Äôs models are designed to cater to a wide range of applications, from edge devices to large-scale enterprise solutions. The company offers both open-source models under Apache 2.0 license and commercial models with negotiable licenses. The latest Ministral model (3B and 8B) is particularly noteworthy for its performance in edge computing scenarios, outperforming similarly-sized models from tech giants.</p><div class=\"kg-card kg-callout-card kg-callout-card-red\"><div class=\"kg-callout-emoji\">‚öôÔ∏è</div><div class=\"kg-callout-text\"><b><strong style=\"white-space: pre-wrap;\">Mistral AI key features</strong></b></div></div><ul><li>Multiple model sizes: from 3B to 124B parameters</li><li>Multilingual and multimodal capabilities</li><li>Large context windows up to 128k tokens</li><li>Native function calling support</li><li>Mixture-of-experts (MoE) architecture in some models</li><li>Efficient models for edge computing and on-device AI</li><li>Fine-tuning capabilities for specific domains or tasks</li></ul><div class=\"kg-card kg-callout-card kg-callout-card-accent\"><div class=\"kg-callout-emoji\">ü¶æ</div><div class=\"kg-callout-text\"><b><strong style=\"white-space: pre-wrap;\">Mistral AI use cases</strong></b></div></div><ul><li>General-purpose text generation and understanding</li><li>High-complexity reasoning and problem-solving</li><li>Code generation and understanding</li><li>Image analysis and multimodal tasks</li><li>On-device AI for smartphones and laptops</li><li>Efficient large-scale processing with MoE models</li></ul><h3 id=\"falcon-3\">Falcon 3</h3><p><strong>Best for</strong>: resource-constrained environments</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXf1xoWpmVt4CR8s4DA5AgxLZW7MwJnl22H-C8a7TzRSvJdLHXQEjvkh-m0zOBBpC6SdVqhNfNwI6OlyNF_pZBv7zhEi7yo5kjVa4c7WKBcZk1lCnJEXaRjlJkJZQGtN6aXkOZt3mg?key=INQ3nGlG9V9oPWqw4SkZT1dt\" class=\"kg-image lightense-target\" alt=\"Falcon 3 models shine in the resource-constrained environments\" loading=\"lazy\" width=\"624\" height=\"312\"><figcaption><span style=\"white-space: pre-wrap;\">Falcon 3 models shine in the resource-constrained environments</span></figcaption></figure><p><a href=\"https://falconllm.tii.ae/falcon3/index.html?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>Falcon 3</u></a> is the latest iteration of open-source large language models developed by the Technology Innovation Institute (TII) in Abu Dhabi. This family of models demonstrates impressive performance for small LLMs while democratizing access to advanced AI by enabling efficient operation on light infrastructures, including laptops.</p><div class=\"kg-card kg-callout-card kg-callout-card-red\"><div class=\"kg-callout-emoji\">‚öôÔ∏è</div><div class=\"kg-callout-text\"><b><strong style=\"white-space: pre-wrap;\">Falcon 3 key features</strong></b></div></div><ul><li>Multiple model sizes: 1B, 3B, 7B, and 10B parameters</li><li>Trained on 14 trillion tokens, more than double its predecessor</li><li>Superior reasoning and enhanced fine-tuning capabilities</li><li>Extended context windows up to 32k tokens (except 1B model with 8k)</li><li>Multilingual support (English, French, Spanish, and Portuguese)</li><li>Falcon3-Mamba-7B variant using an alternative <a href=\"https://thegradient.pub/mamba-explained/?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>State Space Model (SSM) architecture</u></a></li></ul><div class=\"kg-card kg-callout-card kg-callout-card-accent\"><div class=\"kg-callout-emoji\">ü¶æ</div><div class=\"kg-callout-text\"><b><strong style=\"white-space: pre-wrap;\">Falcon 3 use cases</strong></b></div></div><ul><li>General-purpose text generation and understanding</li><li>Code generation and comprehension</li><li>Mathematical and scientific tasks</li><li>Multilingual applications</li><li>Fine-tuning for specific domains or tasks</li><li>Efficient deployment in resource-constrained environments</li></ul><h3 id=\"gemma-2\">Gemma 2</h3><p><strong>Best for</strong>: responsible AI development and deployment</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXcfdVE34X6asO7sAQzUzSdtM0FBDxw8PQaGcKEF7ruzsDNT8I2xWG7ZL67TyaJwsgK5UBquXJdLGufM6jNawGcxEqzYgGiRLTBHsgZ0npzmwEbSwyEUxbhAsrh1KiUkmDLq_75Meg?key=INQ3nGlG9V9oPWqw4SkZT1dt\" class=\"kg-image lightense-target\" alt=\"Gemma 2 put emphasis on responsible AI development and deployment\" loading=\"lazy\" width=\"624\" height=\"277\"><figcaption><span style=\"white-space: pre-wrap;\">Gemma 2 put emphasis on responsible AI development and deployment</span></figcaption></figure><p><a href=\"https://ai.google.dev/gemma?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>Gemma 2</u></a> is Google‚Äôs latest family of open-source LLMs, built on the same research and technology used to create the Gemini models. Offering strong performance for its size, Gemma 2 is designed with a focus on responsible AI development and efficient deployment.</p><div class=\"kg-card kg-callout-card kg-callout-card-red\"><div class=\"kg-callout-emoji\">‚öôÔ∏è</div><div class=\"kg-callout-text\"><b><strong style=\"white-space: pre-wrap;\">Gemma 2 key features</strong></b></div></div><ul><li>Multiple model sizes: 2B, 9B, and 27B parameters</li><li>Exceptional performance, with the 27B model outperforming some larger proprietary models</li><li>Optimized for efficient inference across various hardware, from edge devices to cloud deployments</li><li>Built-in safety advancements and responsible AI practices</li><li>Broad framework compatibility (Keras, JAX, PyTorch, Hugging Face, etc.)</li><li>Complementary tools: ShieldGemma for content safety and Gemma Scope for model interpretability</li></ul><div class=\"kg-card kg-callout-card kg-callout-card-accent\"><div class=\"kg-callout-emoji\">ü¶æ</div><div class=\"kg-callout-text\"><b><strong style=\"white-space: pre-wrap;\">Gemma 2 use cases</strong></b></div></div><ul><li>General-purpose text generation and understanding</li><li>Question answering and summarization</li><li>Code generation and understanding</li><li>Fine-tuning for specific domains or tasks</li><li>Responsible AI research and development</li><li>On-device AI applications (especially with the 2B model)</li></ul><h3 id=\"phi-3x-4\">Phi 3.x / 4</h3><p><strong>Best for</strong>: cost-effective AI solutions</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXcx2s-pwPKAE0xEMkPK73iN1ovTRle20iGpOwPq-UbgEDNR1EcW17aAGP1aX3hZzh2zIRsAx5h5KXIuUNj4QxUbhme3yqXJWlOVBqBp3_3Hh8rbueqW7gs02-Y9WD85IyQtbmwq?key=INQ3nGlG9V9oPWqw4SkZT1dt\" class=\"kg-image lightense-target\" alt=\"Phi 3.x / 4 models are best for cost-effective AI solutions\" loading=\"lazy\" width=\"624\" height=\"312\"><figcaption><span style=\"white-space: pre-wrap;\">Phi 3.x / 4 models are best for cost-effective AI solutions</span></figcaption></figure><p><a href=\"https://azure.microsoft.com/en-us/products/phi/?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>Phi-3.x / 4</u></a> is Microsoft‚Äôs family of open-source Small Language Models (SLMs), designed to be highly capable and cost-effective. Phi-3.5 updates bring enhanced multi-lingual support, improved multi-frame image understanding, and a new MoE architecture. Phi-4, the latest model, emphasizes data quality over size. It was trained on synthetic data, filtered public content, and academic resources. The model achieves impressive performance over a range of benchmarks with just 16B parameters.</p><div class=\"kg-card kg-callout-card kg-callout-card-red\"><div class=\"kg-callout-emoji\">‚öôÔ∏è</div><div class=\"kg-callout-text\"><b><strong style=\"white-space: pre-wrap;\">Phi LLM key features</strong></b></div></div><ul><li>Multiple model sizes: 3.8B (mini), 7B (small), 14B (medium), and 42B (MoE) parameters for Phi-3.x; 16B for Phi-4</li><li>Long context window support up to 128K tokens for Phi-3.x, 16K for Phi-4</li><li>Multilingual capabilities in over 20 languages</li><li>Multi-modal support with Phi-3.5-vision for image understanding</li><li>Mixture-of-Experts (MoE) architecture for improved efficiency</li><li>Optimized for <a href=\"https://onnxruntime.ai/docs/?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>ONNX Runtime</u></a> and various hardware targets</li><li>Developed with Microsoft Responsible AI Standard</li></ul><div class=\"kg-card kg-callout-card kg-callout-card-accent\"><div class=\"kg-callout-emoji\">ü¶æ</div><div class=\"kg-callout-text\"><b><strong style=\"white-space: pre-wrap;\">Phi LLM use cases</strong></b></div></div><ul><li>General-purpose text generation and understanding</li><li>Multilingual applications across various languages</li><li>Code understanding and generation</li><li>Mathematical reasoning and problem-solving</li><li>On-device and offline inference scenarios</li><li>Latency-sensitive applications</li><li>Cost-effective AI solutions for resource-constrained environments</li></ul><h3 id=\"command-r\">Command R</h3><p><strong>Best for</strong>: enterprise-level conversational AI and RAG</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdHfb4eblhnTrB7TXUtycf_HHf-DA_6Oo-5O5VT4p89NTck4W42n1nJXMmfOJfCQR_vAmV3fjfm_Jefvc0hbAuD3jREwZbYZD759Y9uTdbH3Re43rLwPgZsuRwe5tJuDd-41uMrvw?key=INQ3nGlG9V9oPWqw4SkZT1dt\" class=\"kg-image lightense-target\" alt=\"Command R model allows for building enterprise-level conversational AI and RAG\" loading=\"lazy\" width=\"624\" height=\"312\"><figcaption><span style=\"white-space: pre-wrap;\">Command R model allows for building enterprise-level conversational AI and RAG</span></figcaption></figure><p><a href=\"https://cohere.com/command?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>Command R</u></a> is Cohere‚Äôs flagship family of LLMs for enterprise-level applications with a focus on conversational interaction and long-context tasks. The family includes Command R, Command R+, and the compact Command R7B, each optimized for different use cases.</p><div class=\"kg-card kg-callout-card kg-callout-card-red\"><div class=\"kg-callout-emoji\">‚öôÔ∏è</div><div class=\"kg-callout-text\"><b><strong style=\"white-space: pre-wrap;\">Command R key features</strong></b></div></div><ul><li>Long context window of 128k tokens</li><li>Multilingual capabilities in 10 primary languages and 13 additional languages</li><li>Tool use and multi-step reasoning for complex tasks</li><li>Customizable safety modes for responsible AI deployment</li><li>Command R7B offers on-device inference capabilities</li></ul><div class=\"kg-card kg-callout-card kg-callout-card-accent\"><div class=\"kg-callout-emoji\">ü¶æ</div><div class=\"kg-callout-text\"><b><strong style=\"white-space: pre-wrap;\">Command R use cases</strong></b></div></div><ul><li>High-performance conversational AI and chatbots</li><li>Complex RAG workflows for information retrieval and synthesis</li><li>Multi-step tool use for dynamic, reasoning-based tasks</li><li>Cross-lingual applications and translations</li><li>Code generation and understanding</li><li>Financial and numerical data analysis</li><li>On-device applications (with Command R7B)</li></ul><h3 id=\"stablelm\">StableLM</h3><p><strong>Best for</strong>: rapid prototyping and experimentation</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXd9IioEnSbEwrhM6AtqdwvzKb80PlUqYUZV8Z0k3GpZk_Wv96Ekj1A3X-JKI9lVjXjhZsiG-cDDbzPM-nr61PI-nGe-Cwi80-Eli5vqPTmNHxJmzxx5A-maiHLu6tjIEPMb7YON?key=INQ3nGlG9V9oPWqw4SkZT1dt\" class=\"kg-image lightense-target\" alt=\"StableLM is great for rapid prototyping and experimentation\" loading=\"lazy\" width=\"624\" height=\"296\"><figcaption><span style=\"white-space: pre-wrap;\">StableLM is great for rapid prototyping and experimentation</span></figcaption></figure><p><a href=\"https://stability.ai/stable-lm?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>StableLM</u></a> is Stability AI‚Äôs series of open-source LLMs, offering competitive performance in compact sizes. The family includes various model sizes and specializations. The 1.6B model, trained on approximately 2 trillion tokens, outperforms many models under 2B parameters on various benchmarks. Stability AI provides both base and instruction-tuned versions, along with pre-training checkpoints to facilitate further fine-tuning.</p><div class=\"kg-card kg-callout-card kg-callout-card-red\"><div class=\"kg-callout-emoji\">‚öôÔ∏è</div><div class=\"kg-callout-text\"><b><strong style=\"white-space: pre-wrap;\">StableLM key features</strong></b></div></div><ul><li>Multiple model sizes: 1.6B, 3B, and 12B parameters</li><li>Multilingual capabilities in English, Spanish, German, Italian, French, Portuguese, and Dutch</li><li>Fill in Middle (FIM) capability for flexible code generation</li><li>Long context support with sequences up to 16k tokens</li><li>Optimized for speed and performance, enabling fast experimentation</li><li>Specialized versions for code generation, Japanese and Arabic languages</li></ul><div class=\"kg-card kg-callout-card kg-callout-card-accent\"><div class=\"kg-callout-emoji\">ü¶æ</div><div class=\"kg-callout-text\"><b><strong style=\"white-space: pre-wrap;\">StableLM use cases</strong></b></div></div><ul><li>General-purpose text generation and understanding in multiple languages</li><li>Code generation and understanding across various programming languages</li><li>Fine-tuning for specific domains or tasks</li><li>Research and commercial applications</li></ul><h3 id=\"starcoder\">Starcoder</h3><p><strong>Best for</strong>: code-related tasks and multi-language programming</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdgR2Po2mCK4FYwMKCPZyiZI3VT7ODdB_0K3elfwk-pIFN02dsbM__J93SLwB3MJ_kgd0Z9BssRo4DC0YiLOEhPEG2G2YKr6Xt17app8sT8BuWBFiYYc-JVfgnBx5_muREcQbUF?key=INQ3nGlG9V9oPWqw4SkZT1dt\" class=\"kg-image lightense-target\" alt=\"Starcoder is best-suited for code-related tasks and multi-language programming\" loading=\"lazy\" width=\"624\" height=\"312\"><figcaption><span style=\"white-space: pre-wrap;\">Starcoder is best-suited for code-related tasks and multi-language programming</span></figcaption></figure><p><a href=\"https://github.com/bigcode-project/starcoder2?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>StarCoder2</u></a> is the next generation of transparently trained open-source language models for code, developed by the BigCode project. It offers high performance for code-related tasks across a wide range of programming languages. The 15B model, in particular, matches the performance of much larger 33B+ models on many evaluations, while the 3B model matches the performance of the previous 15B StarCoder model, showcasing significant improvements in efficiency and capability.</p><div class=\"kg-card kg-callout-card kg-callout-card-red\"><div class=\"kg-callout-emoji\">‚öôÔ∏è</div><div class=\"kg-callout-text\"><b><strong style=\"white-space: pre-wrap;\">StarCoder2 key features</strong></b></div></div><ul><li>Multiple model sizes: 3B, 7B, and 15B parameters</li><li>Trained on 600+ programming languages (15B model)</li><li>Large context window of 16,384 tokens with sliding window attention of 4,096 tokens</li><li>Grouped Query Attention (GQA) for improved efficiency</li><li>Fill-in-the-Middle training objective</li><li>Trained on 3+ trillion tokens (3B and 7B models) to 4+ trillion tokens (15B model)</li></ul><div class=\"kg-card kg-callout-card kg-callout-card-accent\"><div class=\"kg-callout-emoji\">ü¶æ</div><div class=\"kg-callout-text\"><b><strong style=\"white-space: pre-wrap;\">StarCoder2 use cases</strong></b></div></div><ul><li>Code completion and generation across multiple programming languages</li><li>Code understanding and analysis</li><li>Fine-tuning for specific programming tasks or languages</li><li>Assisting developers in various coding scenarios</li><li>Research in code language models and AI for programming</li></ul><h3 id=\"yi\">Yi</h3><p><strong>Best for</strong>: bilingual applications (English and Chinese)</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXd90ameeVcaYVB8AZsRbu-B0GCZdxQG8_wKMxXhQ7gcM8S4k3Nvvq6HiOz39GgxUsLt8O_ucBXUxRuefoucxUhMYGyTYtopU_vRq99uoBTLJBiGanNHaiRXZHZhRBA1WIaCnUjUoA?key=INQ3nGlG9V9oPWqw4SkZT1dt\" class=\"kg-image lightense-target\" alt=\"Yi LLM is great for bilingual applications (English and Chinese)\" loading=\"lazy\" width=\"624\" height=\"312\"><figcaption><span style=\"white-space: pre-wrap;\">Yi LLM is great for bilingual applications (English and Chinese)</span></figcaption></figure><p><a href=\"https://huggingface.co/01-ai?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>Yi is a series of open-source LLMs</u></a> developed by 01.AI, offering strong performance in both English and Chinese across a wide range of tasks. The Yi-1.5 series, an upgraded version of the original Yi models, delivers enhanced capabilities in coding, math, reasoning, and instruction-following.</p><div class=\"kg-card kg-callout-card kg-callout-card-red\"><div class=\"kg-callout-emoji\">‚öôÔ∏è</div><div class=\"kg-callout-text\"><b><strong style=\"white-space: pre-wrap;\">Yi key features</strong></b></div></div><ul><li>Multiple model sizes: 6B, 9B, and 34B parameters</li><li>Bilingual support for English and Chinese</li><li>Extended context windows up to 200k tokens for larger models</li><li>Continuous pre-training on high-quality corpus (500B tokens for Yi-1.5)</li><li>Fine-tuned on 3M diverse samples for improved instruction-following</li><li>Optimized for efficient deployment and fine-tuning</li></ul><div class=\"kg-card kg-callout-card kg-callout-card-accent\"><div class=\"kg-callout-emoji\">ü¶æ</div><div class=\"kg-callout-text\"><b><strong style=\"white-space: pre-wrap;\">Yi use cases</strong></b></div></div><ul><li>Bilingual text generation and understanding</li><li>Code generation and comprehension</li><li>Mathematical problem-solving and reasoning tasks</li><li>Fine-tuning for domain-specific applications</li><li>Natural language processing in academic and commercial settings</li><li>Building chatbots and AI assistants</li></ul><h3 id=\"qwen25\">Qwen2.5</h3><p><strong>Best for</strong>: multilingual and specialized tasks (coding and math)</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdAM8Foi5i6zjGDAz9xWqkrM5FfulQIrrFh3DIP1mWE_XvomYoQSqom-UElQ76ofl-g_YAmUMt3KzMCPlTdgWuBlBNAPU4yA7D0qZOlcHvGHQDsa4l7wYSdG7bKI6LSSOk-hekuXA?key=INQ3nGlG9V9oPWqw4SkZT1dt\" class=\"kg-image lightense-target\" alt=\"Qwen2.5 works great for multilingual and specialized tasks (coding and math)\" loading=\"lazy\" width=\"624\" height=\"243\"><figcaption><span style=\"white-space: pre-wrap;\">Qwen2.5 works great for multilingual and specialized tasks (coding and math)</span></figcaption></figure><p><a href=\"https://qwenlm.github.io/blog/qwen2.5-coder-family/?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>Qwen2.5</u></a> is Alibaba‚Äôs latest series of open-source LLMs with a wide range of sizes and specialized variants for coding and mathematics. These models represent a significant advancement in multilingual capabilities and task-specific performance.</p><div class=\"kg-card kg-callout-card kg-callout-card-red\"><div class=\"kg-callout-emoji\">‚öôÔ∏è</div><div class=\"kg-callout-text\"><b><strong style=\"white-space: pre-wrap;\">Qwen2.5 key features</strong></b></div></div><ul><li>Multiple model sizes: 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B parameters</li><li>Pretrained on up to 18 trillion tokens</li><li>128K token context window with generation up to 8K tokens</li><li>Multilingual support for over 29 languages</li><li>Specialized models: Qwen2.5-Coder and Qwen2.5-Math</li><li>Improved instruction following and structured data understanding</li><li>Enhanced JSON output generation</li></ul><div class=\"kg-card kg-callout-card kg-callout-card-accent\"><div class=\"kg-callout-emoji\">ü¶æ</div><div class=\"kg-callout-text\"><b><strong style=\"white-space: pre-wrap;\">Qwen2.5 use cases</strong></b></div></div><ul><li>General-purpose text generation and understanding</li><li>Multilingual applications across various languages</li><li>Code generation and understanding with Qwen2.5-Coder</li><li>Mathematical reasoning and problem-solving with Qwen2.5-Math</li><li>Long-form content creation and analysis</li><li>Structured data processing and JSON output generation</li><li>Chatbot development with improved role-play capabilities</li></ul><h3 id=\"deepseek-2x-3\">Deepseek 2.x / 3</h3><p><strong>Best for</strong>: efficient large-scale language processing</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXeMOB661VVWKZBwVvPhX8liuKnDy1tAjBZOiBXydNRDzGEs-pBAZDt3EVMDjnk-Y2J7RuwmDDiz6w2nyUUo8WB6vHif36vnyOOjNdBaqqjiRZTTnx5Up59wgMGuvVuN6VwPozP0Tg?key=INQ3nGlG9V9oPWqw4SkZT1dt\" class=\"kg-image lightense-target\" alt=\"Deepseek 2.x / 3 is a top LLM for efficient large-scale language processing\" loading=\"lazy\" width=\"624\" height=\"208\"><figcaption><span style=\"white-space: pre-wrap;\">Deepseek 2.x / 3 is a top LLM for efficient large-scale language processing</span></figcaption></figure><p><a href=\"https://www.deepseek.com/?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>DeepSeek</u></a> is a series of powerful open-source LLMs developed by DeepSeek AI, featuring innovative architectures for efficient inference and cost-effective training. The DeepSeek-V2 and V2.5 models are available for use with Ollama. While the recently released DeepSeek-V3 offers even more impressive capabilities with its 671B parameters, it is not yet available in Ollama at the moment of writing.</p><div class=\"kg-card kg-callout-card kg-callout-card-red\"><div class=\"kg-callout-emoji\">‚öôÔ∏è</div><div class=\"kg-callout-text\"><b><strong style=\"white-space: pre-wrap;\">DeepSeek key features</strong></b></div></div><ul><li>Mixture-of-Experts (MoE) architecture for efficient parameter usage</li><li>Multi-head Latent Attention (MLA) for improved inference efficiency</li><li>Large context windows of up to 128k tokens</li><li>Multilingual capabilities, with strong performance in English and Chinese</li><li>Optimized for both general text generation and coding tasks</li></ul><div class=\"kg-card kg-callout-card kg-callout-card-accent\"><div class=\"kg-callout-emoji\">ü¶æ</div><div class=\"kg-callout-text\"><b><strong style=\"white-space: pre-wrap;\">DeepSeek use cases</strong></b></div></div><ul><li>General-purpose text generation and understanding</li><li>Multilingual applications and translations</li><li>Code generation and understanding</li><li>Fine-tuning for specific domains or tasks</li><li>Assistant-like interactions in chatbots and AI applications</li><li>Long-form content creation and analysis</li></ul><h2 id=\"getting-started-with-langchain-and-open-source-llms-in-n8n\">Getting started with LangChain and open-source LLMs in n8n</h2><p>If running an open-source LLM seems too complicated, we‚Äôve got great news: in n8n you can jump-start with Ollama. This powerful integration allows you to connect local models to real-world workflows and automate tasks in a meaningful way.</p><p>By combining the flexibility of open-source LLMs with the automation capabilities of n8n, you can build custom AI applications that are both powerful and efficient. LangChain (JavaScript version) is the main framework for building AI agents and LLM-powered workflows in n8n. The possibilities for customization and innovation are virtually limitless ‚Äì use hundreds of pre-built nodes or write custom JS scripts.</p><p>Let‚Äôs explore how n8n makes creating custom LLM-powered apps and workflows easy!</p><p>There are at least 3 easy ways to build projects with open-source LLMs with n8n LangChain nodes:</p><ol><li>Run small Hugging Face models with a <a href=\"https://huggingface.co/docs/hub/security-tokens?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>User Access Token</u></a> completely for free.</li><li>If you want to run larger models or need a quick response, try the Hugging Face service called <a href=\"https://huggingface.co/inference-endpoints?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>Custom Inference Endpoints</u></a>.</li><li>If you have enough computing resources, run the model via <a href=\"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmollama?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>Ollama</u></a> locally or self-hosted.</li></ol><p><a href=\"https://docs.n8n.io/hosting/starter-kits/ai-starter-kit/?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>LangChain nodes in n8n + Ollama integration</u></a> make it easier to access open-source LLMs and give you handy tools for working with them. Here‚Äôs a video with an overview of the most important aspects:</p><figure class=\"kg-card kg-embed-card\"><div class=\"js-reframe\" style=\"position: relative; width: 100%; padding-top: 56.5%;\"><iframe width=\"200\" height=\"113\" src=\"https://www.youtube.com/embed/xz_X2N-hPg0?feature=oembed\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen=\"\" title=\"Installing and Using Local AI for n8n\" style=\"position: absolute; width: 100%; height: 100%; left: 0px; top: 0px;\"></iframe></div></figure><p>After you‚Äôve installed the self-hosted AI Starter Kit, it‚Äôs time for a practical part!</p><p>Here is a workflow template that is particularly useful for enterprise environments where data privacy is crucial. It allows for on-premises processing of personal information.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://blog.n8n.io/content/images/2025/01/WK-Data-ext2-1.png\" class=\"kg-image lightense-target\" alt=\"This workflow takes an input and extracts user information in a consistent JSON format\" loading=\"lazy\" width=\"966\" height=\"726\" srcset=\"https://blog.n8n.io/content/images/size/w600/2025/01/WK-Data-ext2-1.png 600w, https://blog.n8n.io/content/images/2025/01/WK-Data-ext2-1.png 966w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">This workflow takes an input and extracts user information in a consistent JSON format</span></figcaption></figure>\n<!--kg-card-begin: html-->\n<script>\n  workflowBanner(2766, document.currentScript);\n</script><div class=\"workflow\">\n      <div class=\"workflow-content\">\n        <div class=\"workflow-nodes\">\n          \n          <i class=\"fa fa-comments\" aria-hidden=\"true\"></i>\n        <img src=\"data:image/svg+xml;base64,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\" alt=\"Ollama Chat Model\" title=\"Ollama Chat Model\">\n          <i class=\"fa fa-tools\" aria-hidden=\"true\"></i>\n        \n          <i class=\"fa fa-code\" aria-hidden=\"true\"></i>\n        \n          <i class=\"fa fa-link\" aria-hidden=\"true\" style=\"color:#909298\"></i>\n        \n          <span>+3</span>\n        </div>\n        <div class=\"workflow-details\">\n          <p class=\"workflow-details-name\">\n            <a href=\"https://n8n.io/workflows/2766-extract-personal-data-with-self-hosted-llm-mistral-nemo/\" class=\"blog-banner-workflow\">Extract personal data with self-hosted LLM Mistral NeMo</a>\n          </p>\n          <p class=\"workflow-details-stats\">by yulia</p>\n        </div>\n      </div>\n      <a href=\"https://n8n.io/workflows/2766-extract-personal-data-with-self-hosted-llm-mistral-nemo/\" class=\"global-button blog-banner-workflow\">\n        Use this workflow\n      </a>\n    </div>\n<!--kg-card-end: html-->\n<h3 id=\"step-1-configure-the-basic-llm-chain-node\">Step 1: Configure the Basic LLM Chain node</h3><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXdhMUK2iNl9WeDN06C2gN4ok2boEvaAAGnWx2KMEIpoa2Co6HhsaKWIfpHMHGh-BjjR0FR1Us38mdUSx9sW2ALlbq0YAVpxWxtC8GZJ3EI50ttS0pbVpTiVje5zwxhZzcFwr8Xi?key=INQ3nGlG9V9oPWqw4SkZT1dt\" class=\"kg-image lightense-target\" alt=\"Provide a system prompt and make sure a user message is coming for the correct input\" loading=\"lazy\" width=\"624\" height=\"427\"><figcaption><span style=\"white-space: pre-wrap;\">Provide a system prompt and make sure a user message is coming for the correct input</span></figcaption></figure><p>The core of the workflow is the <a href=\"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.chainllm/?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>Basic LLM Chain node</u></a>. Configure it as follows:</p><ul>\n<li>Activate the Require Specific Output Format toggle;</li>\n<li>In the Messages section, add a system message with the following content:<code>Please analyse the incoming user request. Extract information according to the JSON schema. Today is: {{ $now.toISO() }}</code>This is the main prompt with the general task.</li>\n</ul>\n<h3 id=\"step-2-add-the-chat-trigger-node\">Step 2: Add the Chat Trigger node</h3><p>For this example, we‚Äôre using a <a href=\"https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-langchain.chattrigger/?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>Chat Trigger</u></a> to simulate user input.</p><p>üí° In a real-world scenario, this could be replaced with various data sources such as database queries, voice transcripts, or incoming Webhook data.</p><h3 id=\"step-3-configure-the-ollama-chat-model-node\">Step 3: Configure the Ollama Chat Model node</h3><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXe_9JkZcwIFJmGTUNh6PWO1GrLrqdAVhV6FQ44st0A4lgKa4Rd7fGiIi70ZFOJN4zEG4OtOCVQrgzmeBSYDOkckKYLZt2voymt_FpVlMDBsWi9PJ_yZgydacH1iJez7Z03MkHRrYg?key=INQ3nGlG9V9oPWqw4SkZT1dt\" class=\"kg-image lightense-target\" alt=\"Ollama provides several additional settings that are specific to self-hosted LLMs\" loading=\"lazy\" width=\"624\" height=\"388\"><figcaption><span style=\"white-space: pre-wrap;\">Ollama provides several additional settings that are specific to self-hosted LLMs</span></figcaption></figure><p>Connect the <a href=\"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatollama/?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>Ollama Chat Model</u></a> node to provide the language model capabilities:</p><ul>\n<li>Set the model to <code>mistral-nemo:latest</code></li>\n<li>Set temperature to <code>0.1</code> for more consistent outputs</li>\n<li>Set keep Alive setting to <code>2h</code> to maintain the model in memory</li>\n<li>Enable the Use Memory Locking toggle for improved performance</li>\n</ul>\n<h3 id=\"step-4-ensure-consistent-structured-output\">Step 4: Ensure consistent structured output</h3><ol><li>Add an <a href=\"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.outputparserautofixing/?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>Auto-fixing Output Parser node</u></a> and connect it to the same Ollama Chat Model.</li></ol><p>Add a <a href=\"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.outputparserstructured/?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>Structured Output Parser node</u></a> with the following JSON schema:</p><pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"name\": {\n      \"type\": \"string\",\n      \"description\": \"Name of the user\"\n    },\n    \"surname\": {\n      \"type\": \"string\",\n      \"description\": \"Surname of the user\"\n    },\n    \"commtype\": {\n      \"type\": \"string\",\n      \"enum\": [\"email\", \"phone\", \"other\"],\n      \"description\": \"Method of communication\"\n    },\n    \"contacts\": {\n      \"type\": \"string\",\n      \"description\": \"Contact details. ONLY IF PROVIDED\"\n    },\n    \"timestamp\": {\n      \"type\": \"string\",\n      \"format\": \"date-time\",\n      \"description\": \"When the communication occurred\"\n    },\n    \"subject\": {\n      \"type\": \"string\",\n      \"description\": \"Brief description of the communication topic\"\n    }\n  },\n  \"required\": [\"name\", \"communicationType\"]\n}\n\n</code></pre><p>This JSON schema defines several JSON keys to collect various data like name, surname, communication method, user contacts, topic and the timestamp. However, only the name and the communication method are mandatory parameters. You can adjust the schema according to your needs.</p><h3 id=\"step-5-process-the-output\">Step 5: Process the output</h3><figure class=\"kg-card kg-image-card kg-width-wide kg-card-hascaption\"><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXfyOgsB5H6zjCeBegEW54Q00X8atspKeS7AEDxNtxIVTqLOFzKHGqChQ3O6qROpyTEswefxFXBAz-mGhmr8tpx-IGXIz5OGiWJrHdnTPvVSByBp6oEtIk-he8aFN3vjMEdUGxTPRQ?key=INQ3nGlG9V9oPWqw4SkZT1dt\" class=\"kg-image lightense-target\" alt=\"As an optional step, transform the Basic LLM Chain output\" loading=\"lazy\" width=\"624\" height=\"171\"><figcaption><span style=\"white-space: pre-wrap;\">As an optional step, transform the Basic LLM Chain output</span></figcaption></figure><p>After the Basic LLM Chain node processes the request, it will produce a JSON with an output key. Transform this output using a Set node:</p><p>Set the Mode to <code>JSON</code><br>\nUse the following expression: <code>{{ $json.output }}</code></p>\n<p>Adding a Set node is optional, which we did just for convenience.</p><h3 id=\"step-6-handle-errors\">Step 6: Handle errors</h3><p>Add a <a href=\"https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.noop/?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>No Operation node</u></a> after the Error output from the Basic LLM Chain node. This serves as an intermediary step before further error processing.</p><p>That‚Äôs it! Now you‚Äôre done and can test the workflow. Press the Chat button in the bottom middle part of your instance and provide a text message. For example:</p><p><code>Hi, my name is John. I'd like to be contacted via E-mail at john.smith@example.com regarding my recent order #12345.</code></p>\n<p>You can easily adapt this template to various enterprise use cases by modifying the input source, output schema or post-processing steps.</p><p>If you have a specific storage system where you‚Äôd like to save the result, consider switching the Basic LLM Chain node to a <a href=\"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.agent/tools-agent/?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>Tools Agent node</u></a>. Modern LLMs have <a href=\"https://docs.mistral.ai/capabilities/function_calling/?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>built-in capabilities for function calling</u></a>, so you can define the desired output format which can immediately connect to a database and upload the parsed information.</p><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">Additionally, special <a href=\"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.code?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>LangChain Code</u></a> and <a href=\"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.toolcode?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u>Code Tool</u></a> nodes allow you to create completely custom chains. You can build whatever is supported by the LangChainJS library, even if a ready-made node is not yet available.</div></div><h2 id=\"faqs\">FAQs&nbsp;</h2><div class=\"kg-card kg-toggle-card\" data-kg-toggle-state=\"close\">\n            <div class=\"kg-toggle-heading\">\n                <h4 class=\"kg-toggle-heading-text\"><span style=\"white-space: pre-wrap;\">Which types of open-source LLMs are there?</span></h4>\n                <button class=\"kg-toggle-card-icon\" aria-label=\"Expand toggle to read content\">\n                    <svg id=\"Regular\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                        <path class=\"cls-1\" d=\"M23.25,7.311,12.53,18.03a.749.749,0,0,1-1.06,0L.75,7.311\"></path>\n                    </svg>\n                </button>\n            </div>\n            <div class=\"kg-toggle-content\"><p><span style=\"white-space: pre-wrap;\">Open-source models fall into two main categories:</span></p><ul><li value=\"1\"><b><strong style=\"white-space: pre-wrap;\">Pre-trained LLMs </strong></b><span style=\"white-space: pre-wrap;\">are created using vast amounts of text data. These models excel at understanding broad contexts and generating coherent text. While valuable for research and general language tasks, they may struggle with specific instructions or specialized applications.</span></li><li value=\"2\"><b><strong style=\"white-space: pre-wrap;\">Fine-tuned LLMs </strong></b><span style=\"white-space: pre-wrap;\">are adapted from pre-trained models. They undergo additional training on targeted datasets, making them more effective for particular use cases like classification, summarization, or question-answering. Fine-tuned models are essential for modern applications such as turn-based chat messaging and function calling.</span></li></ul><p><span style=\"white-space: pre-wrap;\">Note that some authors distinguish fine-tuning from</span><a href=\"https://medium.com/@eordaxd/fine-tuning-vs-pre-training-651d05186faf?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"> <u><span class=\"underline\" style=\"white-space: pre-wrap;\">continuous pre-training</span></u></a><span style=\"white-space: pre-wrap;\">. The latter involves further pre-training a model with domain-specific data, such as medical or financial reports, to adapt it to a particular field.</span></p></div>\n        </div><div class=\"kg-card kg-toggle-card\" data-kg-toggle-state=\"close\">\n            <div class=\"kg-toggle-heading\">\n                <h4 class=\"kg-toggle-heading-text\"><span style=\"white-space: pre-wrap;\">How to get started with an open-source LLM?</span></h4>\n                <button class=\"kg-toggle-card-icon\" aria-label=\"Expand toggle to read content\">\n                    <svg id=\"Regular\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                        <path class=\"cls-1\" d=\"M23.25,7.311,12.53,18.03a.749.749,0,0,1-1.06,0L.75,7.311\"></path>\n                    </svg>\n                </button>\n            </div>\n            <div class=\"kg-toggle-content\"><p><span style=\"white-space: pre-wrap;\">There are two main approaches to setting up and using open-source LLMs:</span></p><ol><li value=\"1\"><b><strong style=\"white-space: pre-wrap;\">Install locally</strong></b><span style=\"white-space: pre-wrap;\">. Helper tools such as Ollama simplify the process. However, the larger the model, the more difficult it is to meet the hardware requirements. The largest models require industrial-level equipment.</span></li><li value=\"2\"><span style=\"white-space: pre-wrap;\">Instead of hosting everything locally, it‚Äôs also possible to </span><b><strong style=\"white-space: pre-wrap;\">rent a virtual server. </strong></b></li></ol><p><b><strong style=\"white-space: pre-wrap;\">VPS with a GPU</strong></b><span style=\"white-space: pre-wrap;\"> allows for faster inference, but is more expensive. Several hosting providers have automated the process of model installation and deployment, so the entire setup requires just a few clicks and some waiting time.</span></p><p><b><strong style=\"white-space: pre-wrap;\">Traditional CPU-only virtual servers</strong></b><span style=\"white-space: pre-wrap;\"> could be a more cost-efficient alternative, especially when deploying smaller language models without strict requirements on response time.</span></p></div>\n        </div><div class=\"kg-card kg-toggle-card\" data-kg-toggle-state=\"close\">\n            <div class=\"kg-toggle-heading\">\n                <h4 class=\"kg-toggle-heading-text\"><span style=\"white-space: pre-wrap;\">How to run open-source LLM locally?</span></h4>\n                <button class=\"kg-toggle-card-icon\" aria-label=\"Expand toggle to read content\">\n                    <svg id=\"Regular\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                        <path class=\"cls-1\" d=\"M23.25,7.311,12.53,18.03a.749.749,0,0,1-1.06,0L.75,7.311\"></path>\n                    </svg>\n                </button>\n            </div>\n            <div class=\"kg-toggle-content\"><p><span style=\"white-space: pre-wrap;\">There are several ways to run LLMs locally. The easiest approach is to use one of the available frameworks, which can get you up and running in just a few clicks:</span></p><ul><li value=\"1\"><b><strong style=\"white-space: pre-wrap;\">Ollama + OpenWebUI</strong></b><span style=\"white-space: pre-wrap;\">: Ollama as a backend for quick LLM deployment, OpenWebUI as a user-friendly frontend</span></li><li value=\"2\"><b><strong style=\"white-space: pre-wrap;\">GPT4All</strong></b><span style=\"white-space: pre-wrap;\">: General-purpose AI applications and document chat</span></li><li value=\"3\"><b><strong style=\"white-space: pre-wrap;\">LM Studio</strong></b><span style=\"white-space: pre-wrap;\">: LLM customization and fine-tuning</span></li><li value=\"4\"><b><strong style=\"white-space: pre-wrap;\">Jan</strong></b><span style=\"white-space: pre-wrap;\">: Privacy-focused LLM interactions with flexible server options</span></li><li value=\"5\"><b><strong style=\"white-space: pre-wrap;\">NextChat</strong></b><span style=\"white-space: pre-wrap;\">: Building conversational AI with support for various LLMs</span></li></ul></div>\n        </div><div class=\"kg-card kg-toggle-card\" data-kg-toggle-state=\"close\">\n            <div class=\"kg-toggle-heading\">\n                <h4 class=\"kg-toggle-heading-text\"><span style=\"white-space: pre-wrap;\">How much RAM do I need to run an LLM?</span></h4>\n                <button class=\"kg-toggle-card-icon\" aria-label=\"Expand toggle to read content\">\n                    <svg id=\"Regular\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                        <path class=\"cls-1\" d=\"M23.25,7.311,12.53,18.03a.749.749,0,0,1-1.06,0L.75,7.311\"></path>\n                    </svg>\n                </button>\n            </div>\n            <div class=\"kg-toggle-content\"><p><span style=\"white-space: pre-wrap;\">To work, most LLMs have to be loaded into memory (RAM or GPU VRAM). How much memory you need depends on multiple factors (model size, quantization, etc.) as well as specific use-cases (for example, simple inference vs fine-tuning).</span></p><p><span style=\"white-space: pre-wrap;\">Thanks to recent advances, some efficient small language models (SLMs) can run simple tasks on systems with just 4 GB of free RAM. During fine-tuning, however, the requirements increase, because you need to store intermediate steps while model parameter values are updated.</span></p><p><span style=\"white-space: pre-wrap;\">To check specific hardware requirements for an open-source LLM, look up its model card on Hugging Face, GitHub, or the developer's website. For quick estimates, you can use the</span><a href=\"https://huggingface.co/spaces/Vokturz/can-it-run-llm?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"> <u><span class=\"underline\" style=\"white-space: pre-wrap;\">\"Can you run it?\" tool for LLMs</span></u></a><span style=\"white-space: pre-wrap;\">.</span></p></div>\n        </div><div class=\"kg-card kg-toggle-card\" data-kg-toggle-state=\"close\">\n            <div class=\"kg-toggle-heading\">\n                <h4 class=\"kg-toggle-heading-text\"><span style=\"white-space: pre-wrap;\">How much does it cost to run an open-source LLM?</span></h4>\n                <button class=\"kg-toggle-card-icon\" aria-label=\"Expand toggle to read content\">\n                    <svg id=\"Regular\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                        <path class=\"cls-1\" d=\"M23.25,7.311,12.53,18.03a.749.749,0,0,1-1.06,0L.75,7.311\"></path>\n                    </svg>\n                </button>\n            </div>\n            <div class=\"kg-toggle-content\"><p><span style=\"white-space: pre-wrap;\">While open-source models are free to use, the deployment and infrastructure costs vary. The main cost when running open-source LLMs is hardware. Here‚Äôs a concise breakdown of costs depending on different deployment options:</span></p><ul><li value=\"1\"><span style=\"white-space: pre-wrap;\">Locally: free if your computer meets system requirements</span></li><li value=\"2\"><span style=\"white-space: pre-wrap;\">Managed API providers: free limited options or fees comparable to popular services like OpenAI / Anthropic</span></li><li value=\"3\"><span style=\"white-space: pre-wrap;\">Simple VPS: starting from $20/mo for CPU-only servers; GPU server prices are higher, up to dozens of dollars per hour</span></li><li value=\"4\"><span style=\"white-space: pre-wrap;\">Managed options with one-click install on GPU servers: premium pricing</span></li></ul></div>\n        </div><div class=\"kg-card kg-toggle-card\" data-kg-toggle-state=\"close\">\n            <div class=\"kg-toggle-heading\">\n                <h4 class=\"kg-toggle-heading-text\"><span style=\"white-space: pre-wrap;\">Are open-source LLMs secure?</span></h4>\n                <button class=\"kg-toggle-card-icon\" aria-label=\"Expand toggle to read content\">\n                    <svg id=\"Regular\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                        <path class=\"cls-1\" d=\"M23.25,7.311,12.53,18.03a.749.749,0,0,1-1.06,0L.75,7.311\"></path>\n                    </svg>\n                </button>\n            </div>\n            <div class=\"kg-toggle-content\"><p><span style=\"white-space: pre-wrap;\">Open-source LLMs offer transparency but also present certain security challenges:</span></p><ol><li value=\"1\"><span style=\"white-space: pre-wrap;\">Potential vulnerabilities: the publicly available model weights and architecture can attract both collaborators and potential attackers.</span></li><li value=\"2\"><span style=\"white-space: pre-wrap;\">Adversarial attacks: methods like data poisoning, prompt injection, and model evasion can alter input data to produce incorrect or unintended results.</span></li><li value=\"3\"><span style=\"white-space: pre-wrap;\">Wider attack surface: as open-source LLMs are integrated into more applications and platforms, the potential for attacks increases.</span></li></ol><p><span style=\"white-space: pre-wrap;\">While the open-source community actively works on improving LLM security, users should implement additional safeguards. We recommend gating open-source LLMs during prototyping and rollout, making them accessible only through internal services (e.g. via n8n rather than directly by users).</span></p></div>\n        </div><div class=\"kg-card kg-toggle-card\" data-kg-toggle-state=\"close\">\n            <div class=\"kg-toggle-heading\">\n                <h4 class=\"kg-toggle-heading-text\"><span style=\"white-space: pre-wrap;\">Why to use open-source LLMs commercially?</span></h4>\n                <button class=\"kg-toggle-card-icon\" aria-label=\"Expand toggle to read content\">\n                    <svg id=\"Regular\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                        <path class=\"cls-1\" d=\"M23.25,7.311,12.53,18.03a.749.749,0,0,1-1.06,0L.75,7.311\"></path>\n                    </svg>\n                </button>\n            </div>\n            <div class=\"kg-toggle-content\"><p><span style=\"white-space: pre-wrap;\">We‚Äôve gathered insights from real-world users on </span><a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1cub6sg/who_is_using_opensource_llms_commercially/?ref=blog.n8n.io\" target=\"_blank\" rel=\"noopener\"><u><span class=\"underline\" style=\"white-space: pre-wrap;\">Reddit</span></u></a><span style=\"white-space: pre-wrap;\"> to understand why businesses choose open-source LLMs. Here are the key reasons:</span></p><ol><li value=\"1\"><b><strong style=\"white-space: pre-wrap;\">Efficient for simple tasks</strong></b><span style=\"white-space: pre-wrap;\">: smaller open-source models can handle basic text generation, classification, and function calling effectively.</span></li><li value=\"2\"><b><strong style=\"white-space: pre-wrap;\">Data privacy</strong></b><span style=\"white-space: pre-wrap;\">: ideal for processing sensitive documents without relying on external cloud services.</span></li><li value=\"3\"><b><strong style=\"white-space: pre-wrap;\">Integration with existing infrastructure</strong></b><span style=\"white-space: pre-wrap;\">: easy to incorporate if you‚Äôre already running ML models on your own GPUs.</span></li><li value=\"4\"><b><strong style=\"white-space: pre-wrap;\">Cost-effective for high volumes</strong></b><span style=\"white-space: pre-wrap;\">: fine-tuning smaller open-source models can offer a better price-performance ratio for large-scale operations.</span></li><li value=\"5\"><b><strong style=\"white-space: pre-wrap;\">Customization</strong></b><span style=\"white-space: pre-wrap;\">: allows setting your own guidelines to align with company policies and ethical standards.</span></li><li value=\"6\"><b><strong style=\"white-space: pre-wrap;\">Transparency</strong></b><span style=\"white-space: pre-wrap;\">: offers the ability to review training data and understand the model‚Äôs architecture.</span></li><li value=\"7\"><b><strong style=\"white-space: pre-wrap;\">Control over costs</strong></b><span style=\"white-space: pre-wrap;\">: prototyping with open-source models helps manage expenses before committing to specific providers.ntv </span></li></ol></div>\n        </div><div class=\"kg-card kg-callout-card kg-callout-card-blue\"><div class=\"kg-callout-emoji\">üí°</div><div class=\"kg-callout-text\">For a detailed guide on these frameworks and how to use them, check out a <a href=\"https://blog.n8n.io/local-llm/\"><u>comprehensive guide on running local LLMs</u></a>.</div></div><h2 id=\"wrap-up\">Wrap Up</h2><p>In this article, we've highlighted that the best open-source LLM depends on your specific use case, as models like Llama3, Mistral, and Falcon 3 excel in different areas such as speed, accuracy, or resource efficiency. We emphasized evaluating models based on factors like task requirements, deployment setup, and available resources.</p><p>Additionally, we explained how <a href=\"https://n8n.io/ai/?ref=blog.n8n.io\" rel=\"noopener\" target=\"_blank\">tools like n8n</a> and LangChain simplify integrating these LLMs into workflows, making it easier to experiment and find the right fit.</p>\n<!--kg-card-begin: html-->\n<div class=\"content-banner\">\n  <div>\n    <h3>Create your own LLM workflows</h3>\n    <p>Build complex automations 10x faster, without fighting APIs</p>\n  </div>\n  <a href=\"https://app.n8n.cloud/register?ref=blog.n8n.io\" class=\"global-button blog-banner-signup\" target=\"_blank\" rel=\"noopener\">Try n8n now</a>\n</div>\n<!--kg-card-end: html-->\n<h2 id=\"what%E2%80%99s-next\">What‚Äôs next?</h2><p>Now that you‚Äôve got a grasp on using open-source LLMs with n8n, you can explore more advanced AI-powered automation scenarios. Many of the concepts we‚Äôve covered in our other AI-related articles can be applied to local models as well.</p><p>Here are some resources to continue your journey:</p><ul><li>Learn about <a href=\"https://blog.n8n.io/ai-workflow-automation/\"><u>AI workflow automation trends</u></a>;</li><li>Create intelligent workflows with <a href=\"https://blog.n8n.io/ai-agentic-workflows/\"><u>AI agents in n8n automation</u></a>;</li></ul><p>Build your own <a href=\"https://blog.n8n.io/telegram-bots/\"><u>AI chatbot using n8n and Telegram</u></a>.</p>\n<!--kg-card-begin: html-->\n<script>\n  workflowBanner([2729, 2384, 1980], document.currentScript);\n</script><div class=\"workflows\">\n    \n    <h3>Most popular workflows with these integrations</h3>\n  <div class=\"workflow\">\n      <div class=\"workflow-content\">\n        <div class=\"workflow-nodes\">\n          \n          <i class=\"fa fa-comments\" aria-hidden=\"true\"></i>\n        \n          <i class=\"fa fa-link\" aria-hidden=\"true\" style=\"color:#909298\"></i>\n        <img src=\"data:image/svg+xml;base64,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\" alt=\"Ollama Model\" title=\"Ollama Model\">\n          <i class=\"fa fa-sticky-note\" aria-hidden=\"true\" style=\"color:#FFD233\"></i>\n        \n          <i class=\"fa fa-pen\" aria-hidden=\"true\"></i>\n        \n          \n        </div>\n        <div class=\"workflow-details\">\n          <p class=\"workflow-details-name\">\n            <a href=\"https://n8n.io/workflows/2729-private-and-local-ollama-self-hosted-ai-assistant/\" class=\"blog-banner-workflow\">üîêü¶ôü§ñ Private &amp; Local Ollama Self-Hosted AI Assistant</a>\n          </p>\n          <p class=\"workflow-details-stats\">\n          <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"12\" height=\"10\" fill=\"none\" viewBox=\"0 0 12 10\"><path fill=\"#707183\" d=\"M4.94 6.06a1.5 1.5 0 1 1 2.121-2.12 1.5 1.5 0 0 1-2.12 2.12Z\"></path><path fill=\"#707183\" fill-rule=\"evenodd\" d=\"M.462 6.143C1.38 7.135 3.294 9.2 6 9.2s4.62-2.065 5.539-3.057a1.703 1.703 0 0 0 0-2.332l-.001-.002C10.703 2.907 8.749.8 6 .8 3.256.8 1.304 2.903.466 3.806l-.004.005a1.703 1.703 0 0 0 0 2.332ZM3.77 3.305c.689-.38 1.447-.62 2.23-.705a5.88 5.88 0 0 1 3.972 2.17.301.301 0 0 1 0 .414A5.88 5.88 0 0 1 6 7.4 6.216 6.216 0 0 1 1.996 5.24a.3.3 0 0 1-.01-.423A5.937 5.937 0 0 1 3.77 3.305Z\" clip-rule=\"evenodd\"></path></svg>\n          39898 <span>‚Ä¢</span> <strong>by joe</strong> <span>‚Ä¢</span> 4 months\n        </p>\n        </div>\n      </div>\n      <a href=\"https://n8n.io/workflows/2729-private-and-local-ollama-self-hosted-ai-assistant/\" class=\"global-button blog-banner-workflow\">\n        Use this workflow\n      </a>\n    </div><div class=\"workflow\">\n      <div class=\"workflow-content\">\n        <div class=\"workflow-nodes\">\n          \n          <i class=\"fa fa-comments\" aria-hidden=\"true\"></i>\n        <img src=\"data:image/svg+xml;base64,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\" alt=\"Ollama Chat Model\" title=\"Ollama Chat Model\">\n          <i class=\"fa fa-sticky-note\" aria-hidden=\"true\" style=\"color:#FFD233\"></i>\n        \n          <i class=\"fa fa-link\" aria-hidden=\"true\" style=\"color:#909298\"></i>\n        \n          \n        </div>\n        <div class=\"workflow-details\">\n          <p class=\"workflow-details-name\">\n            <a href=\"https://n8n.io/workflows/2384-chat-with-local-llms-using-n8n-and-ollama/\" class=\"blog-banner-workflow\">Chat with local LLMs using n8n and Ollama</a>\n          </p>\n          <p class=\"workflow-details-stats\">\n          <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"12\" height=\"10\" fill=\"none\" viewBox=\"0 0 12 10\"><path fill=\"#707183\" d=\"M4.94 6.06a1.5 1.5 0 1 1 2.121-2.12 1.5 1.5 0 0 1-2.12 2.12Z\"></path><path fill=\"#707183\" fill-rule=\"evenodd\" d=\"M.462 6.143C1.38 7.135 3.294 9.2 6 9.2s4.62-2.065 5.539-3.057a1.703 1.703 0 0 0 0-2.332l-.001-.002C10.703 2.907 8.749.8 6 .8 3.256.8 1.304 2.903.466 3.806l-.004.005a1.703 1.703 0 0 0 0 2.332ZM3.77 3.305c.689-.38 1.447-.62 2.23-.705a5.88 5.88 0 0 1 3.972 2.17.301.301 0 0 1 0 .414A5.88 5.88 0 0 1 6 7.4 6.216 6.216 0 0 1 1.996 5.24a.3.3 0 0 1-.01-.423A5.937 5.937 0 0 1 3.77 3.305Z\" clip-rule=\"evenodd\"></path></svg>\n          65677 <span>‚Ä¢</span> <strong>by mihailtd</strong> <span>‚Ä¢</span> 9 months\n        </p>\n        </div>\n      </div>\n      <a href=\"https://n8n.io/workflows/2384-chat-with-local-llms-using-n8n-and-ollama/\" class=\"global-button blog-banner-workflow\">\n        Use this workflow\n      </a>\n    </div><div class=\"workflow\">\n      <div class=\"workflow-content\">\n        <div class=\"workflow-nodes\">\n          \n          <i class=\"fa fa-sticky-note\" aria-hidden=\"true\" style=\"color:#FFD233\"></i>\n        \n          <i class=\"fa fa-comments\" aria-hidden=\"true\"></i>\n        \n          <i class=\"fa fa-link\" aria-hidden=\"true\" style=\"color:#909298\"></i>\n        <img src=\"data:image/svg+xml;base64,<svg xmlns="http://www.w3.org/2000/svg" width="256" height="256" fill="none"><path fill="#fff" d="M230.721 172.7a19 19 0 0 0-2.575-5.692q.376-1.377.568-2.791c.829-5.976-1.243-11.447-5.147-15.68-2.115-2.312-4.382-3.839-6.783-4.776a90.6 90.6 0 0 0 2.377-20.568c0-3.163-.179-6.261-.479-9.313a106 106 0 0 0-.567-4.56 91 91 0 0 0-3.051-13.21 91 91 0 0 0-3.054-8.374 92 92 0 0 0-6.041-11.754 81 81 0 0 0-4.907-7.262 69 69 0 0 0-2.704-3.446 90.5 90.5 0 0 0-9.033-9.486 70 70 0 0 0-3.315-2.862 82 82 0 0 0-3.424-2.704 96 96 0 0 0-7.262-4.907c-13.781-8.37-29.942-13.17-47.215-13.17-50.292 0-91.052 40.762-91.052 91.051-.002 7.012.81 14 2.42 20.824-2.16.938-4.23 2.4-6.15 4.515-3.903 4.231-5.976 9.682-5.147 15.658q.189 1.425.567 2.813a19 19 0 0 0-2.573 5.694c-1.2 4.561-.805 8.674.72 12.278-1.658 4.71-1.244 9.726.915 14.087 1.57 3.185 3.817 5.649 6.587 7.851 3.293 2.618 7.415 4.842 12.387 6.976 5.932 2.53 13.173 4.907 16.466 5.779 8.506 2.202 16.662 3.598 24.928 3.666 11.777.109 21.919-2.66 29.18-9.747a88 88 0 0 0 10.752.654 94 94 0 0 0 11.358-.715c7.244 7.132 17.425 9.926 29.245 9.814 8.265-.066 16.421-1.462 24.905-3.667 3.315-.872 10.553-3.249 16.488-5.779 4.972-2.137 9.094-4.361 12.409-6.975 2.749-2.203 4.994-4.666 6.565-7.851 2.181-4.362 2.573-9.378.938-14.088 1.51-3.604 1.903-7.726.704-12.283m-8.44 11.973c1.671 3.171 1.778 6.754.304 10.091-2.236 5.057-7.79 9.041-18.577 13.318-6.708 2.66-12.85 4.361-12.904 4.376-8.872 2.301-16.896 3.47-23.842 3.47-11.502 0-20.061-3.174-25.489-9.442a85.5 85.5 0 0 1-27.747.158c-5.435 6.164-13.945 9.284-25.35 9.284-6.947 0-14.97-1.169-23.843-3.47-.054-.015-6.194-1.716-12.904-4.376-10.786-4.277-16.342-8.258-18.577-13.318-1.474-3.337-1.367-6.92.304-10.091q.231-.443.497-.86a12.8 12.8 0 0 1-1.728-10.341c.664-2.523 2.035-4.621 3.897-6.128a12.75 12.75 0 0 1-1.73-4.822c-.536-3.714.697-7.422 3.47-10.446 2.16-2.353 5.213-3.648 8.593-3.648h.09a84.5 84.5 0 0 1-3.832-25.235c0-46.671 37.836-84.51 84.514-84.51s84.513 37.835 84.513 84.51a84.4 84.4 0 0 1-3.859 25.299q.612-.06 1.201-.061c3.38 0 6.434 1.295 8.592 3.648 2.773 3.021 4.007 6.732 3.47 10.446a12.8 12.8 0 0 1-1.729 4.822c1.862 1.507 3.234 3.605 3.897 6.128a12.8 12.8 0 0 1-1.728 10.341q.267.413.497.857"/><path fill="#FF9D00" d="M221.784 183.816a12.8 12.8 0 0 0 1.728-10.341c-.664-2.523-2.036-4.621-3.897-6.128a12.7 12.7 0 0 0 1.729-4.822c.537-3.714-.696-7.422-3.47-10.446-2.158-2.353-5.212-3.648-8.592-3.648q-.59 0-1.201.061a84.4 84.4 0 0 0 3.852-25.297c0-46.672-37.836-84.51-84.509-84.51S42.91 76.52 42.91 123.195a84.5 84.5 0 0 0 3.832 25.235h-.09c-3.38 0-6.433 1.294-8.592 3.647-2.773 3.021-4.007 6.733-3.47 10.446a12.8 12.8 0 0 0 1.73 4.823c-1.862 1.506-3.234 3.604-3.898 6.127a12.8 12.8 0 0 0 1.73 10.343q-.266.417-.497.86c-1.67 3.171-1.778 6.754-.303 10.091 2.236 5.057 7.79 9.041 18.577 13.318 6.707 2.66 12.85 4.361 12.904 4.376 8.872 2.301 16.896 3.47 23.842 3.47 11.406 0 19.916-3.12 25.351-9.284a85.5 85.5 0 0 0 27.747-.158c5.428 6.268 13.987 9.442 25.489 9.442 6.946 0 14.97-1.169 23.841-3.47.055-.015 6.195-1.716 12.905-4.376 10.787-4.277 16.342-8.261 18.577-13.318 1.474-3.337 1.367-6.92-.304-10.091q-.23-.447-.497-.86m-111.647 13.181a35 35 0 0 1-1.502 2.394c-1.405 2.057-3.253 3.629-5.398 4.797-4.1 2.236-9.29 3.017-14.562 3.017-8.329 0-16.867-1.949-21.652-3.19-.236-.061-29.334-8.28-25.65-15.276.62-1.177 1.64-1.647 2.925-1.647 5.187 0 14.632 7.724 18.69 7.724.908 0 1.548-.386 1.809-1.328 1.73-6.204-26.293-8.812-23.933-17.796.416-1.59 1.546-2.236 3.134-2.236 6.858-.001 22.25 12.06 25.469 12.06.247 0 .424-.073.52-.225l.041-.069c1.511-2.495.644-4.309-9.707-10.649l-.994-.605c-11.391-6.894-19.386-11.043-14.84-15.993.524-.571 1.266-.824 2.167-.824 1.068 0 2.36.357 3.785.957 6.016 2.537 14.354 9.456 17.837 12.473a146 146 0 0 1 1.633 1.441s4.41 4.586 7.076 4.586c.614 0 1.135-.242 1.488-.84 1.891-3.188-17.563-17.93-18.66-24.013-.744-4.121.522-6.209 2.862-6.209 1.113 0 2.47.474 3.97 1.425 4.65 2.951 13.628 18.379 16.915 24.381 1.102 2.011 2.983 2.861 4.678 2.861 3.363 0 5.992-3.343.308-7.591-8.543-6.392-5.545-16.84-1.468-17.483q.262-.042.525-.042c3.708 0 5.343 6.389 5.343 6.389s4.794 12.038 13.029 20.267c7.472 7.469 8.516 13.598 4.162 21.244m26.629 1.41-.427.051-.728.083q-.575.06-1.152.113l-.375.034-.343.029-.486.039-.537.039-.536.035-.119.008q-.21.013-.422.024l-.179.01q-.248.013-.5.024l-.581.025-.527.018-.352.01h-.179c-.11 0-.219.006-.329.007h-.174q-.165-.001-.329.005l-.448.006h-.625q-.737 0-1.469-.015l-.396-.009q-.17-.001-.337-.009l-.42-.012-.521-.02-.47-.021-.121-.005-.447-.023q-.187-.01-.372-.022l-.289-.017a80 80 0 0 1-1.089-.076l-.38-.031q-.24-.019-.479-.041-.28-.024-.561-.052a60 60 0 0 1-.939-.095h-.015c4.57-10.195 2.259-19.717-6.976-28.944-6.057-6.049-10.086-14.981-10.922-16.942-1.692-5.805-6.17-12.258-13.607-12.258q-.945 0-1.878.148c-3.258.513-6.106 2.388-8.138 5.21-2.196-2.731-4.33-4.902-6.26-6.128-2.91-1.845-5.814-2.781-8.643-2.781-3.531 0-6.687 1.45-8.887 4.08l-.056.067-.123-.52-.005-.023a74 74 0 0 1-1.054-5.412q.002-.018-.006-.036l-.063-.412q-.093-.61-.173-1.22l-.073-.555-.068-.555c-.022-.185-.04-.353-.06-.529l-.006-.044q-.12-1.08-.21-2.162l-.022-.277-.035-.472q-.016-.194-.027-.389c0-.031-.005-.061-.006-.09a53 53 0 0 1-.065-1.088q-.016-.283-.028-.567l-.02-.496-.005-.15-.016-.457-.01-.389c0-.155-.008-.31-.01-.465-.003-.155-.007-.325-.008-.489-.002-.164 0-.326-.005-.489-.004-.164 0-.327 0-.49 0-41.853 33.93-75.784 75.788-75.784 41.856 0 75.786 33.93 75.786 75.784v.979q-.002.245-.008.489c0 .135-.006.268-.01.405q-.002.182-.009.357c0 .153-.009.306-.014.459v.012l-.021.531-.021.466-.005.11-.027.496a81 81 0 0 1-.241 3.184v.013q-.025.261-.053.522l-.045.411-.089.804-.051.407-.063.479q-.034.262-.072.522-.04.292-.084.583l-.069.459-.082.52q-.042.26-.09.517c-.033.173-.059.345-.089.517q-.09.516-.189 1.031-.151.77-.31 1.537l-.11.507q-.055.254-.113.508c-2.133-2.073-4.958-3.202-8.073-3.202-2.827 0-5.734.935-8.643 2.78-1.93 1.226-4.063 3.398-6.26 6.128-2.035-2.822-4.883-4.697-8.139-5.21a12 12 0 0 0-1.878-.148c-7.439 0-11.914 6.453-13.607 12.258-.84 1.961-4.87 10.893-10.932 16.951-9.229 9.198-11.557 18.677-7.059 28.83m78.241-20.409-.03.089a5 5 0 0 1-.263.587q-.112.21-.244.408-.25.374-.549.711c-.046.052-.09.104-.142.155a8 8 0 0 1-.22.227c-1.346 1.334-3.398 2.504-5.718 3.577l-.799.358-.268.119c-.179.079-.358.157-.546.234q-.27.118-.551.232l-.558.23c-1.305.537-2.642 1.049-3.946 1.554l-.558.217-.551.216q-.55.214-1.085.429l-.531.214-.522.213-.256.108-.505.213c-3.837 1.647-6.598 3.322-6.018 5.4q.024.088.054.17.078.23.212.436.078.122.179.228c.682.709 1.923.597 3.488.034q.33-.121.656-.253l.136-.056c.358-.152.737-.322 1.124-.506.097-.046.195-.09.293-.141 1.914-.936 4.083-2.196 6.235-3.343a56 56 0 0 1 2.618-1.325c2.038-.959 3.954-1.639 5.494-1.639.723 0 1.361.148 1.893.488l.089.059c.334.235.614.537.823.887q.061.101.12.211c.761 1.445.124 2.941-1.367 4.408-1.431 1.409-3.657 2.79-6.187 4.068q-.281.143-.567.283c-7.53 3.698-17.391 6.483-17.528 6.518-2.628.681-6.386 1.575-10.62 2.244l-.626.098-.103.015q-.711.108-1.425.201-.725.097-1.462.179l-.09.01a69 69 0 0 1-5.358.406h-.026q-.971.035-1.943.035h-.747a46 46 0 0 1-2.959-.134q-.035.002-.071-.006-.53-.04-1.057-.099a40 40 0 0 1-1.092-.132 25 25 0 0 1-.715-.107 58 58 0 0 1-.725-.121l-.329-.062-.025-.005a26 26 0 0 1-1.036-.219c-.2-.045-.399-.089-.596-.143l-.119-.03q-.146-.037-.29-.076l-.053-.014-.308-.09-.336-.098-.039-.011-.291-.089q-.165-.052-.329-.106l-.268-.089-.197-.069q-.285-.1-.566-.208l-.178-.07-.147-.058a23 23 0 0 1-.845-.358l-.185-.09-.031-.014q-.099-.048-.197-.089a17 17 0 0 1-.384-.191l-.039-.019-.184-.097a16 16 0 0 1-.961-.546l-.172-.106a9 9 0 0 1-.256-.164l-.224-.148-.241-.166-.144-.103q-.228-.162-.447-.335l-.233-.179a15 15 0 0 1-.276-.228q-.115-.095-.227-.195l-.006-.005q-.121-.106-.239-.215a12 12 0 0 1-.232-.216l-.009-.009a9 9 0 0 1-.235-.232q-.117-.117-.231-.236t-.226-.243c-.074-.082-.142-.157-.212-.238l-.023-.027a9 9 0 0 1-.201-.238 12 12 0 0 1-.416-.525q-.203-.27-.396-.551l-.123-.184q-.246-.36-.479-.728a19 19 0 0 1-.339-.536q-.106-.17-.207-.339l-.028-.046q-.098-.164-.191-.327a3 3 0 0 1-.102-.179c-.033-.062-.071-.125-.106-.188l-.057-.099-.035-.064q-.1-.18-.197-.363-.044-.082-.09-.16l-.089-.173-.09-.171q-.338-.675-.638-1.368l-.071-.169q-.067-.17-.135-.336-.034-.08-.063-.161a17 17 0 0 1-.776-2.639q-.017-.083-.031-.163a12 12 0 0 1-.127-.806q-.012-.08-.02-.159l-.017-.162a11 11 0 0 1-.049-.638q-.002-.082-.007-.16a8 8 0 0 1-.008-.318c-.056-4.273 2.106-8.381 6.729-13.002 8.235-8.227 13.029-20.266 13.029-20.266s.129-.505.397-1.232q.055-.151.12-.314.234-.611.527-1.197l.039-.075q.25-.498.544-.971c.046-.073.09-.145.141-.218q.22-.326.465-.634c.089-.111.186-.221.283-.328q.057-.064.118-.124c.477-.493 1.022-.895 1.639-1.109l.078-.026q.078-.025.157-.048.091-.024.185-.043l.029-.006q.195-.039.395-.052h.011c.069 0 .139-.007.21-.007.089 0 .172 0 .259.009q.135.012.269.032c.742.118 1.448.56 2.056 1.242q.347.391.621.836.18.288.34.609c.043.089.084.171.124.259a8 8 0 0 1 .28.691q.293.827.439 1.692.126.758.15 1.526.012.41 0 .829a12 12 0 0 1-.787 3.792q-.065.167-.134.335a9 9 0 0 1-.302.665q-.12.248-.258.496-.09.165-.186.33-.242.41-.522.817l-.113.162a13.6 13.6 0 0 1-1.472 1.728 15.4 15.4 0 0 1-1.699 1.47q-.905.67-1.689 1.477c-1.503 1.577-1.853 2.969-1.515 4.024q.081.25.211.479.152.261.357.48l.053.055.054.054q.08.078.172.153l.06.048q.218.168.46.3c.047.025.089.05.142.074q.261.128.537.217.076.025.154.048l.065.017.09.024.077.019.084.018.083.017.079.013q.087.016.178.027l.057.009.104.01.064.007.105.007h.062l.11.006h.346l.099-.006.114-.007.139-.013.13-.015c.03 0 .06-.008.09-.014.4-.058.792-.164 1.167-.316l.159-.067a5 5 0 0 0 .772-.421q.344-.225.647-.503.072-.066.141-.133.034-.032.067-.067.067-.066.133-.138.482-.53.825-1.157a229 229 0 0 1 6.151-10.514l.294-.471.297-.471q.223-.358.447-.708l.15-.234q.747-1.17 1.519-2.324l.305-.456c.612-.907 1.222-1.789 1.827-2.627l.301-.415a56 56 0 0 1 2.054-2.661l.282-.338c.047-.056.09-.112.141-.166q.14-.165.277-.321c.046-.053.089-.105.138-.157l.268-.302.134-.147q.202-.217.397-.417c.09-.09.173-.179.259-.263a10.5 10.5 0 0 1 1.669-1.386l.14-.09q.201-.136.415-.25c2.364-1.342 4.321-1.441 5.448-.314.682.682 1.06 1.813 1.039 3.387q.001.103-.005.211v.077q-.002.108-.012.217c0 .09-.01.179-.019.269-.009.089-.014.157-.023.237q-.001.033-.008.069-.01.104-.025.211.002.032-.008.065a5 5 0 0 1-.041.283c-.011.09-.026.174-.042.262l-.026.149a4 4 0 0 1-.1.42 6 6 0 0 1-.283.758 11 11 0 0 1-.514.987q-.156.267-.322.526-.172.269-.358.543-.474.678-.985 1.328l-.156.197a51 51 0 0 1-1.722 2.035l-.187.21q-.378.422-.77.848l-.197.214c-.131.143-.268.286-.4.43q-.198.215-.406.433l-.411.433-.417.436-.42.436q-.423.438-.85.876c-4.055 4.159-8.327 8.304-9.773 10.888a5 5 0 0 0-.262.519c-.206.47-.292.872-.233 1.197a.9.9 0 0 0 .111.303q.122.212.295.387.08.078.168.144c.298.212.657.321 1.023.311h.114l.117-.009.117-.013.097-.014q.02-.002.04-.008l.089-.017.023-.005.099-.021.035-.009.104-.028c.035-.01.083-.023.125-.037q.264-.08.517-.188.133-.054.262-.117.068-.03.132-.063l.134-.066q.48-.25.936-.541l.133-.09q.068-.041.133-.089l.133-.089.071-.049.192-.135c.179-.123.346-.25.515-.379l.015-.012.269-.208c.367-.29.715-.582 1.031-.857l.21-.184.019-.018.11-.097c.258-.232.488-.448.679-.626l.079-.077q.103-.099.189-.178l.112-.111.04-.039.011-.011.117-.117.074-.077.009-.007.035-.032.044-.04.014-.013.037-.034.204-.179.114-.102q.091-.08.179-.162l.136-.121q.037-.03.074-.064l.143-.125.21-.185.112-.097c.435-.378.964-.835 1.572-1.35l.249-.211.411-.345.421-.351c.55-.457 1.142-.942 1.768-1.445l.411-.33q.526-.42 1.073-.849.22-.173.448-.344.558-.434 1.123-.859c.858-.648 1.743-1.297 2.639-1.929l.384-.268q.403-.279.805-.552l.243-.164q.72-.488 1.455-.951l.243-.153.241-.15q.363-.225.721-.44l.239-.143.478-.278.469-.269.095-.052.371-.204q.232-.126.463-.244l.229-.118.223-.112.231-.113a22 22 0 0 1 1.954-.845l.41-.144q.185-.062.358-.115l.04-.012q.093-.03.185-.054l.018-.005q.192-.055.38-.099h.009a8 8 0 0 1 1.077-.183q.25-.026.502-.025h.084q.167 0 .327.018c.049 0 .098.01.146.016h.02q.072.008.144.024.071.013.141.028h.015c.047.01.089.022.138.036.254.071.496.181.718.325q.164.107.305.24l.027.026.051.05.049.053c.402.421.734.904.984 1.43l.038.09q.128.3.2.618c.142.646.087 1.32-.158 1.935a4.7 4.7 0 0 1-.339.715 8 8 0 0 1-1.099 1.452l-.089.095q-.199.21-.416.42-.097.094-.198.188l-.205.189-.107.095q-.388.345-.792.669-.244.198-.493.388-.879.672-1.791 1.297a60 60 0 0 1-1.11.743 100 100 0 0 1-2.786 1.763c-1.968 1.21-4.149 2.504-6.474 3.911l-.602.365q-.988.603-1.867 1.152l-.295.185-.558.358q-.555.356-1.108.715l-.297.196q-.217.142-.433.286l-.141.09-.432.291-.229.157-.268.186-.249.173q-.626.442-1.162.843l-.134.102q-.315.239-.623.486-.464.373-.844.716l-.124.113q-.107.098-.208.194-.067.068-.137.133l-.064.064q-.215.216-.418.442l-.066.076q-.22.254-.387.492l-.05.071q-.132.192-.242.396-.028.051-.053.102l-.049.102-.033.074-.021.05-.017.045-.023.061a2.6 2.6 0 0 0-.13.523l-.008.062-.006.058v.315q-.002.038.007.08l.005.048c0 .026.006.051.01.079.004.026.011.073.019.11v.005q.01.052.023.104c.009.035.018.075.029.111q.03.106.068.211.023.063.048.124.002.012.01.025l.036.081.05.111q.081.172.177.336l.066.114.068.114a.4.4 0 0 0 .041.054l.022.023.025.023.026.02a.5.5 0 0 0 .122.063q.035.012.071.02c.577.13 1.763-.347 3.339-1.179.089-.048.187-.098.282-.15l.48-.262.234-.13c.167-.089.337-.19.511-.289l.317-.179c2.083-1.199 4.571-2.739 7.143-4.243q.362-.212.725-.421l.486-.311q.846-.485 1.7-.957a80 80 0 0 1 2.168-1.15l.476-.241q.474-.234.938-.456a41 41 0 0 1 1.815-.809l.335-.136.04-.016c1.775-.703 3.384-1.137 4.686-1.137q.423-.004.84.069h.009c.089.016.17.034.253.054h.015q.32.081.616.23c.275.142.522.331.731.559q.144.16.256.343.21.324.329.692c.032.095.06.188.089.287a3.93 3.93 0 0 1-.06 2.305"/><path fill="#FFD21E" fill-rule="evenodd" d="M203.21 123.685v-.491c0-41.854-33.918-75.783-75.775-75.783-41.856 0-75.787 33.931-75.787 75.783v.164a7 7 0 0 0 0 .327q.008.244.005.489l.005.36.003.129q0 .09.004.179.005.143.005.286l.011.389.016.457.005.15.02.473v.023l.027.553.001.014q.014.282.032.566.014.261.033.522l.002.031.03.448.003.04.033.432.003.028q.008.127.02.249.09 1.082.21 2.161l.004.045.061.529.068.555.05.377.023.177q.08.612.173 1.221l.004.027.059.384q.429 2.745 1.06 5.45l.005.022.032.135.091.385.056-.067c2.2-2.63 5.356-4.08 8.887-4.08 2.83 0 5.733.936 8.643 2.781 1.93 1.226 4.064 3.397 6.26 6.128 2.032-2.822 4.88-4.698 8.138-5.21.621-.098 1.25-.147 1.878-.148 7.436 0 11.915 6.453 13.607 12.258.836 1.961 4.865 10.893 10.941 16.935 9.236 9.227 11.547 18.748 6.976 28.943h.016q.466.053.939.096.28.028.561.052l.066.006.413.035.38.03q.543.042 1.089.077l.289.017.229.014.142.008.447.023.122.005.469.021.522.02.419.012.07.002q.133.006.267.007l.096.003q.885.021 1.769.02h.626l.447-.005q.164-.006.33-.005h.174l.151-.004q.088-.004.178-.004h.179l.351-.009.528-.018.581-.026q.252-.01.5-.023l.179-.011.266-.014.156-.009.118-.008.537-.035.536-.039.487-.039.342-.029.376-.034a62 62 0 0 0 1.88-.197l.427-.051c-4.499-10.152-2.17-19.632 7.027-28.822 6.063-6.058 10.092-14.99 10.932-16.952 1.693-5.804 6.169-12.257 13.607-12.257.629 0 1.258.05 1.879.148 3.255.512 6.103 2.388 8.138 5.21 2.197-2.73 4.33-4.903 6.261-6.129 2.909-1.844 5.815-2.78 8.642-2.78 3.116 0 5.94 1.13 8.073 3.203q.058-.253.114-.508l.109-.506q.059-.278.115-.555.1-.49.195-.984.099-.513.189-1.03l.031-.186.058-.331q.05-.258.09-.518l.011-.066.071-.453.07-.459v-.004q.076-.508.144-1.017l.011-.084.063-.478.051-.408.09-.804.035-.323.009-.088q.029-.261.053-.522v-.014l.039-.416q.107-1.182.179-2.375.013-.195.024-.392v-.006l.026-.491.006-.11q.024-.498.041-.996v-.012l.005-.13q.008-.164.009-.329l.002-.044q.007-.156.008-.314l.003-.09q.005-.157.006-.314l.002-.089q.005-.2.006-.4zm-94.572 75.706c6.002-8.801 5.576-15.407-2.658-23.637-8.236-8.231-13.029-20.267-13.029-20.267s-1.789-6.991-5.869-6.349-7.073 11.089 1.47 17.484c8.542 6.395-1.7 10.731-4.988 4.73-3.288-6.002-12.265-21.429-16.919-24.38s-7.927-1.297-6.83 4.785c.545 3.019 5.613 8.172 10.348 12.986 4.804 4.884 9.265 9.42 8.311 11.025-1.893 3.187-8.56-3.745-8.56-3.745s-20.876-18.998-25.42-14.047c-4.19 4.563 2.271 8.442 12.227 14.421q1.27.762 2.611 1.572c11.391 6.896 12.277 8.715 10.66 11.324-.597.964-4.41-1.325-9.1-4.14-7.995-4.801-18.537-11.13-20.026-5.465-1.288 4.903 6.468 7.907 13.502 10.632 5.86 2.27 11.22 4.346 10.431 7.164-.817 2.922-5.246.485-10.087-2.179-5.435-2.991-11.39-6.267-13.339-2.57-3.683 6.99 25.41 15.219 25.65 15.28 9.4 2.438 33.272 7.604 41.615-4.624m38.665 0c-6.002-8.801-5.576-15.407 2.659-23.637s13.028-20.267 13.028-20.267 1.789-6.991 5.869-6.349 7.073 11.089-1.469 17.484 1.699 10.731 4.987 4.73c3.289-6.002 12.26-21.429 16.914-24.38s7.929-1.297 6.831 4.785c-.544 3.019-5.613 8.172-10.348 12.987-4.804 4.884-9.265 9.419-8.312 11.024 1.893 3.187 8.565-3.749 8.565-3.749s20.875-18.997 25.421-14.046c4.189 4.562-2.272 8.442-12.229 14.421q-1.306.784-2.61 1.572c-11.391 6.896-12.277 8.715-10.661 11.323.598.965 4.411-1.325 9.1-4.14 7.996-4.8 18.538-11.13 20.027-5.464 1.289 4.903-6.468 7.907-13.502 10.632-5.86 2.27-11.22 4.346-10.432 7.164.816 2.921 5.244.484 10.084-2.18 5.435-2.991 11.391-6.269 13.339-2.569 3.684 6.994-25.414 15.215-25.649 15.275-9.4 2.446-33.272 7.612-41.612-4.616" clip-rule="evenodd"/><path fill="#32343D" fill-rule="evenodd" d="M152.047 102.567c1.182.418 2.061 1.69 2.897 2.901 1.13 1.636 2.182 3.159 3.796 2.301a10.91 10.91 0 0 0 4.247-15.214 10.91 10.91 0 0 0-7.742-5.198 10.904 10.904 0 0 0-11.689 6.589 10.9 10.9 0 0 0 .436 9.314c.748 1.407 2.408.743 4.16.042 1.373-.549 2.804-1.121 3.895-.735m-51.375 0c-1.182.418-2.061 1.691-2.897 2.901-1.13 1.637-2.183 3.159-3.796 2.301a10.903 10.903 0 0 1 8.263-20.068 10.91 10.91 0 0 1 7.707 9.348 10.9 10.9 0 0 1-1.221 6.211c-.749 1.407-2.409.743-4.161.043-1.374-.55-2.803-1.122-3.895-.736m43.427 46.751c8.143-6.415 11.134-16.889 11.134-23.341 0-5.1-3.431-3.495-8.924-.775l-.31.153c-5.042 2.497-11.754 5.822-19.122 5.822s-14.081-3.325-19.122-5.823c-5.671-2.809-9.228-4.571-9.228.624 0 6.656 3.182 17.585 11.916 23.934a18.97 18.97 0 0 1 11.575-9.786c.872-.26 1.77 1.241 2.689 2.778.887 1.482 1.794 2.998 2.716 2.998.983 0 1.948-1.494 2.891-2.952.985-1.525 1.946-3.01 2.875-2.713a18.97 18.97 0 0 1 10.91 9.081" clip-rule="evenodd"/><path fill="#FF323D" d="M144.097 149.317c-4.241 3.342-9.878 5.583-17.219 5.583-6.897 0-12.291-1.978-16.435-4.989a18.97 18.97 0 0 1 11.575-9.786c1.712-.511 3.527 5.776 5.405 5.776 2.01 0 3.947-6.246 5.766-5.665a18.97 18.97 0 0 1 10.908 9.081"/><path fill="#FFAD03" fill-rule="evenodd" d="M81.2 111.64a7.08 7.08 0 0 1-6.65.655 7.06 7.06 0 0 1-3.837-3.837 7.08 7.08 0 0 1 .657-6.65 7.087 7.087 0 1 1 9.83 9.832m101.413 0a7.08 7.08 0 0 1-6.651.655 7.06 7.06 0 0 1-3.837-3.837 7.1 7.1 0 0 1-.504-3.407 7.1 7.1 0 0 1 3.411-5.385 7.08 7.08 0 0 1 8.656 1.07 7.08 7.08 0 0 1 1.536 7.724 7.1 7.1 0 0 1-2.611 3.18" clip-rule="evenodd"/></svg>\" alt=\"Hugging Face Inference Model\" title=\"Hugging Face Inference Model\">\n          \n        </div>\n        <div class=\"workflow-details\">\n          <p class=\"workflow-details-name\">\n            <a href=\"https://n8n.io/workflows/1980-use-an-open-source-llm-via-huggingface/\" class=\"blog-banner-workflow\">Use an open-source LLM (via HuggingFace)</a>\n          </p>\n          <p class=\"workflow-details-stats\">\n          <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"12\" height=\"10\" fill=\"none\" viewBox=\"0 0 12 10\"><path fill=\"#707183\" d=\"M4.94 6.06a1.5 1.5 0 1 1 2.121-2.12 1.5 1.5 0 0 1-2.12 2.12Z\"></path><path fill=\"#707183\" fill-rule=\"evenodd\" d=\"M.462 6.143C1.38 7.135 3.294 9.2 6 9.2s4.62-2.065 5.539-3.057a1.703 1.703 0 0 0 0-2.332l-.001-.002C10.703 2.907 8.749.8 6 .8 3.256.8 1.304 2.903.466 3.806l-.004.005a1.703 1.703 0 0 0 0 2.332ZM3.77 3.305c.689-.38 1.447-.62 2.23-.705a5.88 5.88 0 0 1 3.972 2.17.301.301 0 0 1 0 .414A5.88 5.88 0 0 1 6 7.4 6.216 6.216 0 0 1 1.996 5.24a.3.3 0 0 1-.01-.423A5.937 5.937 0 0 1 3.77 3.305Z\" clip-rule=\"evenodd\"></path></svg>\n          35022 <span>‚Ä¢</span> <strong>by n8n-team</strong> <span>‚Ä¢</span> 1 year\n        </p>\n        </div>\n      </div>\n      <a href=\"https://n8n.io/workflows/1980-use-an-open-source-llm-via-huggingface/\" class=\"global-button blog-banner-workflow\">\n        Use this workflow\n      </a>\n    </div>\n    <div class=\"workflows-button\">\n      <a href=\"https://app.n8n.cloud/register\" class=\"global-button blog-banner-signup\">\n        Get started\n      </a>\n    </div>\n  </div>\n<!--kg-card-end: html-->\n\n\t\t<div class=\"newsletter-banner\">\n\t    <div class=\"newsletter-banner-content\">\n\t      <div class=\"section-header\">\n\t        <h2>Subscribe to <span>n8n newsletter</span></h2>\n\t        <div class=\"section-subheader--bottom\">\n\t          Get the best, coolest, and latest in automation and low-code delivered to your inbox each week.\n\t        </div>\n\t      </div>\n\t      <div class=\"newsletter-banner-form\">\n\t        <form autocomplete=\"off\" class=\"contact-form\" onsubmit=\"subscribeNewsletter(event)\">\n\t        \t<div id=\"recaptcha\" class=\"g-recaptcha\" data-sitekey=\"6LeAQeopAAAAAKlLsRb1weWm6T_vijoQBkGkbHzB\" data-callback=\"submitSubscription\" data-size=\"invisible\"><div class=\"grecaptcha-badge\" data-style=\"bottomright\" style=\"width: 256px; height: 60px; display: block; transition: right 0.3s ease 0s; position: fixed; bottom: 14px; right: -186px; box-shadow: gray 0px 0px 5px; border-radius: 2px; overflow: hidden;\"><div class=\"grecaptcha-logo\"><iframe title=\"reCAPTCHA\" width=\"256\" height=\"60\" role=\"presentation\" name=\"a-3zks7ygohn2h\" frameborder=\"0\" scrolling=\"no\" sandbox=\"allow-forms allow-popups allow-same-origin allow-scripts allow-top-navigation allow-modals allow-popups-to-escape-sandbox allow-storage-access-by-user-activation\" src=\"https://www.google.com/recaptcha/api2/anchor?ar=1&amp;k=6LeAQeopAAAAAKlLsRb1weWm6T_vijoQBkGkbHzB&amp;co=aHR0cHM6Ly9ibG9nLm44bi5pbzo0NDM.&amp;hl=en&amp;v=jt8Oh2-Ue1u7nEbJQUIdocyd&amp;size=invisible&amp;cb=gr5emnug79fs\"></iframe></div><div class=\"grecaptcha-error\"></div><textarea id=\"g-recaptcha-response\" name=\"g-recaptcha-response\" class=\"g-recaptcha-response\" style=\"width: 250px; height: 40px; border: 1px solid rgb(193, 193, 193); margin: 10px 25px; padding: 0px; resize: none; display: none;\"></textarea></div><iframe style=\"display: none;\"></iframe></div>\n\t          <div class=\"input-wrapper\">\n\t            <input placeholder=\"Email\" name=\"email\" type=\"email\" required=\"required\" class=\"\">\n\t            <div class=\"messages\">\n\t              <div class=\"message message--error\">Something went wrong. Please try again later.</div>\n\t              <div class=\"message message--success\">Subscribed!</div>\n\t            </div>\n\t          </div>\n\t          <button type=\"submit\" class=\"submit-btn\">Subscribe</button>\n\t        </form>\n\t      </div>\n\t    </div>\n    </div>\n\t\t<div class=\"post-share-section\">\n\t<div class=\"post-share-wrap\">\n\t\t<a href=\"https://twitter.com/intent/tweet?text=The%2011%20best%20open-source%20LLMs%20for%202025&amp;url=https://blog.n8n.io/open-source-llm/\" target=\"_blank\" rel=\"noopener\" aria-label=\"Twitter share icon\"><svg role=\"img\" viewBox=\"0 0 24 24\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M23.954 4.569c-.885.389-1.83.654-2.825.775 1.014-.611 1.794-1.574 2.163-2.723-.951.555-2.005.959-3.127 1.184-.896-.959-2.173-1.559-3.591-1.559-2.717 0-4.92 2.203-4.92 4.917 0 .39.045.765.127 1.124C7.691 8.094 4.066 6.13 1.64 3.161c-.427.722-.666 1.561-.666 2.475 0 1.71.87 3.213 2.188 4.096-.807-.026-1.566-.248-2.228-.616v.061c0 2.385 1.693 4.374 3.946 4.827-.413.111-.849.171-1.296.171-.314 0-.615-.03-.916-.086.631 1.953 2.445 3.377 4.604 3.417-1.68 1.319-3.809 2.105-6.102 2.105-.39 0-.779-.023-1.17-.067 2.189 1.394 4.768 2.209 7.557 2.209 9.054 0 13.999-7.496 13.999-13.986 0-.209 0-.42-.015-.63.961-.689 1.8-1.56 2.46-2.548l-.047-.02z\"></path></svg></a>\n\t\t<a href=\"https://www.facebook.com/sharer/sharer.php?u=https://blog.n8n.io/open-source-llm/\" target=\"_blank\" rel=\"noopener\" aria-label=\"Facebook share icon\"><svg role=\"img\" viewBox=\"0 0 24 24\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M23.9981 11.9991C23.9981 5.37216 18.626 0 11.9991 0C5.37216 0 0 5.37216 0 11.9991C0 17.9882 4.38789 22.9522 10.1242 23.8524V15.4676H7.07758V11.9991H10.1242V9.35553C10.1242 6.34826 11.9156 4.68714 14.6564 4.68714C15.9692 4.68714 17.3424 4.92149 17.3424 4.92149V7.87439H15.8294C14.3388 7.87439 13.8739 8.79933 13.8739 9.74824V11.9991H17.2018L16.6698 15.4676H13.8739V23.8524C19.6103 22.9522 23.9981 17.9882 23.9981 11.9991Z\"></path></svg></a>\n\t\t<!-- <a href=\"javascript:\" class=\"post-share-link\" id=\"copy\" data-clipboard-target=\"#copy-link\" aria-label=\"Copy link icon\"><svg role=\"img\" viewBox=\"0 0 33 24\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M27.3999996,13.4004128 L21.7999996,13.4004128 L21.7999996,19 L18.9999996,19 L18.9999996,13.4004128 L13.3999996,13.4004128 L13.3999996,10.6006192 L18.9999996,10.6006192 L18.9999996,5 L21.7999996,5 L21.7999996,10.6006192 L27.3999996,10.6006192 L27.3999996,13.4004128 Z M12,20.87 C7.101,20.87 3.13,16.898 3.13,12 C3.13,7.102 7.101,3.13 12,3.13 C12.091,3.13 12.181,3.139 12.272,3.142 C9.866,5.336 8.347,8.487 8.347,12 C8.347,15.512 9.866,18.662 12.271,20.857 C12.18,20.859 12.091,20.87 12,20.87 Z M20.347,0 C18.882,0 17.484,0.276 16.186,0.756 C14.882,0.271 13.473,0 12,0 C5.372,0 0,5.373 0,12 C0,18.628 5.372,24 12,24 C13.471,24 14.878,23.726 16.181,23.242 C17.481,23.724 18.88,24 20.347,24 C26.975,24 32.347,18.628 32.347,12 C32.347,5.373 26.975,0 20.347,0 Z\"/></svg></a>\n\t\t<small class=\"share-link-info\">The link has been copied!</small> -->\n\t</div>\n\t<input type=\"text\" value=\"https://blog.n8n.io/open-source-llm/\" id=\"copy-link\" aria-label=\"Copy link input\">\n</div>",
  "readme": "Open-source models are changing the LLM landscape, promising better security, cost-efficiency, and customization for AI deployments. While [_ChatGPT has over 180 million users_](https://nerdynav.com/chatgpt-statistics/?ref=blog.n8n.io), on-premises solutions already control more than half of the LLM market, with [_projections indicating continued growth_](https://market.us/report/large-language-model-llm-market/?ref=blog.n8n.io) in the coming years.\n\nThe trend is clear: since early 2023, new open-source model releases have nearly doubled compared to their closed-source counterparts.\n\n![LLM releases by year: blue cards = pre-trained models, orange cards = instruction-tuned. Top half shows open-source models, bottom half contains closed-source ones. Source: https://arxiv.org/abs/2307.06435](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdk1JmwPe5tLC6-XHppDpkBwOXd1t-WL4Pu871KVEdheZm03QYtJPD4WU1OXmG_ZiQDp-zHiN9BU5CruVQslAXf4QfqZd5mYPY4oUGbONPORbmVy9AZcy0hUh3QzsiBZ5ca46m1uw?key=INQ3nGlG9V9oPWqw4SkZT1dt)LLM releases by year: blue cards = pre-trained models, orange cards = instruction-tuned. Top half shows open-source models, bottom half contains closed-source ones. Source: [_https://arxiv.org/abs/2307.06435_](https://arxiv.org/abs/2307.06435?ref=blog.n8n.io)\n\nToday, we‚Äôll dive into the world of open-source LLMs and:\n\n  * discuss the reasons behind the surge in open-source LLM deployments;\n  * recognize potential pitfalls and challenges;\n  * review the 11 best open-source LLMs on the market;\n  * show you how to easily access these powerful open-source AI models;\n  * guide you on how to get started with open-source LLMs using [_Ollama and LangChain in n8n_](https://n8n.io/integrations/categories/ai/?ref=blog.n8n.io).\n\n\n\nRead on to find out!\n\n## Are there any open-source LLMs?\n\nFor this article, we‚Äôve selected 11 popular open-source LLM models, focusing on both widely used and available in [_Ollama_](https://ollama.com/library?ref=blog.n8n.io).\n\nOur review covers a range of pre-trained ‚Äúbase‚Äù models and their fine-tuned variants. These models come in various sizes, and you can either use them directly or opt for fine-tuned versions from original developers or third-party sources.\n\nüí°\n\nWhile pre-trained models provide a strong foundation, fine-tuned versions are typically necessary for practical, task-specific applications. Many vendors offer pre-fine-tuned models, but users can create their own datasets to further fine-tune for more specialized use cases.\n\nHere's our open-source LLM leaderboard:\n\nModel  \nFamily | Developer | Params | Context  \nwindow | Use-cases | License  \n---|---|---|---|---|---  \nLlama 3 | Meta | 1B, 3B, 8B,  \n70B, 405B | 8k, 128k | \\- General text generation  \n\\- Multilingual tasks  \n\\- Code generation  \n\\- Long-form content  \n\\- Fine-tuning for specific domains | Llama Community License  \nMistral | Mistral AI | 3B-124B | 32k-128k | \\- High-complexity tasks  \n\\- Multilingual processing  \n\\- Code generation  \n\\- Image understanding  \n\\- Edge computing  \n\\- On-device AI  \n\\- Function calling  \n\\- Efficient large-scale processing | Apache 2.0  \nMistral Research License  \nCommercial License  \nFalcon 3 | TII | 1B, 3B,  \n7B, 10B | 8k-32k | \\- General text generation  \n\\- Code generation  \n\\- Mathematical tasks  \n\\- Scientific knowledge  \n\\- Multilingual applications  \n\\- Fine-tuning for specific domains | TII Falcon License  \nGemma 2 | Google | 2B, 9B, 27B | 8k | \\- General text generation  \n\\- Question answering  \n\\- Summarization  \n\\- Code generation  \n\\- Fine-tuning for specific domains | Gemma license  \nPhi-3.x / 4 | Microsoft | 3.8B (mini)  \n7B (small)  \n14B (medium)  \n42B (MoE) | 4k, 8k, 128k  \n16k (Phi-4) | \\- General text generation  \n\\- Multi-lingual tasks  \n\\- Code understanding  \n\\- Math reasoning  \n\\- Image understanding (vision model)  \n\\- On-device inference | Microsoft Research  \nLicense  \nCommand R | Cohere | 7B, 35B, 104B | 128k | \\- Conversational AI  \n\\- RAG  \n\\- Tool use  \n\\- Multilingual tasks  \n\\- Long-form content generation | CC-BY-NC 4.0  \nStableLM 2 | Stability AI | 1.6B, 3B, 12B | Up to 16k | \\- Multilingual text generation  \n\\- Code generation and understanding  \n\\- Fine-tuning for specific tasks  \n\\- Research and commercial applications | Stability AI Community  \nand Enterprise licenses  \nStarCoder2 | BigCode | 3B, 7B, 15B | 16k | \\- Code completion  \n\\- Multi-language programming  \n\\- Code understanding  \n\\- Fine-tuning for specific tasks | Apache 2.0  \nYi | 01.AI | 6B, 9B, 34B | 4k, 8k, 200k | \\- Bilingual text generation  \n\\- Code understanding and generation  \n\\- Math and reasoning tasks  \n\\- Fine-tuning for specific domains | Apache 2.0  \nQwen2.5 | Alibaba | 0.5B to 72B | 128K | \\- General text generation  \n\\- Multilingual tasks  \n\\- Code generation  \n\\- Mathematical reasoning  \n\\- Structured data processing | Qwen license  \n(3B and 72B size models)  \nApache 2.0 (others)  \nDeepSeek-V2.x/V3 | DeepSeek AI | 16B, 236B,  \n671B for V3  \n(2.4B-37B  \nactivated) | 32k-128k | \\- General text generation  \n\\- Multilingual tasks  \n\\- Code generation  \n\\- Fine-tuning  \n\\- Advanced reasoning (V3) | DeepSeek License  \n  \nFor a comprehensive list of available LLMs beyond our selection, you can explore the [_Awesome-LLM GitHub repository_](https://github.com/Hannibal046/Awesome-LLM?ref=blog.n8n.io), which provides an extensive catalog of language models and related resources.\n\nüí°\n\nDid you know that in [_n8n ‚Äì a workflow automation tool_](https://n8n.io/integrations/?ref=blog.n8n.io) ‚Äì you can use open-source LLMs in several ways.   \n  \n****First**** , there is a dedicated node to connect to Ollama models ‚Äì the easiest way to start working with locally deployed LLMs.  \n  \n****Second**** , an OpenAI node allows you to specify a custom endpoint. This way you can swap between OpenAI and open-source LLMs ‚Äì a perfect solution for working with OpenRouter.  \n  \n****Finally**** , there are several nodes to other providers, such as HuggingFace, and even a custom HTTP Request node. Thanks to the straightforward user interface in n8n, your LLM-powered workflow automations and AI-agents remain the same when you switch between models. To easily deploy a local model, begin with a [_n8n‚Äôs self-hosted AI starter kit with Ollama_](https://github.com/n8n-io/self-hosted-ai-starter-kit?ref=blog.n8n.io) integration.\n\n## What are the advantages and disadvantages of open-source LLMs?\n\nOpen-source LLMs offer several advantages beyond publicly available model weights and increased transparency:\n\n  * Full ownership ensures complete control over the model, additional training data, and practical applications.\n  * Better fine-tuning accuracy is possible due to flexible customization of local model parameters, supported by community contributions.\n  * Longevity is guaranteed as self-hosted models don‚Äôt become obsolete, unlike closed-source providers who may ‚Äúretire‚Äù older models.\n  * Better cost estimation is possible as expenses shift from potentially volatile usage-based pricing to infrastructure costs. However, total costs may exceed subscription-based services, depending on usage patterns and infrastructure choices.\n  * Flexibility in choosing software and hardware combinations allows for optimal resource allocation based on specific needs.\n  * Community contributions enable model optimization through techniques like quantization and pruning, as well as the development of efficient deployment strategies and supporting tools.\n\n\n\nDespite their benefits, open-source LLMs come with some potential drawbacks:\n\n  * Quality may not match solutions offered by large corporations due to limited resources.\n  * Vulnerability to attacks is a concern, as bad actors can potentially manipulate input data and interfere with the model‚Äôs behavior in open-source environments.\n  * License requirements vary widely. Some models use permissive licenses (like Apache 2.0), others have non-commercial restrictions, and some (like Meta Llama 3) include specific terms for commercial usage.\n\n\n\nüîó\n\nLLMs are commonly used for [_chatbots_](https://blog.n8n.io/open-source-chatbot/), [_AI agents_](https://blog.n8n.io/llm-agents/) and [_workflow automations_](https://blog.n8n.io/ai-agentic-workflows/). Check out our earlier blog articles.\n\n## What is the best open-source LLM?\n\nThere is no single best open-source LLM. \n\nAnd here‚Äôs why.\n\nThere are many benchmarks for rating the models, and various research groups decide which benchmarks are suitable. This makes objective comparison rather non-trivial.\n\nThanks to the Hugging Face, there is a [_public leaderboard for the open-source LLMs_](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?ref=blog.n8n.io).\n\nIt [_performs tests on 6 key benchmarks_](https://huggingface.co/docs/leaderboards/open_llm_leaderboard/about?ref=blog.n8n.io) using the Eleuther AI Language Model Evaluation Harness. The results are aggregated and each model receives a final score.\n\nThe leaderboard has several quick filters for consumer-grade, edge device models and so on. Several adjustable columns such as model size, quantization method, etc. are also available.\n\nThe leaderboard is an open competition and anyone can submit their model for evaluation.\n\nLet‚Äôs take open-source LLMs one by one and have a closer look at them!\n\n### Llama3\n\n**Best for** : general-purpose applications with scalability needs\n\n![Llama3 is great for general-purpose applications with scalability needs](https://lh7-rt.googleusercontent.com/docsz/AD_4nXcNs-EXXhwbBahLP_uGL0DEP8hPIEj81LWKRqUr13CS4XlEExOU3GMPZvDpZSn_mOMi9eXPjQxJqW9WxqcXU6Y56okQgEHgl-Da2YBdBomvAhusoiYvMnzyxvM-NcpbFXRpXsmA?key=INQ3nGlG9V9oPWqw4SkZT1dt)Llama3 is great for general-purpose applications with scalability needs\n\n[ _Llama 3_](https://ai.meta.com/blog/meta-llama-3/?ref=blog.n8n.io) is Meta‚Äôs latest generation of open-source large language models, offering high performance across a wide range of tasks. The latest Llama 3.3 70B model offers performance comparable to the 405B parameter model at a fraction of the computational cost, making it an attractive option for developers and researchers.\n\n‚öôÔ∏è\n\n****Llama 3 key features****\n\n  * Multiple model sizes: 1B, 3B, 8B, 70B, and 405B parameters\n  * Multilingual and multimodal capabilities\n  * [ _Grouped Query Attention_](https://www.ibm.com/think/topics/grouped-query-attention?ref=blog.n8n.io) (GQA) for improved inference efficiency\n  * Context windows of 8k tokens for smaller models, up to 128k tokens for larger models\n  * Responsible AI development with tools like Llama Guard 2 and Code Shield\n\n\n\nü¶æ\n\n****Llama 3 use cases****\n\n  * General-purpose text generation and understanding\n  * Multilingual applications across various languages\n  * Code generation and understanding\n  * Long-form content creation and analysis\n  * Fine-tuning for specific domains or tasks\n  * Assistant-like interactions in chatbots and AI applications\n\n\n\n### Mistral\n\n**Best for** : on-device AI with function calling\n\n![Mistral models are best for on-device AI with function calling](https://lh7-rt.googleusercontent.com/docsz/AD_4nXeDPDSDS0Zu7yMbO_1NciZn8EUNfBbbvtlsb4LXxyiUOCYfLHtuzjubjNBwh9Qjt2IUVTpBuLKPgYM8lhZuctOdQug0ZwbkvMjg2-dqerMV_ggpO6eTMTMqWr-Tng7mm1yXNk3L?key=INQ3nGlG9V9oPWqw4SkZT1dt)Mistral models are best for on-device AI with function calling\n\n[ _Mistral AI_](https://mistral.ai/technology/?ref=blog.n8n.io#models), a French startup, has rapidly become a major player in the open-source LLM space. Mistral‚Äôs models are designed to cater to a wide range of applications, from edge devices to large-scale enterprise solutions. The company offers both open-source models under Apache 2.0 license and commercial models with negotiable licenses. The latest Ministral model (3B and 8B) is particularly noteworthy for its performance in edge computing scenarios, outperforming similarly-sized models from tech giants.\n\n‚öôÔ∏è\n\n****Mistral AI key features****\n\n  * Multiple model sizes: from 3B to 124B parameters\n  * Multilingual and multimodal capabilities\n  * Large context windows up to 128k tokens\n  * Native function calling support\n  * Mixture-of-experts (MoE) architecture in some models\n  * Efficient models for edge computing and on-device AI\n  * Fine-tuning capabilities for specific domains or tasks\n\n\n\nü¶æ\n\n****Mistral AI use cases****\n\n  * General-purpose text generation and understanding\n  * High-complexity reasoning and problem-solving\n  * Code generation and understanding\n  * Image analysis and multimodal tasks\n  * On-device AI for smartphones and laptops\n  * Efficient large-scale processing with MoE models\n\n\n\n### Falcon 3\n\n**Best for** : resource-constrained environments\n\n![Falcon 3 models shine in the resource-constrained environments](https://lh7-rt.googleusercontent.com/docsz/AD_4nXf1xoWpmVt4CR8s4DA5AgxLZW7MwJnl22H-C8a7TzRSvJdLHXQEjvkh-m0zOBBpC6SdVqhNfNwI6OlyNF_pZBv7zhEi7yo5kjVa4c7WKBcZk1lCnJEXaRjlJkJZQGtN6aXkOZt3mg?key=INQ3nGlG9V9oPWqw4SkZT1dt)Falcon 3 models shine in the resource-constrained environments\n\n[ _Falcon 3_](https://falconllm.tii.ae/falcon3/index.html?ref=blog.n8n.io) is the latest iteration of open-source large language models developed by the Technology Innovation Institute (TII) in Abu Dhabi. This family of models demonstrates impressive performance for small LLMs while democratizing access to advanced AI by enabling efficient operation on light infrastructures, including laptops.\n\n‚öôÔ∏è\n\n****Falcon 3 key features****\n\n  * Multiple model sizes: 1B, 3B, 7B, and 10B parameters\n  * Trained on 14 trillion tokens, more than double its predecessor\n  * Superior reasoning and enhanced fine-tuning capabilities\n  * Extended context windows up to 32k tokens (except 1B model with 8k)\n  * Multilingual support (English, French, Spanish, and Portuguese)\n  * Falcon3-Mamba-7B variant using an alternative [_State Space Model (SSM) architecture_](https://thegradient.pub/mamba-explained/?ref=blog.n8n.io)\n\n\n\nü¶æ\n\n****Falcon 3 use cases****\n\n  * General-purpose text generation and understanding\n  * Code generation and comprehension\n  * Mathematical and scientific tasks\n  * Multilingual applications\n  * Fine-tuning for specific domains or tasks\n  * Efficient deployment in resource-constrained environments\n\n\n\n### Gemma 2\n\n**Best for** : responsible AI development and deployment\n\n![Gemma 2 put emphasis on responsible AI development and deployment](https://lh7-rt.googleusercontent.com/docsz/AD_4nXcfdVE34X6asO7sAQzUzSdtM0FBDxw8PQaGcKEF7ruzsDNT8I2xWG7ZL67TyaJwsgK5UBquXJdLGufM6jNawGcxEqzYgGiRLTBHsgZ0npzmwEbSwyEUxbhAsrh1KiUkmDLq_75Meg?key=INQ3nGlG9V9oPWqw4SkZT1dt)Gemma 2 put emphasis on responsible AI development and deployment\n\n[ _Gemma 2_](https://ai.google.dev/gemma?ref=blog.n8n.io) is Google‚Äôs latest family of open-source LLMs, built on the same research and technology used to create the Gemini models. Offering strong performance for its size, Gemma 2 is designed with a focus on responsible AI development and efficient deployment.\n\n‚öôÔ∏è\n\n****Gemma 2 key features****\n\n  * Multiple model sizes: 2B, 9B, and 27B parameters\n  * Exceptional performance, with the 27B model outperforming some larger proprietary models\n  * Optimized for efficient inference across various hardware, from edge devices to cloud deployments\n  * Built-in safety advancements and responsible AI practices\n  * Broad framework compatibility (Keras, JAX, PyTorch, Hugging Face, etc.)\n  * Complementary tools: ShieldGemma for content safety and Gemma Scope for model interpretability\n\n\n\nü¶æ\n\n****Gemma 2 use cases****\n\n  * General-purpose text generation and understanding\n  * Question answering and summarization\n  * Code generation and understanding\n  * Fine-tuning for specific domains or tasks\n  * Responsible AI research and development\n  * On-device AI applications (especially with the 2B model)\n\n\n\n### Phi 3.x / 4\n\n**Best for** : cost-effective AI solutions\n\n![Phi 3.x / 4 models are best for cost-effective AI solutions](https://lh7-rt.googleusercontent.com/docsz/AD_4nXcx2s-pwPKAE0xEMkPK73iN1ovTRle20iGpOwPq-UbgEDNR1EcW17aAGP1aX3hZzh2zIRsAx5h5KXIuUNj4QxUbhme3yqXJWlOVBqBp3_3Hh8rbueqW7gs02-Y9WD85IyQtbmwq?key=INQ3nGlG9V9oPWqw4SkZT1dt)Phi 3.x / 4 models are best for cost-effective AI solutions\n\n[ _Phi-3.x / 4_](https://azure.microsoft.com/en-us/products/phi/?ref=blog.n8n.io) is Microsoft‚Äôs family of open-source Small Language Models (SLMs), designed to be highly capable and cost-effective. Phi-3.5 updates bring enhanced multi-lingual support, improved multi-frame image understanding, and a new MoE architecture. Phi-4, the latest model, emphasizes data quality over size. It was trained on synthetic data, filtered public content, and academic resources. The model achieves impressive performance over a range of benchmarks with just 16B parameters.\n\n‚öôÔ∏è\n\n****Phi LLM key features****\n\n  * Multiple model sizes: 3.8B (mini), 7B (small), 14B (medium), and 42B (MoE) parameters for Phi-3.x; 16B for Phi-4\n  * Long context window support up to 128K tokens for Phi-3.x, 16K for Phi-4\n  * Multilingual capabilities in over 20 languages\n  * Multi-modal support with Phi-3.5-vision for image understanding\n  * Mixture-of-Experts (MoE) architecture for improved efficiency\n  * Optimized for [_ONNX Runtime_](https://onnxruntime.ai/docs/?ref=blog.n8n.io) and various hardware targets\n  * Developed with Microsoft Responsible AI Standard\n\n\n\nü¶æ\n\n****Phi LLM use cases****\n\n  * General-purpose text generation and understanding\n  * Multilingual applications across various languages\n  * Code understanding and generation\n  * Mathematical reasoning and problem-solving\n  * On-device and offline inference scenarios\n  * Latency-sensitive applications\n  * Cost-effective AI solutions for resource-constrained environments\n\n\n\n### Command R\n\n**Best for** : enterprise-level conversational AI and RAG\n\n![Command R model allows for building enterprise-level conversational AI and RAG](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdHfb4eblhnTrB7TXUtycf_HHf-DA_6Oo-5O5VT4p89NTck4W42n1nJXMmfOJfCQR_vAmV3fjfm_Jefvc0hbAuD3jREwZbYZD759Y9uTdbH3Re43rLwPgZsuRwe5tJuDd-41uMrvw?key=INQ3nGlG9V9oPWqw4SkZT1dt)Command R model allows for building enterprise-level conversational AI and RAG\n\n[ _Command R_](https://cohere.com/command?ref=blog.n8n.io) is Cohere‚Äôs flagship family of LLMs for enterprise-level applications with a focus on conversational interaction and long-context tasks. The family includes Command R, Command R+, and the compact Command R7B, each optimized for different use cases.\n\n‚öôÔ∏è\n\n****Command R key features****\n\n  * Long context window of 128k tokens\n  * Multilingual capabilities in 10 primary languages and 13 additional languages\n  * Tool use and multi-step reasoning for complex tasks\n  * Customizable safety modes for responsible AI deployment\n  * Command R7B offers on-device inference capabilities\n\n\n\nü¶æ\n\n****Command R use cases****\n\n  * High-performance conversational AI and chatbots\n  * Complex RAG workflows for information retrieval and synthesis\n  * Multi-step tool use for dynamic, reasoning-based tasks\n  * Cross-lingual applications and translations\n  * Code generation and understanding\n  * Financial and numerical data analysis\n  * On-device applications (with Command R7B)\n\n\n\n### StableLM\n\n**Best for** : rapid prototyping and experimentation\n\n![StableLM is great for rapid prototyping and experimentation](https://lh7-rt.googleusercontent.com/docsz/AD_4nXd9IioEnSbEwrhM6AtqdwvzKb80PlUqYUZV8Z0k3GpZk_Wv96Ekj1A3X-JKI9lVjXjhZsiG-cDDbzPM-nr61PI-nGe-Cwi80-Eli5vqPTmNHxJmzxx5A-maiHLu6tjIEPMb7YON?key=INQ3nGlG9V9oPWqw4SkZT1dt)StableLM is great for rapid prototyping and experimentation\n\n[ _StableLM_](https://stability.ai/stable-lm?ref=blog.n8n.io) is Stability AI‚Äôs series of open-source LLMs, offering competitive performance in compact sizes. The family includes various model sizes and specializations. The 1.6B model, trained on approximately 2 trillion tokens, outperforms many models under 2B parameters on various benchmarks. Stability AI provides both base and instruction-tuned versions, along with pre-training checkpoints to facilitate further fine-tuning.\n\n‚öôÔ∏è\n\n****StableLM key features****\n\n  * Multiple model sizes: 1.6B, 3B, and 12B parameters\n  * Multilingual capabilities in English, Spanish, German, Italian, French, Portuguese, and Dutch\n  * Fill in Middle (FIM) capability for flexible code generation\n  * Long context support with sequences up to 16k tokens\n  * Optimized for speed and performance, enabling fast experimentation\n  * Specialized versions for code generation, Japanese and Arabic languages\n\n\n\nü¶æ\n\n****StableLM use cases****\n\n  * General-purpose text generation and understanding in multiple languages\n  * Code generation and understanding across various programming languages\n  * Fine-tuning for specific domains or tasks\n  * Research and commercial applications\n\n\n\n### Starcoder\n\n**Best for** : code-related tasks and multi-language programming\n\n![Starcoder is best-suited for code-related tasks and multi-language programming](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdgR2Po2mCK4FYwMKCPZyiZI3VT7ODdB_0K3elfwk-pIFN02dsbM__J93SLwB3MJ_kgd0Z9BssRo4DC0YiLOEhPEG2G2YKr6Xt17app8sT8BuWBFiYYc-JVfgnBx5_muREcQbUF?key=INQ3nGlG9V9oPWqw4SkZT1dt)Starcoder is best-suited for code-related tasks and multi-language programming\n\n[ _StarCoder2_](https://github.com/bigcode-project/starcoder2?ref=blog.n8n.io) is the next generation of transparently trained open-source language models for code, developed by the BigCode project. It offers high performance for code-related tasks across a wide range of programming languages. The 15B model, in particular, matches the performance of much larger 33B+ models on many evaluations, while the 3B model matches the performance of the previous 15B StarCoder model, showcasing significant improvements in efficiency and capability.\n\n‚öôÔ∏è\n\n****StarCoder2 key features****\n\n  * Multiple model sizes: 3B, 7B, and 15B parameters\n  * Trained on 600+ programming languages (15B model)\n  * Large context window of 16,384 tokens with sliding window attention of 4,096 tokens\n  * Grouped Query Attention (GQA) for improved efficiency\n  * Fill-in-the-Middle training objective\n  * Trained on 3+ trillion tokens (3B and 7B models) to 4+ trillion tokens (15B model)\n\n\n\nü¶æ\n\n****StarCoder2 use cases****\n\n  * Code completion and generation across multiple programming languages\n  * Code understanding and analysis\n  * Fine-tuning for specific programming tasks or languages\n  * Assisting developers in various coding scenarios\n  * Research in code language models and AI for programming\n\n\n\n### Yi\n\n**Best for** : bilingual applications (English and Chinese)\n\n![Yi LLM is great for bilingual applications \\(English and Chinese\\)](https://lh7-rt.googleusercontent.com/docsz/AD_4nXd90ameeVcaYVB8AZsRbu-B0GCZdxQG8_wKMxXhQ7gcM8S4k3Nvvq6HiOz39GgxUsLt8O_ucBXUxRuefoucxUhMYGyTYtopU_vRq99uoBTLJBiGanNHaiRXZHZhRBA1WIaCnUjUoA?key=INQ3nGlG9V9oPWqw4SkZT1dt)Yi LLM is great for bilingual applications (English and Chinese)\n\n[_Yi is a series of open-source LLMs_](https://huggingface.co/01-ai?ref=blog.n8n.io) developed by 01.AI, offering strong performance in both English and Chinese across a wide range of tasks. The Yi-1.5 series, an upgraded version of the original Yi models, delivers enhanced capabilities in coding, math, reasoning, and instruction-following.\n\n‚öôÔ∏è\n\n****Yi key features****\n\n  * Multiple model sizes: 6B, 9B, and 34B parameters\n  * Bilingual support for English and Chinese\n  * Extended context windows up to 200k tokens for larger models\n  * Continuous pre-training on high-quality corpus (500B tokens for Yi-1.5)\n  * Fine-tuned on 3M diverse samples for improved instruction-following\n  * Optimized for efficient deployment and fine-tuning\n\n\n\nü¶æ\n\n****Yi use cases****\n\n  * Bilingual text generation and understanding\n  * Code generation and comprehension\n  * Mathematical problem-solving and reasoning tasks\n  * Fine-tuning for domain-specific applications\n  * Natural language processing in academic and commercial settings\n  * Building chatbots and AI assistants\n\n\n\n### Qwen2.5\n\n**Best for** : multilingual and specialized tasks (coding and math)\n\n![Qwen2.5 works great for multilingual and specialized tasks \\(coding and math\\)](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdAM8Foi5i6zjGDAz9xWqkrM5FfulQIrrFh3DIP1mWE_XvomYoQSqom-UElQ76ofl-g_YAmUMt3KzMCPlTdgWuBlBNAPU4yA7D0qZOlcHvGHQDsa4l7wYSdG7bKI6LSSOk-hekuXA?key=INQ3nGlG9V9oPWqw4SkZT1dt)Qwen2.5 works great for multilingual and specialized tasks (coding and math)\n\n[_Qwen2.5_](https://qwenlm.github.io/blog/qwen2.5-coder-family/?ref=blog.n8n.io) is Alibaba‚Äôs latest series of open-source LLMs with a wide range of sizes and specialized variants for coding and mathematics. These models represent a significant advancement in multilingual capabilities and task-specific performance.\n\n‚öôÔ∏è\n\n****Qwen2.5 key features****\n\n  * Multiple model sizes: 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B parameters\n  * Pretrained on up to 18 trillion tokens\n  * 128K token context window with generation up to 8K tokens\n  * Multilingual support for over 29 languages\n  * Specialized models: Qwen2.5-Coder and Qwen2.5-Math\n  * Improved instruction following and structured data understanding\n  * Enhanced JSON output generation\n\n\n\nü¶æ\n\n****Qwen2.5 use cases****\n\n  * General-purpose text generation and understanding\n  * Multilingual applications across various languages\n  * Code generation and understanding with Qwen2.5-Coder\n  * Mathematical reasoning and problem-solving with Qwen2.5-Math\n  * Long-form content creation and analysis\n  * Structured data processing and JSON output generation\n  * Chatbot development with improved role-play capabilities\n\n\n\n### Deepseek 2.x / 3\n\n**Best for** : efficient large-scale language processing\n\n![Deepseek 2.x / 3 is a top LLM for efficient large-scale language processing](https://lh7-rt.googleusercontent.com/docsz/AD_4nXeMOB661VVWKZBwVvPhX8liuKnDy1tAjBZOiBXydNRDzGEs-pBAZDt3EVMDjnk-Y2J7RuwmDDiz6w2nyUUo8WB6vHif36vnyOOjNdBaqqjiRZTTnx5Up59wgMGuvVuN6VwPozP0Tg?key=INQ3nGlG9V9oPWqw4SkZT1dt)Deepseek 2.x / 3 is a top LLM for efficient large-scale language processing\n\n[ _DeepSeek_](https://www.deepseek.com/?ref=blog.n8n.io) is a series of powerful open-source LLMs developed by DeepSeek AI, featuring innovative architectures for efficient inference and cost-effective training. The DeepSeek-V2 and V2.5 models are available for use with Ollama. While the recently released DeepSeek-V3 offers even more impressive capabilities with its 671B parameters, it is not yet available in Ollama at the moment of writing.\n\n‚öôÔ∏è\n\n****DeepSeek key features****\n\n  * Mixture-of-Experts (MoE) architecture for efficient parameter usage\n  * Multi-head Latent Attention (MLA) for improved inference efficiency\n  * Large context windows of up to 128k tokens\n  * Multilingual capabilities, with strong performance in English and Chinese\n  * Optimized for both general text generation and coding tasks\n\n\n\nü¶æ\n\n****DeepSeek use cases****\n\n  * General-purpose text generation and understanding\n  * Multilingual applications and translations\n  * Code generation and understanding\n  * Fine-tuning for specific domains or tasks\n  * Assistant-like interactions in chatbots and AI applications\n  * Long-form content creation and analysis\n\n\n\n## Getting started with LangChain and open-source LLMs in n8n\n\nIf running an open-source LLM seems too complicated, we‚Äôve got great news: in n8n you can jump-start with Ollama. This powerful integration allows you to connect local models to real-world workflows and automate tasks in a meaningful way.\n\nBy combining the flexibility of open-source LLMs with the automation capabilities of n8n, you can build custom AI applications that are both powerful and efficient. LangChain (JavaScript version) is the main framework for building AI agents and LLM-powered workflows in n8n. The possibilities for customization and innovation are virtually limitless ‚Äì use hundreds of pre-built nodes or write custom JS scripts.\n\nLet‚Äôs explore how n8n makes creating custom LLM-powered apps and workflows easy!\n\nThere are at least 3 easy ways to build projects with open-source LLMs with n8n LangChain nodes:\n\n  1. Run small Hugging Face models with a [_User Access Token_](https://huggingface.co/docs/hub/security-tokens?ref=blog.n8n.io) completely for free.\n  2. If you want to run larger models or need a quick response, try the Hugging Face service called [_Custom Inference Endpoints_](https://huggingface.co/inference-endpoints?ref=blog.n8n.io).\n  3. If you have enough computing resources, run the model via [_Ollama_](https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmollama?ref=blog.n8n.io) locally or self-hosted.\n\n\n\n[_LangChain nodes in n8n + Ollama integration_](https://docs.n8n.io/hosting/starter-kits/ai-starter-kit/?ref=blog.n8n.io) make it easier to access open-source LLMs and give you handy tools for working with them. Here‚Äôs a video with an overview of the most important aspects:\n\nAfter you‚Äôve installed the self-hosted AI Starter Kit, it‚Äôs time for a practical part!\n\nHere is a workflow template that is particularly useful for enterprise environments where data privacy is crucial. It allows for on-premises processing of personal information.\n\n![This workflow takes an input and extracts user information in a consistent JSON format](https://blog.n8n.io/content/images/2025/01/WK-Data-ext2-1.png)This workflow takes an input and extracts user information in a consistent JSON format\n\n__![Ollama Chat Model](data:image/svg+xml;base64,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) ______ +3\n\n[Extract personal data with self-hosted LLM Mistral NeMo](https://n8n.io/workflows/2766-extract-personal-data-with-self-hosted-llm-mistral-nemo/)\n\nby yulia\n\n[ Use this workflow ](https://n8n.io/workflows/2766-extract-personal-data-with-self-hosted-llm-mistral-nemo/)\n\n### Step 1: Configure the Basic LLM Chain node\n\n![Provide a system prompt and make sure a user message is coming for the correct input](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdhMUK2iNl9WeDN06C2gN4ok2boEvaAAGnWx2KMEIpoa2Co6HhsaKWIfpHMHGh-BjjR0FR1Us38mdUSx9sW2ALlbq0YAVpxWxtC8GZJ3EI50ttS0pbVpTiVje5zwxhZzcFwr8Xi?key=INQ3nGlG9V9oPWqw4SkZT1dt)Provide a system prompt and make sure a user message is coming for the correct input\n\nThe core of the workflow is the [_Basic LLM Chain node_](https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.chainllm/?ref=blog.n8n.io). Configure it as follows:\n\n  * Activate the Require Specific Output Format toggle;\n  * In the Messages section, add a system message with the following content:`Please analyse the incoming user request. Extract information according to the JSON schema. Today is: {{ $now.toISO() }}`This is the main prompt with the general task.\n\n\n\n### Step 2: Add the Chat Trigger node\n\nFor this example, we‚Äôre using a [_Chat Trigger_](https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-langchain.chattrigger/?ref=blog.n8n.io) to simulate user input.\n\nüí° In a real-world scenario, this could be replaced with various data sources such as database queries, voice transcripts, or incoming Webhook data.\n\n### Step 3: Configure the Ollama Chat Model node\n\n![Ollama provides several additional settings that are specific to self-hosted LLMs](https://lh7-rt.googleusercontent.com/docsz/AD_4nXe_9JkZcwIFJmGTUNh6PWO1GrLrqdAVhV6FQ44st0A4lgKa4Rd7fGiIi70ZFOJN4zEG4OtOCVQrgzmeBSYDOkckKYLZt2voymt_FpVlMDBsWi9PJ_yZgydacH1iJez7Z03MkHRrYg?key=INQ3nGlG9V9oPWqw4SkZT1dt)Ollama provides several additional settings that are specific to self-hosted LLMs\n\nConnect the [_Ollama Chat Model_](https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatollama/?ref=blog.n8n.io) node to provide the language model capabilities:\n\n  * Set the model to `mistral-nemo:latest`\n  * Set temperature to `0.1` for more consistent outputs\n  * Set keep Alive setting to `2h` to maintain the model in memory\n  * Enable the Use Memory Locking toggle for improved performance\n\n\n\n### Step 4: Ensure consistent structured output\n\n  1. Add an [_Auto-fixing Output Parser node_](https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.outputparserautofixing/?ref=blog.n8n.io) and connect it to the same Ollama Chat Model.\n\n\n\nAdd a [_Structured Output Parser node_](https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.outputparserstructured/?ref=blog.n8n.io) with the following JSON schema:\n    \n    \n    {\n      \"type\": \"object\",\n      \"properties\": {\n        \"name\": {\n          \"type\": \"string\",\n          \"description\": \"Name of the user\"\n        },\n        \"surname\": {\n          \"type\": \"string\",\n          \"description\": \"Surname of the user\"\n        },\n        \"commtype\": {\n          \"type\": \"string\",\n          \"enum\": [\"email\", \"phone\", \"other\"],\n          \"description\": \"Method of communication\"\n        },\n        \"contacts\": {\n          \"type\": \"string\",\n          \"description\": \"Contact details. ONLY IF PROVIDED\"\n        },\n        \"timestamp\": {\n          \"type\": \"string\",\n          \"format\": \"date-time\",\n          \"description\": \"When the communication occurred\"\n        },\n        \"subject\": {\n          \"type\": \"string\",\n          \"description\": \"Brief description of the communication topic\"\n        }\n      },\n      \"required\": [\"name\", \"communicationType\"]\n    }\n    \n    \n\nThis JSON schema defines several JSON keys to collect various data like name, surname, communication method, user contacts, topic and the timestamp. However, only the name and the communication method are mandatory parameters. You can adjust the schema according to your needs.\n\n### Step 5: Process the output\n\n![As an optional step, transform the Basic LLM Chain output](https://lh7-rt.googleusercontent.com/docsz/AD_4nXfyOgsB5H6zjCeBegEW54Q00X8atspKeS7AEDxNtxIVTqLOFzKHGqChQ3O6qROpyTEswefxFXBAz-mGhmr8tpx-IGXIz5OGiWJrHdnTPvVSByBp6oEtIk-he8aFN3vjMEdUGxTPRQ?key=INQ3nGlG9V9oPWqw4SkZT1dt)As an optional step, transform the Basic LLM Chain output\n\nAfter the Basic LLM Chain node processes the request, it will produce a JSON with an output key. Transform this output using a Set node:\n\nSet the Mode to `JSON`  \nUse the following expression: `{{ $json.output }}`\n\nAdding a Set node is optional, which we did just for convenience.\n\n### Step 6: Handle errors\n\nAdd a [_No Operation node_](https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.noop/?ref=blog.n8n.io) after the Error output from the Basic LLM Chain node. This serves as an intermediary step before further error processing.\n\nThat‚Äôs it! Now you‚Äôre done and can test the workflow. Press the Chat button in the bottom middle part of your instance and provide a text message. For example:\n\n`Hi, my name is John. I'd like to be contacted via E-mail at john.smith@example.com regarding my recent order #12345.`\n\nYou can easily adapt this template to various enterprise use cases by modifying the input source, output schema or post-processing steps.\n\nIf you have a specific storage system where you‚Äôd like to save the result, consider switching the Basic LLM Chain node to a [_Tools Agent node_](https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.agent/tools-agent/?ref=blog.n8n.io). Modern LLMs have [_built-in capabilities for function calling_](https://docs.mistral.ai/capabilities/function_calling/?ref=blog.n8n.io), so you can define the desired output format which can immediately connect to a database and upload the parsed information.\n\nüí°\n\nAdditionally, special [_LangChain Code_](https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.code?ref=blog.n8n.io) and [_Code Tool_](https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.toolcode?ref=blog.n8n.io) nodes allow you to create completely custom chains. You can build whatever is supported by the LangChainJS library, even if a ready-made node is not yet available.\n\n## FAQs \n\n#### Which types of open-source LLMs are there?\n\nOpen-source models fall into two main categories:\n\n  * ****Pre-trained LLMs**** are created using vast amounts of text data. These models excel at understanding broad contexts and generating coherent text. While valuable for research and general language tasks, they may struggle with specific instructions or specialized applications.\n  * ****Fine-tuned LLMs**** are adapted from pre-trained models. They undergo additional training on targeted datasets, making them more effective for particular use cases like classification, summarization, or question-answering. Fine-tuned models are essential for modern applications such as turn-based chat messaging and function calling.\n\n\n\nNote that some authors distinguish fine-tuning from[ _continuous pre-training_](https://medium.com/@eordaxd/fine-tuning-vs-pre-training-651d05186faf?ref=blog.n8n.io). The latter involves further pre-training a model with domain-specific data, such as medical or financial reports, to adapt it to a particular field.\n\n#### How to get started with an open-source LLM?\n\nThere are two main approaches to setting up and using open-source LLMs:\n\n  1. ****Install locally****. Helper tools such as Ollama simplify the process. However, the larger the model, the more difficult it is to meet the hardware requirements. The largest models require industrial-level equipment.\n  2. Instead of hosting everything locally, it‚Äôs also possible to ****rent a virtual server.****\n\n\n\n****VPS with a GPU**** allows for faster inference, but is more expensive. Several hosting providers have automated the process of model installation and deployment, so the entire setup requires just a few clicks and some waiting time.\n\n****Traditional CPU-only virtual servers**** could be a more cost-efficient alternative, especially when deploying smaller language models without strict requirements on response time.\n\n#### How to run open-source LLM locally?\n\nThere are several ways to run LLMs locally. The easiest approach is to use one of the available frameworks, which can get you up and running in just a few clicks:\n\n  * ****Ollama + OpenWebUI**** : Ollama as a backend for quick LLM deployment, OpenWebUI as a user-friendly frontend\n  * ****GPT4All**** : General-purpose AI applications and document chat\n  * ****LM Studio**** : LLM customization and fine-tuning\n  * ****Jan**** : Privacy-focused LLM interactions with flexible server options\n  * ****NextChat**** : Building conversational AI with support for various LLMs\n\n\n\n#### How much RAM do I need to run an LLM?\n\nTo work, most LLMs have to be loaded into memory (RAM or GPU VRAM). How much memory you need depends on multiple factors (model size, quantization, etc.) as well as specific use-cases (for example, simple inference vs fine-tuning).\n\nThanks to recent advances, some efficient small language models (SLMs) can run simple tasks on systems with just 4 GB of free RAM. During fine-tuning, however, the requirements increase, because you need to store intermediate steps while model parameter values are updated.\n\nTo check specific hardware requirements for an open-source LLM, look up its model card on Hugging Face, GitHub, or the developer's website. For quick estimates, you can use the[ _\"Can you run it?\" tool for LLMs_](https://huggingface.co/spaces/Vokturz/can-it-run-llm?ref=blog.n8n.io).\n\n#### How much does it cost to run an open-source LLM?\n\nWhile open-source models are free to use, the deployment and infrastructure costs vary. The main cost when running open-source LLMs is hardware. Here‚Äôs a concise breakdown of costs depending on different deployment options:\n\n  * Locally: free if your computer meets system requirements\n  * Managed API providers: free limited options or fees comparable to popular services like OpenAI / Anthropic\n  * Simple VPS: starting from $20/mo for CPU-only servers; GPU server prices are higher, up to dozens of dollars per hour\n  * Managed options with one-click install on GPU servers: premium pricing\n\n\n\n#### Are open-source LLMs secure?\n\nOpen-source LLMs offer transparency but also present certain security challenges:\n\n  1. Potential vulnerabilities: the publicly available model weights and architecture can attract both collaborators and potential attackers.\n  2. Adversarial attacks: methods like data poisoning, prompt injection, and model evasion can alter input data to produce incorrect or unintended results.\n  3. Wider attack surface: as open-source LLMs are integrated into more applications and platforms, the potential for attacks increases.\n\n\n\nWhile the open-source community actively works on improving LLM security, users should implement additional safeguards. We recommend gating open-source LLMs during prototyping and rollout, making them accessible only through internal services (e.g. via n8n rather than directly by users).\n\n#### Why to use open-source LLMs commercially?\n\nWe‚Äôve gathered insights from real-world users on [_Reddit_](https://www.reddit.com/r/LocalLLaMA/comments/1cub6sg/who_is_using_opensource_llms_commercially/?ref=blog.n8n.io) to understand why businesses choose open-source LLMs. Here are the key reasons:\n\n  1. ****Efficient for simple tasks**** : smaller open-source models can handle basic text generation, classification, and function calling effectively.\n  2. ****Data privacy**** : ideal for processing sensitive documents without relying on external cloud services.\n  3. ****Integration with existing infrastructure**** : easy to incorporate if you‚Äôre already running ML models on your own GPUs.\n  4. ****Cost-effective for high volumes**** : fine-tuning smaller open-source models can offer a better price-performance ratio for large-scale operations.\n  5. ****Customization**** : allows setting your own guidelines to align with company policies and ethical standards.\n  6. ****Transparency**** : offers the ability to review training data and understand the model‚Äôs architecture.\n  7. ****Control over costs**** : prototyping with open-source models helps manage expenses before committing to specific providers.ntv \n\n\n\nüí°\n\nFor a detailed guide on these frameworks and how to use them, check out a [_comprehensive guide on running local LLMs_](https://blog.n8n.io/local-llm/).\n\n## Wrap Up\n\nIn this article, we've highlighted that the best open-source LLM depends on your specific use case, as models like Llama3, Mistral, and Falcon 3 excel in different areas such as speed, accuracy, or resource efficiency. We emphasized evaluating models based on factors like task requirements, deployment setup, and available resources.\n\nAdditionally, we explained how [tools like n8n](https://n8n.io/ai/?ref=blog.n8n.io) and LangChain simplify integrating these LLMs into workflows, making it easier to experiment and find the right fit.\n\n### Create your own LLM workflows\n\nBuild complex automations 10x faster, without fighting APIs\n\n[Try n8n now](https://app.n8n.cloud/register?ref=blog.n8n.io)\n\n## What‚Äôs next?\n\nNow that you‚Äôve got a grasp on using open-source LLMs with n8n, you can explore more advanced AI-powered automation scenarios. Many of the concepts we‚Äôve covered in our other AI-related articles can be applied to local models as well.\n\nHere are some resources to continue your journey:\n\n  * Learn about [_AI workflow automation trends_](https://blog.n8n.io/ai-workflow-automation/);\n  * Create intelligent workflows with [_AI agents in n8n automation_](https://blog.n8n.io/ai-agentic-workflows/);\n\n\n\nBuild your own [_AI chatbot using n8n and Telegram_](https://blog.n8n.io/telegram-bots/).\n\n### Most popular workflows with these integrations\n\n____![Ollama Model](data:image/svg+xml;base64,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) ____\n\n[üîêü¶ôü§ñ Private & Local Ollama Self-Hosted AI Assistant](https://n8n.io/workflows/2729-private-and-local-ollama-self-hosted-ai-assistant/)\n\n39898 ‚Ä¢ **by joe** ‚Ä¢ 4 months \n\n[ Use this workflow ](https://n8n.io/workflows/2729-private-and-local-ollama-self-hosted-ai-assistant/)\n\n__![Ollama Chat Model](data:image/svg+xml;base64,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) ____\n\n[Chat with local LLMs using n8n and Ollama](https://n8n.io/workflows/2384-chat-with-local-llms-using-n8n-and-ollama/)\n\n65677 ‚Ä¢ **by mihailtd** ‚Ä¢ 9 months \n\n[ Use this workflow ](https://n8n.io/workflows/2384-chat-with-local-llms-using-n8n-and-ollama/)\n\n______![Hugging Face Inference Model](data:image/svg+xml;base64,<svg xmlns="http://www.w3.org/2000/svg" width="256" height="256" fill="none"><path fill="#fff" d="M230.721 172.7a19 19 0 0 0-2.575-5.692q.376-1.377.568-2.791c.829-5.976-1.243-11.447-5.147-15.68-2.115-2.312-4.382-3.839-6.783-4.776a90.6 90.6 0 0 0 2.377-20.568c0-3.163-.179-6.261-.479-9.313a106 106 0 0 0-.567-4.56 91 91 0 0 0-3.051-13.21 91 91 0 0 0-3.054-8.374 92 92 0 0 0-6.041-11.754 81 81 0 0 0-4.907-7.262 69 69 0 0 0-2.704-3.446 90.5 90.5 0 0 0-9.033-9.486 70 70 0 0 0-3.315-2.862 82 82 0 0 0-3.424-2.704 96 96 0 0 0-7.262-4.907c-13.781-8.37-29.942-13.17-47.215-13.17-50.292 0-91.052 40.762-91.052 91.051-.002 7.012.81 14 2.42 20.824-2.16.938-4.23 2.4-6.15 4.515-3.903 4.231-5.976 9.682-5.147 15.658q.189 1.425.567 2.813a19 19 0 0 0-2.573 5.694c-1.2 4.561-.805 8.674.72 12.278-1.658 4.71-1.244 9.726.915 14.087 1.57 3.185 3.817 5.649 6.587 7.851 3.293 2.618 7.415 4.842 12.387 6.976 5.932 2.53 13.173 4.907 16.466 5.779 8.506 2.202 16.662 3.598 24.928 3.666 11.777.109 21.919-2.66 29.18-9.747a88 88 0 0 0 10.752.654 94 94 0 0 0 11.358-.715c7.244 7.132 17.425 9.926 29.245 9.814 8.265-.066 16.421-1.462 24.905-3.667 3.315-.872 10.553-3.249 16.488-5.779 4.972-2.137 9.094-4.361 12.409-6.975 2.749-2.203 4.994-4.666 6.565-7.851 2.181-4.362 2.573-9.378.938-14.088 1.51-3.604 1.903-7.726.704-12.283m-8.44 11.973c1.671 3.171 1.778 6.754.304 10.091-2.236 5.057-7.79 9.041-18.577 13.318-6.708 2.66-12.85 4.361-12.904 4.376-8.872 2.301-16.896 3.47-23.842 3.47-11.502 0-20.061-3.174-25.489-9.442a85.5 85.5 0 0 1-27.747.158c-5.435 6.164-13.945 9.284-25.35 9.284-6.947 0-14.97-1.169-23.843-3.47-.054-.015-6.194-1.716-12.904-4.376-10.786-4.277-16.342-8.258-18.577-13.318-1.474-3.337-1.367-6.92.304-10.091q.231-.443.497-.86a12.8 12.8 0 0 1-1.728-10.341c.664-2.523 2.035-4.621 3.897-6.128a12.75 12.75 0 0 1-1.73-4.822c-.536-3.714.697-7.422 3.47-10.446 2.16-2.353 5.213-3.648 8.593-3.648h.09a84.5 84.5 0 0 1-3.832-25.235c0-46.671 37.836-84.51 84.514-84.51s84.513 37.835 84.513 84.51a84.4 84.4 0 0 1-3.859 25.299q.612-.06 1.201-.061c3.38 0 6.434 1.295 8.592 3.648 2.773 3.021 4.007 6.732 3.47 10.446a12.8 12.8 0 0 1-1.729 4.822c1.862 1.507 3.234 3.605 3.897 6.128a12.8 12.8 0 0 1-1.728 10.341q.267.413.497.857"/><path fill="#FF9D00" d="M221.784 183.816a12.8 12.8 0 0 0 1.728-10.341c-.664-2.523-2.036-4.621-3.897-6.128a12.7 12.7 0 0 0 1.729-4.822c.537-3.714-.696-7.422-3.47-10.446-2.158-2.353-5.212-3.648-8.592-3.648q-.59 0-1.201.061a84.4 84.4 0 0 0 3.852-25.297c0-46.672-37.836-84.51-84.509-84.51S42.91 76.52 42.91 123.195a84.5 84.5 0 0 0 3.832 25.235h-.09c-3.38 0-6.433 1.294-8.592 3.647-2.773 3.021-4.007 6.733-3.47 10.446a12.8 12.8 0 0 0 1.73 4.823c-1.862 1.506-3.234 3.604-3.898 6.127a12.8 12.8 0 0 0 1.73 10.343q-.266.417-.497.86c-1.67 3.171-1.778 6.754-.303 10.091 2.236 5.057 7.79 9.041 18.577 13.318 6.707 2.66 12.85 4.361 12.904 4.376 8.872 2.301 16.896 3.47 23.842 3.47 11.406 0 19.916-3.12 25.351-9.284a85.5 85.5 0 0 0 27.747-.158c5.428 6.268 13.987 9.442 25.489 9.442 6.946 0 14.97-1.169 23.841-3.47.055-.015 6.195-1.716 12.905-4.376 10.787-4.277 16.342-8.261 18.577-13.318 1.474-3.337 1.367-6.92-.304-10.091q-.23-.447-.497-.86m-111.647 13.181a35 35 0 0 1-1.502 2.394c-1.405 2.057-3.253 3.629-5.398 4.797-4.1 2.236-9.29 3.017-14.562 3.017-8.329 0-16.867-1.949-21.652-3.19-.236-.061-29.334-8.28-25.65-15.276.62-1.177 1.64-1.647 2.925-1.647 5.187 0 14.632 7.724 18.69 7.724.908 0 1.548-.386 1.809-1.328 1.73-6.204-26.293-8.812-23.933-17.796.416-1.59 1.546-2.236 3.134-2.236 6.858-.001 22.25 12.06 25.469 12.06.247 0 .424-.073.52-.225l.041-.069c1.511-2.495.644-4.309-9.707-10.649l-.994-.605c-11.391-6.894-19.386-11.043-14.84-15.993.524-.571 1.266-.824 2.167-.824 1.068 0 2.36.357 3.785.957 6.016 2.537 14.354 9.456 17.837 12.473a146 146 0 0 1 1.633 1.441s4.41 4.586 7.076 4.586c.614 0 1.135-.242 1.488-.84 1.891-3.188-17.563-17.93-18.66-24.013-.744-4.121.522-6.209 2.862-6.209 1.113 0 2.47.474 3.97 1.425 4.65 2.951 13.628 18.379 16.915 24.381 1.102 2.011 2.983 2.861 4.678 2.861 3.363 0 5.992-3.343.308-7.591-8.543-6.392-5.545-16.84-1.468-17.483q.262-.042.525-.042c3.708 0 5.343 6.389 5.343 6.389s4.794 12.038 13.029 20.267c7.472 7.469 8.516 13.598 4.162 21.244m26.629 1.41-.427.051-.728.083q-.575.06-1.152.113l-.375.034-.343.029-.486.039-.537.039-.536.035-.119.008q-.21.013-.422.024l-.179.01q-.248.013-.5.024l-.581.025-.527.018-.352.01h-.179c-.11 0-.219.006-.329.007h-.174q-.165-.001-.329.005l-.448.006h-.625q-.737 0-1.469-.015l-.396-.009q-.17-.001-.337-.009l-.42-.012-.521-.02-.47-.021-.121-.005-.447-.023q-.187-.01-.372-.022l-.289-.017a80 80 0 0 1-1.089-.076l-.38-.031q-.24-.019-.479-.041-.28-.024-.561-.052a60 60 0 0 1-.939-.095h-.015c4.57-10.195 2.259-19.717-6.976-28.944-6.057-6.049-10.086-14.981-10.922-16.942-1.692-5.805-6.17-12.258-13.607-12.258q-.945 0-1.878.148c-3.258.513-6.106 2.388-8.138 5.21-2.196-2.731-4.33-4.902-6.26-6.128-2.91-1.845-5.814-2.781-8.643-2.781-3.531 0-6.687 1.45-8.887 4.08l-.056.067-.123-.52-.005-.023a74 74 0 0 1-1.054-5.412q.002-.018-.006-.036l-.063-.412q-.093-.61-.173-1.22l-.073-.555-.068-.555c-.022-.185-.04-.353-.06-.529l-.006-.044q-.12-1.08-.21-2.162l-.022-.277-.035-.472q-.016-.194-.027-.389c0-.031-.005-.061-.006-.09a53 53 0 0 1-.065-1.088q-.016-.283-.028-.567l-.02-.496-.005-.15-.016-.457-.01-.389c0-.155-.008-.31-.01-.465-.003-.155-.007-.325-.008-.489-.002-.164 0-.326-.005-.489-.004-.164 0-.327 0-.49 0-41.853 33.93-75.784 75.788-75.784 41.856 0 75.786 33.93 75.786 75.784v.979q-.002.245-.008.489c0 .135-.006.268-.01.405q-.002.182-.009.357c0 .153-.009.306-.014.459v.012l-.021.531-.021.466-.005.11-.027.496a81 81 0 0 1-.241 3.184v.013q-.025.261-.053.522l-.045.411-.089.804-.051.407-.063.479q-.034.262-.072.522-.04.292-.084.583l-.069.459-.082.52q-.042.26-.09.517c-.033.173-.059.345-.089.517q-.09.516-.189 1.031-.151.77-.31 1.537l-.11.507q-.055.254-.113.508c-2.133-2.073-4.958-3.202-8.073-3.202-2.827 0-5.734.935-8.643 2.78-1.93 1.226-4.063 3.398-6.26 6.128-2.035-2.822-4.883-4.697-8.139-5.21a12 12 0 0 0-1.878-.148c-7.439 0-11.914 6.453-13.607 12.258-.84 1.961-4.87 10.893-10.932 16.951-9.229 9.198-11.557 18.677-7.059 28.83m78.241-20.409-.03.089a5 5 0 0 1-.263.587q-.112.21-.244.408-.25.374-.549.711c-.046.052-.09.104-.142.155a8 8 0 0 1-.22.227c-1.346 1.334-3.398 2.504-5.718 3.577l-.799.358-.268.119c-.179.079-.358.157-.546.234q-.27.118-.551.232l-.558.23c-1.305.537-2.642 1.049-3.946 1.554l-.558.217-.551.216q-.55.214-1.085.429l-.531.214-.522.213-.256.108-.505.213c-3.837 1.647-6.598 3.322-6.018 5.4q.024.088.054.17.078.23.212.436.078.122.179.228c.682.709 1.923.597 3.488.034q.33-.121.656-.253l.136-.056c.358-.152.737-.322 1.124-.506.097-.046.195-.09.293-.141 1.914-.936 4.083-2.196 6.235-3.343a56 56 0 0 1 2.618-1.325c2.038-.959 3.954-1.639 5.494-1.639.723 0 1.361.148 1.893.488l.089.059c.334.235.614.537.823.887q.061.101.12.211c.761 1.445.124 2.941-1.367 4.408-1.431 1.409-3.657 2.79-6.187 4.068q-.281.143-.567.283c-7.53 3.698-17.391 6.483-17.528 6.518-2.628.681-6.386 1.575-10.62 2.244l-.626.098-.103.015q-.711.108-1.425.201-.725.097-1.462.179l-.09.01a69 69 0 0 1-5.358.406h-.026q-.971.035-1.943.035h-.747a46 46 0 0 1-2.959-.134q-.035.002-.071-.006-.53-.04-1.057-.099a40 40 0 0 1-1.092-.132 25 25 0 0 1-.715-.107 58 58 0 0 1-.725-.121l-.329-.062-.025-.005a26 26 0 0 1-1.036-.219c-.2-.045-.399-.089-.596-.143l-.119-.03q-.146-.037-.29-.076l-.053-.014-.308-.09-.336-.098-.039-.011-.291-.089q-.165-.052-.329-.106l-.268-.089-.197-.069q-.285-.1-.566-.208l-.178-.07-.147-.058a23 23 0 0 1-.845-.358l-.185-.09-.031-.014q-.099-.048-.197-.089a17 17 0 0 1-.384-.191l-.039-.019-.184-.097a16 16 0 0 1-.961-.546l-.172-.106a9 9 0 0 1-.256-.164l-.224-.148-.241-.166-.144-.103q-.228-.162-.447-.335l-.233-.179a15 15 0 0 1-.276-.228q-.115-.095-.227-.195l-.006-.005q-.121-.106-.239-.215a12 12 0 0 1-.232-.216l-.009-.009a9 9 0 0 1-.235-.232q-.117-.117-.231-.236t-.226-.243c-.074-.082-.142-.157-.212-.238l-.023-.027a9 9 0 0 1-.201-.238 12 12 0 0 1-.416-.525q-.203-.27-.396-.551l-.123-.184q-.246-.36-.479-.728a19 19 0 0 1-.339-.536q-.106-.17-.207-.339l-.028-.046q-.098-.164-.191-.327a3 3 0 0 1-.102-.179c-.033-.062-.071-.125-.106-.188l-.057-.099-.035-.064q-.1-.18-.197-.363-.044-.082-.09-.16l-.089-.173-.09-.171q-.338-.675-.638-1.368l-.071-.169q-.067-.17-.135-.336-.034-.08-.063-.161a17 17 0 0 1-.776-2.639q-.017-.083-.031-.163a12 12 0 0 1-.127-.806q-.012-.08-.02-.159l-.017-.162a11 11 0 0 1-.049-.638q-.002-.082-.007-.16a8 8 0 0 1-.008-.318c-.056-4.273 2.106-8.381 6.729-13.002 8.235-8.227 13.029-20.266 13.029-20.266s.129-.505.397-1.232q.055-.151.12-.314.234-.611.527-1.197l.039-.075q.25-.498.544-.971c.046-.073.09-.145.141-.218q.22-.326.465-.634c.089-.111.186-.221.283-.328q.057-.064.118-.124c.477-.493 1.022-.895 1.639-1.109l.078-.026q.078-.025.157-.048.091-.024.185-.043l.029-.006q.195-.039.395-.052h.011c.069 0 .139-.007.21-.007.089 0 .172 0 .259.009q.135.012.269.032c.742.118 1.448.56 2.056 1.242q.347.391.621.836.18.288.34.609c.043.089.084.171.124.259a8 8 0 0 1 .28.691q.293.827.439 1.692.126.758.15 1.526.012.41 0 .829a12 12 0 0 1-.787 3.792q-.065.167-.134.335a9 9 0 0 1-.302.665q-.12.248-.258.496-.09.165-.186.33-.242.41-.522.817l-.113.162a13.6 13.6 0 0 1-1.472 1.728 15.4 15.4 0 0 1-1.699 1.47q-.905.67-1.689 1.477c-1.503 1.577-1.853 2.969-1.515 4.024q.081.25.211.479.152.261.357.48l.053.055.054.054q.08.078.172.153l.06.048q.218.168.46.3c.047.025.089.05.142.074q.261.128.537.217.076.025.154.048l.065.017.09.024.077.019.084.018.083.017.079.013q.087.016.178.027l.057.009.104.01.064.007.105.007h.062l.11.006h.346l.099-.006.114-.007.139-.013.13-.015c.03 0 .06-.008.09-.014.4-.058.792-.164 1.167-.316l.159-.067a5 5 0 0 0 .772-.421q.344-.225.647-.503.072-.066.141-.133.034-.032.067-.067.067-.066.133-.138.482-.53.825-1.157a229 229 0 0 1 6.151-10.514l.294-.471.297-.471q.223-.358.447-.708l.15-.234q.747-1.17 1.519-2.324l.305-.456c.612-.907 1.222-1.789 1.827-2.627l.301-.415a56 56 0 0 1 2.054-2.661l.282-.338c.047-.056.09-.112.141-.166q.14-.165.277-.321c.046-.053.089-.105.138-.157l.268-.302.134-.147q.202-.217.397-.417c.09-.09.173-.179.259-.263a10.5 10.5 0 0 1 1.669-1.386l.14-.09q.201-.136.415-.25c2.364-1.342 4.321-1.441 5.448-.314.682.682 1.06 1.813 1.039 3.387q.001.103-.005.211v.077q-.002.108-.012.217c0 .09-.01.179-.019.269-.009.089-.014.157-.023.237q-.001.033-.008.069-.01.104-.025.211.002.032-.008.065a5 5 0 0 1-.041.283c-.011.09-.026.174-.042.262l-.026.149a4 4 0 0 1-.1.42 6 6 0 0 1-.283.758 11 11 0 0 1-.514.987q-.156.267-.322.526-.172.269-.358.543-.474.678-.985 1.328l-.156.197a51 51 0 0 1-1.722 2.035l-.187.21q-.378.422-.77.848l-.197.214c-.131.143-.268.286-.4.43q-.198.215-.406.433l-.411.433-.417.436-.42.436q-.423.438-.85.876c-4.055 4.159-8.327 8.304-9.773 10.888a5 5 0 0 0-.262.519c-.206.47-.292.872-.233 1.197a.9.9 0 0 0 .111.303q.122.212.295.387.08.078.168.144c.298.212.657.321 1.023.311h.114l.117-.009.117-.013.097-.014q.02-.002.04-.008l.089-.017.023-.005.099-.021.035-.009.104-.028c.035-.01.083-.023.125-.037q.264-.08.517-.188.133-.054.262-.117.068-.03.132-.063l.134-.066q.48-.25.936-.541l.133-.09q.068-.041.133-.089l.133-.089.071-.049.192-.135c.179-.123.346-.25.515-.379l.015-.012.269-.208c.367-.29.715-.582 1.031-.857l.21-.184.019-.018.11-.097c.258-.232.488-.448.679-.626l.079-.077q.103-.099.189-.178l.112-.111.04-.039.011-.011.117-.117.074-.077.009-.007.035-.032.044-.04.014-.013.037-.034.204-.179.114-.102q.091-.08.179-.162l.136-.121q.037-.03.074-.064l.143-.125.21-.185.112-.097c.435-.378.964-.835 1.572-1.35l.249-.211.411-.345.421-.351c.55-.457 1.142-.942 1.768-1.445l.411-.33q.526-.42 1.073-.849.22-.173.448-.344.558-.434 1.123-.859c.858-.648 1.743-1.297 2.639-1.929l.384-.268q.403-.279.805-.552l.243-.164q.72-.488 1.455-.951l.243-.153.241-.15q.363-.225.721-.44l.239-.143.478-.278.469-.269.095-.052.371-.204q.232-.126.463-.244l.229-.118.223-.112.231-.113a22 22 0 0 1 1.954-.845l.41-.144q.185-.062.358-.115l.04-.012q.093-.03.185-.054l.018-.005q.192-.055.38-.099h.009a8 8 0 0 1 1.077-.183q.25-.026.502-.025h.084q.167 0 .327.018c.049 0 .098.01.146.016h.02q.072.008.144.024.071.013.141.028h.015c.047.01.089.022.138.036.254.071.496.181.718.325q.164.107.305.24l.027.026.051.05.049.053c.402.421.734.904.984 1.43l.038.09q.128.3.2.618c.142.646.087 1.32-.158 1.935a4.7 4.7 0 0 1-.339.715 8 8 0 0 1-1.099 1.452l-.089.095q-.199.21-.416.42-.097.094-.198.188l-.205.189-.107.095q-.388.345-.792.669-.244.198-.493.388-.879.672-1.791 1.297a60 60 0 0 1-1.11.743 100 100 0 0 1-2.786 1.763c-1.968 1.21-4.149 2.504-6.474 3.911l-.602.365q-.988.603-1.867 1.152l-.295.185-.558.358q-.555.356-1.108.715l-.297.196q-.217.142-.433.286l-.141.09-.432.291-.229.157-.268.186-.249.173q-.626.442-1.162.843l-.134.102q-.315.239-.623.486-.464.373-.844.716l-.124.113q-.107.098-.208.194-.067.068-.137.133l-.064.064q-.215.216-.418.442l-.066.076q-.22.254-.387.492l-.05.071q-.132.192-.242.396-.028.051-.053.102l-.049.102-.033.074-.021.05-.017.045-.023.061a2.6 2.6 0 0 0-.13.523l-.008.062-.006.058v.315q-.002.038.007.08l.005.048c0 .026.006.051.01.079.004.026.011.073.019.11v.005q.01.052.023.104c.009.035.018.075.029.111q.03.106.068.211.023.063.048.124.002.012.01.025l.036.081.05.111q.081.172.177.336l.066.114.068.114a.4.4 0 0 0 .041.054l.022.023.025.023.026.02a.5.5 0 0 0 .122.063q.035.012.071.02c.577.13 1.763-.347 3.339-1.179.089-.048.187-.098.282-.15l.48-.262.234-.13c.167-.089.337-.19.511-.289l.317-.179c2.083-1.199 4.571-2.739 7.143-4.243q.362-.212.725-.421l.486-.311q.846-.485 1.7-.957a80 80 0 0 1 2.168-1.15l.476-.241q.474-.234.938-.456a41 41 0 0 1 1.815-.809l.335-.136.04-.016c1.775-.703 3.384-1.137 4.686-1.137q.423-.004.84.069h.009c.089.016.17.034.253.054h.015q.32.081.616.23c.275.142.522.331.731.559q.144.16.256.343.21.324.329.692c.032.095.06.188.089.287a3.93 3.93 0 0 1-.06 2.305"/><path fill="#FFD21E" fill-rule="evenodd" d="M203.21 123.685v-.491c0-41.854-33.918-75.783-75.775-75.783-41.856 0-75.787 33.931-75.787 75.783v.164a7 7 0 0 0 0 .327q.008.244.005.489l.005.36.003.129q0 .09.004.179.005.143.005.286l.011.389.016.457.005.15.02.473v.023l.027.553.001.014q.014.282.032.566.014.261.033.522l.002.031.03.448.003.04.033.432.003.028q.008.127.02.249.09 1.082.21 2.161l.004.045.061.529.068.555.05.377.023.177q.08.612.173 1.221l.004.027.059.384q.429 2.745 1.06 5.45l.005.022.032.135.091.385.056-.067c2.2-2.63 5.356-4.08 8.887-4.08 2.83 0 5.733.936 8.643 2.781 1.93 1.226 4.064 3.397 6.26 6.128 2.032-2.822 4.88-4.698 8.138-5.21.621-.098 1.25-.147 1.878-.148 7.436 0 11.915 6.453 13.607 12.258.836 1.961 4.865 10.893 10.941 16.935 9.236 9.227 11.547 18.748 6.976 28.943h.016q.466.053.939.096.28.028.561.052l.066.006.413.035.38.03q.543.042 1.089.077l.289.017.229.014.142.008.447.023.122.005.469.021.522.02.419.012.07.002q.133.006.267.007l.096.003q.885.021 1.769.02h.626l.447-.005q.164-.006.33-.005h.174l.151-.004q.088-.004.178-.004h.179l.351-.009.528-.018.581-.026q.252-.01.5-.023l.179-.011.266-.014.156-.009.118-.008.537-.035.536-.039.487-.039.342-.029.376-.034a62 62 0 0 0 1.88-.197l.427-.051c-4.499-10.152-2.17-19.632 7.027-28.822 6.063-6.058 10.092-14.99 10.932-16.952 1.693-5.804 6.169-12.257 13.607-12.257.629 0 1.258.05 1.879.148 3.255.512 6.103 2.388 8.138 5.21 2.197-2.73 4.33-4.903 6.261-6.129 2.909-1.844 5.815-2.78 8.642-2.78 3.116 0 5.94 1.13 8.073 3.203q.058-.253.114-.508l.109-.506q.059-.278.115-.555.1-.49.195-.984.099-.513.189-1.03l.031-.186.058-.331q.05-.258.09-.518l.011-.066.071-.453.07-.459v-.004q.076-.508.144-1.017l.011-.084.063-.478.051-.408.09-.804.035-.323.009-.088q.029-.261.053-.522v-.014l.039-.416q.107-1.182.179-2.375.013-.195.024-.392v-.006l.026-.491.006-.11q.024-.498.041-.996v-.012l.005-.13q.008-.164.009-.329l.002-.044q.007-.156.008-.314l.003-.09q.005-.157.006-.314l.002-.089q.005-.2.006-.4zm-94.572 75.706c6.002-8.801 5.576-15.407-2.658-23.637-8.236-8.231-13.029-20.267-13.029-20.267s-1.789-6.991-5.869-6.349-7.073 11.089 1.47 17.484c8.542 6.395-1.7 10.731-4.988 4.73-3.288-6.002-12.265-21.429-16.919-24.38s-7.927-1.297-6.83 4.785c.545 3.019 5.613 8.172 10.348 12.986 4.804 4.884 9.265 9.42 8.311 11.025-1.893 3.187-8.56-3.745-8.56-3.745s-20.876-18.998-25.42-14.047c-4.19 4.563 2.271 8.442 12.227 14.421q1.27.762 2.611 1.572c11.391 6.896 12.277 8.715 10.66 11.324-.597.964-4.41-1.325-9.1-4.14-7.995-4.801-18.537-11.13-20.026-5.465-1.288 4.903 6.468 7.907 13.502 10.632 5.86 2.27 11.22 4.346 10.431 7.164-.817 2.922-5.246.485-10.087-2.179-5.435-2.991-11.39-6.267-13.339-2.57-3.683 6.99 25.41 15.219 25.65 15.28 9.4 2.438 33.272 7.604 41.615-4.624m38.665 0c-6.002-8.801-5.576-15.407 2.659-23.637s13.028-20.267 13.028-20.267 1.789-6.991 5.869-6.349 7.073 11.089-1.469 17.484 1.699 10.731 4.987 4.73c3.289-6.002 12.26-21.429 16.914-24.38s7.929-1.297 6.831 4.785c-.544 3.019-5.613 8.172-10.348 12.987-4.804 4.884-9.265 9.419-8.312 11.024 1.893 3.187 8.565-3.749 8.565-3.749s20.875-18.997 25.421-14.046c4.189 4.562-2.272 8.442-12.229 14.421q-1.306.784-2.61 1.572c-11.391 6.896-12.277 8.715-10.661 11.323.598.965 4.411-1.325 9.1-4.14 7.996-4.8 18.538-11.13 20.027-5.464 1.289 4.903-6.468 7.907-13.502 10.632-5.86 2.27-11.22 4.346-10.432 7.164.816 2.921 5.244.484 10.084-2.18 5.435-2.991 11.391-6.269 13.339-2.569 3.684 6.994-25.414 15.215-25.649 15.275-9.4 2.446-33.272 7.612-41.612-4.616" clip-rule="evenodd"/><path fill="#32343D" fill-rule="evenodd" d="M152.047 102.567c1.182.418 2.061 1.69 2.897 2.901 1.13 1.636 2.182 3.159 3.796 2.301a10.91 10.91 0 0 0 4.247-15.214 10.91 10.91 0 0 0-7.742-5.198 10.904 10.904 0 0 0-11.689 6.589 10.9 10.9 0 0 0 .436 9.314c.748 1.407 2.408.743 4.16.042 1.373-.549 2.804-1.121 3.895-.735m-51.375 0c-1.182.418-2.061 1.691-2.897 2.901-1.13 1.637-2.183 3.159-3.796 2.301a10.903 10.903 0 0 1 8.263-20.068 10.91 10.91 0 0 1 7.707 9.348 10.9 10.9 0 0 1-1.221 6.211c-.749 1.407-2.409.743-4.161.043-1.374-.55-2.803-1.122-3.895-.736m43.427 46.751c8.143-6.415 11.134-16.889 11.134-23.341 0-5.1-3.431-3.495-8.924-.775l-.31.153c-5.042 2.497-11.754 5.822-19.122 5.822s-14.081-3.325-19.122-5.823c-5.671-2.809-9.228-4.571-9.228.624 0 6.656 3.182 17.585 11.916 23.934a18.97 18.97 0 0 1 11.575-9.786c.872-.26 1.77 1.241 2.689 2.778.887 1.482 1.794 2.998 2.716 2.998.983 0 1.948-1.494 2.891-2.952.985-1.525 1.946-3.01 2.875-2.713a18.97 18.97 0 0 1 10.91 9.081" clip-rule="evenodd"/><path fill="#FF323D" d="M144.097 149.317c-4.241 3.342-9.878 5.583-17.219 5.583-6.897 0-12.291-1.978-16.435-4.989a18.97 18.97 0 0 1 11.575-9.786c1.712-.511 3.527 5.776 5.405 5.776 2.01 0 3.947-6.246 5.766-5.665a18.97 18.97 0 0 1 10.908 9.081"/><path fill="#FFAD03" fill-rule="evenodd" d="M81.2 111.64a7.08 7.08 0 0 1-6.65.655 7.06 7.06 0 0 1-3.837-3.837 7.08 7.08 0 0 1 .657-6.65 7.087 7.087 0 1 1 9.83 9.832m101.413 0a7.08 7.08 0 0 1-6.651.655 7.06 7.06 0 0 1-3.837-3.837 7.1 7.1 0 0 1-.504-3.407 7.1 7.1 0 0 1 3.411-5.385 7.08 7.08 0 0 1 8.656 1.07 7.08 7.08 0 0 1 1.536 7.724 7.1 7.1 0 0 1-2.611 3.18" clip-rule="evenodd"/></svg>)\n\n[Use an open-source LLM (via HuggingFace)](https://n8n.io/workflows/1980-use-an-open-source-llm-via-huggingface/)\n\n35022 ‚Ä¢ **by n8n-team** ‚Ä¢ 1 year \n\n[ Use this workflow ](https://n8n.io/workflows/1980-use-an-open-source-llm-via-huggingface/)\n\n[ Get started ](https://app.n8n.cloud/register)\n\n## Subscribe to n8n newsletter\n\nGet the best, coolest, and latest in automation and low-code delivered to your inbox each week. \n\nSomething went wrong. Please try again later.\n\nSubscribed!\n\nSubscribe\n\n[](https://twitter.com/intent/tweet?text=The%2011%20best%20open-source%20LLMs%20for%202025&url=https://blog.n8n.io/open-source-llm/) [](https://www.facebook.com/sharer/sharer.php?u=https://blog.n8n.io/open-source-llm/)\n",
  "crawled_at": "2025-05-28T10:47:05.732003"
}