{
  "url": "https://n8n.io/workflows/2467-narrating-over-a-video-using-multimodal-ai/",
  "title": "Narrating over a Video using Multimodal AI",
  "author": "Jimleuk",
  "publish_date": "Last update 7 months ago",
  "publish_date_absolute": "2024-10-25",
  "categories": [
    {
      "name": "Building Blocks"
    },
    {
      "name": "AI"
    }
  ],
  "workflow_json": "{\"meta\":{\"instanceId\":\"408f9fb9940c3cb18ffdef0e0150fe342d6e655c3a9fac21f0f644e8bedabcd9\"},\"nodes\":[{\"id\":\"6d16b5be-8f7b-49f2-8523-9b84c62f2759\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[1960,660],\"parameters\":{\"model\":\"gpt-4o-2024-08-06\",\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"8gccIjcuf3gvaoEr\",\"name\":\"OpenAi account\"}},\"typeVersion\":1},{\"id\":\"a6084f09-9a4f-478a-ac1a-ab1413628c1f\",\"name\":\"Capture Frames\",\"type\":\"n8n-nodes-base.code\",\"position\":[720,460],\"parameters\":{\"mode\":\"runOnceForEachItem\",\"language\":\"python\",\"pythonCode\":\"import cv2\\nimport numpy as np\\nimport base64\\n\\ndef extract_evenly_distributed_frames_from_base64(base64_string, max_frames=90):\\n    # Decode the Base64 string into bytes\\n    video_bytes = base64.b64decode(base64_string)\\n    \\n    # Write the bytes to a temporary file\\n    video_path = '/tmp/temp_video.mp4'\\n    with open(video_path, 'wb') as video_file:\\n        video_file.write(video_bytes)\\n    \\n    # Open the video file using OpenCV\\n    video_capture = cv2.VideoCapture(video_path)\\n    \\n    # Get the total number of frames in the video\\n    total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\\n    \\n    # Calculate the step size to take 'max_frames' evenly distributed frames\\n    step_size = max(1, total_frames // (max_frames - 1))\\n    \\n    # List to store selected frames as base64\\n    selected_frames_base64 = []\\n    \\n    for i in range(0, total_frames, step_size):\\n        # Set the current frame position\\n        video_capture.set(cv2.CAP_PROP_POS_FRAMES, i)\\n        \\n        # Read the frame\\n        ret, frame = video_capture.read()\\n        if ret:\\n            # Convert frame (NumPy array) to a Base64 string\\n            frame_base64 = convert_frame_to_base64(frame)\\n            selected_frames_base64.append(frame_base64)\\n        if len(selected_frames_base64) >= max_frames:\\n            break\\n    \\n    # Release the video capture object\\n    video_capture.release()\\n\\n    return selected_frames_base64\\n\\ndef convert_frame_to_base64(frame):\\n    # Convert the frame (NumPy array) to JPEG format\\n    ret, buffer = cv2.imencode('.jpg', frame)\\n    if not ret:\\n        return None\\n\\n    # Encode JPEG image to Base64\\n    frame_base64 = base64.b64encode(buffer).decode('utf-8')\\n    return frame_base64\\n\\nbase64_video = _input.item.binary.data.data\\nframes_base64 = extract_evenly_distributed_frames_from_base64(base64_video, max_frames=90)\\n\\nreturn { \\\"output\\\": frames_base64 }\"},\"typeVersion\":2},{\"id\":\"b45e82a4-f304-4733-a9cf-07cae6df13ea\",\"name\":\"Split Out Frames\",\"type\":\"n8n-nodes-base.splitOut\",\"position\":[920,460],\"parameters\":{\"options\":{},\"fieldToSplitOut\":\"output\"},\"typeVersion\":1},{\"id\":\"83d29c51-a415-476d-b380-1ca5f0d4f521\",\"name\":\"Download Video\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[329,346],\"parameters\":{\"url\":\"=https://cdn.pixabay.com/video/2016/05/12/3175-166339863_small.mp4\",\"options\":{}},\"typeVersion\":4.2},{\"id\":\"0304ebb5-945d-4b0b-9597-f83ae8c1fe31\",\"name\":\"Convert to Binary\",\"type\":\"n8n-nodes-base.convertToFile\",\"position\":[1480,500],\"parameters\":{\"options\":{},\"operation\":\"toBinary\",\"sourceProperty\":\"output\"},\"typeVersion\":1.1},{\"id\":\"32a21e1d-1d8b-411e-8281-8d0e68a06889\",\"name\":\"When clicking ‘Test workflow’\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[149,346],\"parameters\":{},\"typeVersion\":1},{\"id\":\"0ad2ea6a-e1f4-4b26-a4de-9103ecbb3831\",\"name\":\"Combine Script\",\"type\":\"n8n-nodes-base.aggregate\",\"position\":[2640,360],\"parameters\":{\"options\":{},\"aggregate\":\"aggregateAllItemData\"},\"typeVersion\":1},{\"id\":\"2d9bb91a-3369-4268-882f-f97e73897bb8\",\"name\":\"Upload to GDrive\",\"type\":\"n8n-nodes-base.googleDrive\",\"position\":[3040,360],\"parameters\":{\"name\":\"=narrating-video-using-vision-ai-{{ $now.format('yyyyMMddHHmmss') }}.mp3\",\"driveId\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"My Drive\",\"cachedResultUrl\":\"https://drive.google.com/drive/my-drive\",\"cachedResultName\":\"My Drive\"},\"options\":{},\"folderId\":{\"__rl\":true,\"mode\":\"id\",\"value\":\"1dBJZL_SCh6F2U7N7kIMsnSiI4QFxn2xD\"}},\"credentials\":{\"googleDriveOAuth2Api\":{\"id\":\"yOwz41gMQclOadgu\",\"name\":\"Google Drive account\"}},\"typeVersion\":3},{\"id\":\"137185f6-ba32-4c68-844f-f50c7a5a261d\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-440,0],\"parameters\":{\"width\":476.34074202271484,\"height\":586.0597334122469,\"content\":\"## Try It Out!\\n\\n### This n8n template takes a video and extracts frames from it which are used with a multimodal LLM to generate a script. The script is then passed to the same multimodal LLM to generate a voiceover clip.\\n\\nThis template was inspired by [Processing and narrating a video with GPT's visual capabilities and the TTS API](https://cookbook.openai.com/examples/gpt_with_vision_for_video_understanding)\\n\\n* Video is downloaded using the HTTP node.\\n* Python code node is used to extract the frames using OpenCV.\\n* Loop node is used o batch the frames for the LLM to generate partial scripts.\\n* All partial scripts are combined to form the full script which is then sent to OpenAI to generate audio from it.\\n* The finished voiceover clip is uploaded to Google Drive.\\n\\nSample the finished product here: https://drive.google.com/file/d/1-XCoii0leGB2MffBMPpCZoxboVyeyeIX/view?usp=sharing\\n\\n\\n### Need Help?\\nJoin the [Discord](https://discord.com/invite/XPKeKXeB7d) or ask in the [Forum](https://community.n8n.io/)!\"},\"typeVersion\":1},{\"id\":\"23700b04-2549-4121-b442-4b92adf7f6d6\",\"name\":\"Sticky Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[60,120],\"parameters\":{\"color\":7,\"width\":459.41860465116287,\"height\":463.313953488372,\"content\":\"## 1. Download Video\\n[Learn more about the HTTP Request node](https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.httprequest/)\\n\\nIn this demonstration, we'll download a stock video from pixabay using the HTTP Request node. Feel free to use other sources but ensure they are in a format support by OpenCV ([See docs](https://docs.opencv.org/3.4/dd/d43/tutorial_py_video_display.html))\"},\"typeVersion\":1},{\"id\":\"0a42aeb0-96cd-401c-abeb-c50e0f04f7ad\",\"name\":\"Sticky Note2\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[560,120],\"parameters\":{\"color\":7,\"width\":605.2674418604653,\"height\":522.6860465116279,\"content\":\"## 2. Split Video into Frames\\n[Learn more about the Code node](https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.code/)\\n\\nWe need to think of videos are a sum of 2 parts; a visual track and an audio track. The visual track is technically just a collection of images displayed one after the other and are typically referred to as frames. When we want LLM to understand videos, most of the time we can do so by giving it a series of frames as images to process.\\n\\nHere, we use the Python Code node to extract the frames from the video using OpenCV, a computer vision library. For performance reasons, we'll also capture only a max of 90 frames from the video but ensure they are evenly distributed across the video. This step takes about 1-2 mins to complete on a 3mb video.\"},\"typeVersion\":1},{\"id\":\"b518461c-13f1-45ae-a156-20ae6051fc19\",\"name\":\"Sticky Note3\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[560,660],\"parameters\":{\"color\":3,\"width\":418.11627906976724,\"height\":132.89534883720933,\"content\":\"### 🚨 PERFORMANCE WARNING!\\nUsing large videos or capturing a large number of frames is really memory intensive and could crash your n8n instance. Be sure you have sufficient memory and to optimise the video beforehand! \"},\"typeVersion\":1},{\"id\":\"585f7a7f-1676-4bc3-a6fb-eace443aa5da\",\"name\":\"Sticky Note4\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1200,118.69767441860472],\"parameters\":{\"color\":7,\"width\":1264.8139534883715,\"height\":774.3720930232558,\"content\":\"## 3. Use Vision AI to Narrate on Batches of Frames\\n[Read more about the Basic LLM node](https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.chainllm/)\\n\\nTo keep within token limits of our LLM, we'll need to send our frames in sequential batches to represent chunks of our original video. We'll use the loop node to create batches of 15 frames - this is because of our max of 90 frames, this fits perfectly for a total of 6 loops. Next, we'll convert each frame to a binary image so we can resize for and attach to the Basic LLM node. One trick to point out is that within the Basic LLM node, previous iterations of the generation are prepended to form a cohesive script. Without, the LLM will assume it needs to start fresh for each batch of frames.\\n\\nA wait node is used to stay within service rate limits. This is useful for new users who are still on lower tiers. If you do not have such restrictions, feel free to remove this wait node!\"},\"typeVersion\":1},{\"id\":\"42c002a3-37f6-4dd7-af14-20391b19cb5a\",\"name\":\"Stay Within Service Limits\",\"type\":\"n8n-nodes-base.wait\",\"position\":[2280,640],\"webhookId\":\"677fa706-b4dd-4fe3-ba17-feea944c3193\",\"parameters\":{},\"typeVersion\":1.1},{\"id\":\"5beb17fa-8a57-4c72-9c3b-b7fdf41b545a\",\"name\":\"For Every 15 Frames\",\"type\":\"n8n-nodes-base.splitInBatches\",\"position\":[1320,380],\"parameters\":{\"options\":{},\"batchSize\":15},\"typeVersion\":3},{\"id\":\"9a57256a-076a-4823-8cad-3b64a17ff705\",\"name\":\"Resize Frame\",\"type\":\"n8n-nodes-base.editImage\",\"position\":[1640,500],\"parameters\":{\"width\":768,\"height\":768,\"options\":{\"format\":\"jpeg\"},\"operation\":\"resize\"},\"typeVersion\":1},{\"id\":\"3e776939-1a25-4ea0-8106-c3072d108106\",\"name\":\"Aggregate Frames\",\"type\":\"n8n-nodes-base.aggregate\",\"position\":[1800,500],\"parameters\":{\"options\":{\"includeBinaries\":true},\"aggregate\":\"aggregateAllItemData\"},\"typeVersion\":1},{\"id\":\"3a973a9c-2c7a-43c5-9c45-a14d49b56622\",\"name\":\"Sticky Note5\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[2500,120.6860465116277],\"parameters\":{\"color\":7,\"width\":769.1860465116274,\"height\":487.83720930232533,\"content\":\"## 4. Generate Voice Over Clip Using TTS\\n[Read more about the OpenAI node](https://docs.n8n.io/integrations/builtin/app-nodes/n8n-nodes-langchain.openai)\\n\\nFinally with our generated script parts, we can combine them into one and use OpenAI's Audio generation capabilities to generate a voice over from the full script. Once we have the output mp3, we can upload it to somewhere like Google Drive for later use.\\n\\nHave a listen to the finished product here: https://drive.google.com/file/d/1-XCoii0leGB2MffBMPpCZoxboVyeyeIX/view?usp=sharing\"},\"typeVersion\":1},{\"id\":\"92e07c18-4058-4098-a448-13451bd8a17a\",\"name\":\"Use Text-to-Speech\",\"type\":\"@n8n/n8n-nodes-langchain.openAi\",\"position\":[2840,360],\"parameters\":{\"input\":\"={{ $json.data.map(item => item.text).join('\\\\n') }}\",\"options\":{\"response_format\":\"mp3\"},\"resource\":\"audio\"},\"credentials\":{\"openAiApi\":{\"id\":\"8gccIjcuf3gvaoEr\",\"name\":\"OpenAi account\"}},\"typeVersion\":1.5},{\"id\":\"0696c336-1814-4ad4-aa5e-b86489a4231e\",\"name\":\"Sticky Note6\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[61,598],\"parameters\":{\"color\":7,\"width\":458.1279069767452,\"height\":296.8139534883723,\"content\":\"**The video used in this demonstration is**\\n&copy; [Coverr-Free-Footage](https://pixabay.com/users/coverr-free-footage-1281706/) via [Pixabay](https://pixabay.com/videos/india-street-busy-rickshaw-people-3175/)\\n![](https://res.cloudinary.com/daglih2g8/image/upload/f_auto,q_auto/v1/n8n-workflows/jhx2tma2gxaabkeiqlgp#full-width)\"},\"typeVersion\":1},{\"id\":\"81185ac4-c7fd-4921-937f-109662d5dfa5\",\"name\":\"Generate Narration Script\",\"type\":\"@n8n/n8n-nodes-langchain.chainLlm\",\"position\":[1960,500],\"parameters\":{\"text\":\"=These are frames of a video. Create a short voiceover script in the style of David Attenborough. Only include the narration.\\n{{\\n$('Generate Narration Script').isExecuted\\n ? `Continue from this script:\\\\n${$('Generate Narration Script').all().map(item => item.json.text.replace(/\\\\n/g,'')).join('\\\\n')}`\\n : ''\\n}}\",\"messages\":{\"messageValues\":[{\"type\":\"HumanMessagePromptTemplate\",\"messageType\":\"imageBinary\"},{\"type\":\"HumanMessagePromptTemplate\",\"messageType\":\"imageBinary\",\"binaryImageDataKey\":\"data_1\"},{\"type\":\"HumanMessagePromptTemplate\",\"messageType\":\"imageBinary\",\"binaryImageDataKey\":\"data_2\"},{\"type\":\"HumanMessagePromptTemplate\",\"messageType\":\"imageBinary\",\"binaryImageDataKey\":\"data_3\"},{\"type\":\"HumanMessagePromptTemplate\",\"messageType\":\"imageBinary\",\"binaryImageDataKey\":\"data_4\"},{\"type\":\"HumanMessagePromptTemplate\",\"messageType\":\"imageBinary\",\"binaryImageDataKey\":\"data_5\"},{\"type\":\"HumanMessagePromptTemplate\",\"messageType\":\"imageBinary\",\"binaryImageDataKey\":\"data_6\"},{\"type\":\"HumanMessagePromptTemplate\",\"messageType\":\"imageBinary\",\"binaryImageDataKey\":\"data_7\"},{\"type\":\"HumanMessagePromptTemplate\",\"messageType\":\"imageBinary\",\"binaryImageDataKey\":\"data_8\"},{\"type\":\"HumanMessagePromptTemplate\",\"messageType\":\"imageBinary\",\"binaryImageDataKey\":\"data_9\"},{\"type\":\"HumanMessagePromptTemplate\",\"messageType\":\"imageBinary\",\"binaryImageDataKey\":\"data_10\"},{\"type\":\"HumanMessagePromptTemplate\",\"messageType\":\"imageBinary\",\"binaryImageDataKey\":\"data_11\"},{\"type\":\"HumanMessagePromptTemplate\",\"messageType\":\"imageBinary\",\"binaryImageDataKey\":\"data_12\"},{\"type\":\"HumanMessagePromptTemplate\",\"messageType\":\"imageBinary\",\"binaryImageDataKey\":\"data_13\"},{\"type\":\"HumanMessagePromptTemplate\",\"messageType\":\"imageBinary\",\"binaryImageDataKey\":\"data_14\"}]},\"promptType\":\"define\"},\"typeVersion\":1.4}],\"pinData\":{},\"connections\":{\"Resize Frame\":{\"main\":[[{\"node\":\"Aggregate Frames\",\"type\":\"main\",\"index\":0}]]},\"Capture Frames\":{\"main\":[[{\"node\":\"Split Out Frames\",\"type\":\"main\",\"index\":0}]]},\"Combine Script\":{\"main\":[[{\"node\":\"Use Text-to-Speech\",\"type\":\"main\",\"index\":0}]]},\"Download Video\":{\"main\":[[{\"node\":\"Capture Frames\",\"type\":\"main\",\"index\":0}]]},\"Aggregate Frames\":{\"main\":[[{\"node\":\"Generate Narration Script\",\"type\":\"main\",\"index\":0}]]},\"Split Out Frames\":{\"main\":[[{\"node\":\"For Every 15 Frames\",\"type\":\"main\",\"index\":0}]]},\"Convert to Binary\":{\"main\":[[{\"node\":\"Resize Frame\",\"type\":\"main\",\"index\":0}]]},\"OpenAI Chat Model\":{\"ai_languageModel\":[[{\"node\":\"Generate Narration Script\",\"type\":\"ai_languageModel\",\"index\":0}]]},\"Use Text-to-Speech\":{\"main\":[[{\"node\":\"Upload to GDrive\",\"type\":\"main\",\"index\":0}]]},\"For Every 15 Frames\":{\"main\":[[{\"node\":\"Combine Script\",\"type\":\"main\",\"index\":0}],[{\"node\":\"Convert to Binary\",\"type\":\"main\",\"index\":0}]]},\"Generate Narration Script\":{\"main\":[[{\"node\":\"Stay Within Service Limits\",\"type\":\"main\",\"index\":0}]]},\"Stay Within Service Limits\":{\"main\":[[{\"node\":\"For Every 15 Frames\",\"type\":\"main\",\"index\":0}]]},\"When clicking ‘Test workflow’\":{\"main\":[[{\"node\":\"Download Video\",\"type\":\"main\",\"index\":0}]]}}}",
  "readme": "This n8n template takes a video and extracts frames from it which are used with a multimodal LLM to generate a script. The script is then passed to the same multimodal LLM to generate a voiceover clip.\n\nThis template was inspired by [Processing and narrating a video with GPT's visual capabilities and the TTS API](https://cookbook.openai.com/examples/gpt_with_vision_for_video_understanding)\n\n## How it works\n\n  * Video is downloaded using the HTTP node.\n  * Python code node is used to extract the frames using OpenCV.\n  * Loop node is used o batch the frames for the LLM to generate partial scripts.\n  * All partial scripts are combined to form the full script which is then sent to OpenAI to generate audio from it.\n  * The finished voiceover clip is uploaded to Google Drive.\n\n\n\nSample the finished product here: <https://drive.google.com/file/d/1-XCoii0leGB2MffBMPpCZoxboVyeyeIX/view?usp=sharing>\n\n## Requirements\n\n  * OpenAI for LLM\n  * Ideally, a mid-range (16GB RAM) machine for acceptable performance!\n\n\n\n## Customising this workflow\n\n  * For larger videos, consider splitting into smaller clips for better performance\n  * Use a multimodal LLM which supports fully video such as Google's Gemini.\n\n\n",
  "crawled_at": "2025-05-25T23:59:23.027446",
  "readme_zh": "该n8n模板通过提取视频帧，结合多模态大语言模型生成解说脚本，随后将脚本输入同一模型合成语音旁白。\n\n本模板灵感源自[《运用GPT视觉能力与TTS API处理视频并生成解说》](https://cookbook.openai.com/examples/gpt_with_vision_for_video_understanding)\n\n## 工作原理\n  * 通过HTTP节点下载视频文件\n  * 使用Python代码节点调用OpenCV提取视频帧\n  * 通过循环节点分批处理视频帧，由大语言模型生成分段脚本\n  * 整合所有分段脚本后，提交至OpenAI生成语音文件\n  * 最终旁白文件自动上传至Google云端硬盘\n\n成品示例：<https://drive.google.com/file/d/1-XCoii0leGB2MffBMPpCZoxboVyeyeIX/view?usp=sharing>\n\n## 环境要求\n  * 需配置OpenAI大语言模型服务\n  * 建议使用中端配置设备（16GB内存）以保证运行效率\n\n## 定制建议\n  * 处理长视频时可分段处理提升性能\n  * 推荐选用支持完整视频处理的多模态模型（如Google Gemini）",
  "title_zh": "利用多模态AI为视频配音",
  "publish_date_zh": "最后更新于6个月前",
  "workflow_json_zh": "{\n  \"meta\": {\n    \"instanceId\": \"408f9fb9940c3cb18ffdef0e0150fe342d6e655c3a9fac21f0f644e8bedabcd9\"\n  },\n  \"nodes\": [\n    {\n      \"id\": \"6d16b5be-8f7b-49f2-8523-9b84c62f2759\",\n      \"name\": \"OpenAI Chat Model\",\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\n      \"position\": [\n        1960,\n        660\n      ],\n      \"parameters\": {\n        \"model\": \"gpt-4o-2024-08-06\",\n        \"options\": {}\n      },\n      \"credentials\": {\n        \"openAiApi\": {\n          \"id\": \"8gccIjcuf3gvaoEr\",\n          \"name\": \"OpenAi account\"\n        }\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"a6084f09-9a4f-478a-ac1a-ab1413628c1f\",\n      \"name\": \"Capture Frames\",\n      \"type\": \"n8n-nodes-base.code\",\n      \"position\": [\n        720,\n        460\n      ],\n      \"parameters\": {\n        \"mode\": \"runOnceForEachItem\",\n        \"language\": \"python\",\n        \"pythonCode\": \"import cv2\\nimport numpy as np\\nimport base64\\n\\ndef extract_evenly_distributed_frames_from_base64(base64_string, max_frames=90):\\n    # Decode the Base64 string into bytes\\n    video_bytes = base64.b64decode(base64_string)\\n    \\n    # Write the bytes to a temporary file\\n    video_path = '/tmp/temp_video.mp4'\\n    with open(video_path, 'wb') as video_file:\\n        video_file.write(video_bytes)\\n    \\n    # Open the video file using OpenCV\\n    video_capture = cv2.VideoCapture(video_path)\\n    \\n    # Get the total number of frames in the video\\n    total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\\n    \\n    # Calculate the step size to take 'max_frames' evenly distributed frames\\n    step_size = max(1, total_frames // (max_frames - 1))\\n    \\n    # List to store selected frames as base64\\n    selected_frames_base64 = []\\n    \\n    for i in range(0, total_frames, step_size):\\n        # Set the current frame position\\n        video_capture.set(cv2.CAP_PROP_POS_FRAMES, i)\\n        \\n        # Read the frame\\n        ret, frame = video_capture.read()\\n        if ret:\\n            # Convert frame (NumPy array) to a Base64 string\\n            frame_base64 = convert_frame_to_base64(frame)\\n            selected_frames_base64.append(frame_base64)\\n        if len(selected_frames_base64) >= max_frames:\\n            break\\n    \\n    # Release the video capture object\\n    video_capture.release()\\n\\n    return selected_frames_base64\\n\\ndef convert_frame_to_base64(frame):\\n    # Convert the frame (NumPy array) to JPEG format\\n    ret, buffer = cv2.imencode('.jpg', frame)\\n    if not ret:\\n        return None\\n\\n    # Encode JPEG image to Base64\\n    frame_base64 = base64.b64encode(buffer).decode('utf-8')\\n    return frame_base64\\n\\nbase64_video = _input.item.binary.data.data\\nframes_base64 = extract_evenly_distributed_frames_from_base64(base64_video, max_frames=90)\\n\\nreturn { \\\"output\\\": frames_base64 }\"\n      },\n      \"typeVersion\": 2\n    },\n    {\n      \"id\": \"b45e82a4-f304-4733-a9cf-07cae6df13ea\",\n      \"name\": \"Split Out Frames\",\n      \"type\": \"n8n-nodes-base.splitOut\",\n      \"position\": [\n        920,\n        460\n      ],\n      \"parameters\": {\n        \"options\": {},\n        \"fieldToSplitOut\": \"output\"\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"83d29c51-a415-476d-b380-1ca5f0d4f521\",\n      \"name\": \"Download Video\",\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"position\": [\n        329,\n        346\n      ],\n      \"parameters\": {\n        \"url\": \"=https://cdn.pixabay.com/video/2016/05/12/3175-166339863_small.mp4\",\n        \"options\": {}\n      },\n      \"typeVersion\": 4.2\n    },\n    {\n      \"id\": \"0304ebb5-945d-4b0b-9597-f83ae8c1fe31\",\n      \"name\": \"Convert to Binary\",\n      \"type\": \"n8n-nodes-base.convertToFile\",\n      \"position\": [\n        1480,\n        500\n      ],\n      \"parameters\": {\n        \"options\": {},\n        \"operation\": \"toBinary\",\n        \"sourceProperty\": \"output\"\n      },\n      \"typeVersion\": 1.1\n    },\n    {\n      \"id\": \"32a21e1d-1d8b-411e-8281-8d0e68a06889\",\n      \"name\": \"When clicking ‘Test workflow’\",\n      \"type\": \"n8n-nodes-base.manualTrigger\",\n      \"position\": [\n        149,\n        346\n      ],\n      \"parameters\": {},\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"0ad2ea6a-e1f4-4b26-a4de-9103ecbb3831\",\n      \"name\": \"Combine Script\",\n      \"type\": \"n8n-nodes-base.aggregate\",\n      \"position\": [\n        2640,\n        360\n      ],\n      \"parameters\": {\n        \"options\": {},\n        \"aggregate\": \"aggregateAllItemData\"\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"2d9bb91a-3369-4268-882f-f97e73897bb8\",\n      \"name\": \"Upload to GDrive\",\n      \"type\": \"n8n-nodes-base.googleDrive\",\n      \"position\": [\n        3040,\n        360\n      ],\n      \"parameters\": {\n        \"name\": \"=narrating-video-using-vision-ai-{{ $now.format('yyyyMMddHHmmss') }}.mp3\",\n        \"driveId\": {\n          \"__rl\": true,\n          \"mode\": \"list\",\n          \"value\": \"My Drive\",\n          \"cachedResultUrl\": \"https://drive.google.com/drive/my-drive\",\n          \"cachedResultName\": \"My Drive\"\n        },\n        \"options\": {},\n        \"folderId\": {\n          \"__rl\": true,\n          \"mode\": \"id\",\n          \"value\": \"1dBJZL_SCh6F2U7N7kIMsnSiI4QFxn2xD\"\n        }\n      },\n      \"credentials\": {\n        \"googleDriveOAuth2Api\": {\n          \"id\": \"yOwz41gMQclOadgu\",\n          \"name\": \"Google Drive account\"\n        }\n      },\n      \"typeVersion\": 3\n    },\n    {\n      \"id\": \"137185f6-ba32-4c68-844f-f50c7a5a261d\",\n      \"name\": \"Sticky Note\",\n      \"type\": \"n8n-nodes-base.stickyNote\",\n      \"position\": [\n        -440,\n        0\n      ],\n      \"parameters\": {\n        \"width\": 476.34074202271484,\n        \"height\": 586.0597334122469,\n        \"content\": \"## 试试看吧！\\n\\n### 这个n8n模板会提取视频中的画面帧，通过多模态大语言模型生成解说脚本，再将脚本交由同一模型转换为语音旁白。\\n\\n本模板灵感来源于[《利用GPT视觉能力与TTS API处理并解说视频》](https://cookbook.openai.com/examples/gpt_with_vision_for_video_understanding)\\n\\n* 使用HTTP节点下载视频\\n* 通过Python代码节点调用OpenCV提取画面帧\\n* 利用循环节点分批处理画面帧，由大语言模型生成分段脚本\\n* 整合所有分段脚本后，交由OpenAI生成语音文件\\n* 最终成品将上传至Google云端硬盘\\n\\n点击试听成品样例：https://drive.google.com/file/d/1-XCoii0leGB2MffBMPpCZoxboVyeyeIX/view?usp=sharing\\n\\n\\n### 需要帮助？\\n加入[Discord讨论群](https://discord.com/invite/XPKeKXeB7d) 或访问[官方论坛](https://community.n8n.io/)提问！\"\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"23700b04-2549-4121-b442-4b92adf7f6d6\",\n      \"name\": \"Sticky Note1\",\n      \"type\": \"n8n-nodes-base.stickyNote\",\n      \"position\": [\n        60,\n        120\n      ],\n      \"parameters\": {\n        \"color\": 7,\n        \"width\": 459.41860465116287,\n        \"height\": 463.313953488372,\n        \"content\": \"## 1. 下载视频\\n[了解HTTP请求节点的更多信息](https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.httprequest/)\\n\\n本次演示中，我们将使用HTTP请求节点从pixabay下载一个示例视频。您也可以选择其他视频来源，但请确保其格式受OpenCV支持（[查阅文档](https://docs.opencv.org/3.4/dd/d43/tutorial_py_video_display.html)）\"\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"0a42aeb0-96cd-401c-abeb-c50e0f04f7ad\",\n      \"name\": \"Sticky Note2\",\n      \"type\": \"n8n-nodes-base.stickyNote\",\n      \"position\": [\n        560,\n        120\n      ],\n      \"parameters\": {\n        \"color\": 7,\n        \"width\": 605.2674418604653,\n        \"height\": 522.6860465116279,\n        \"content\": \"## 2. 将视频拆分为帧\\n[了解更多关于代码节点的信息](https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.code/)\\n\\n我们需要将视频视为由两部分组成：视觉轨道和音频轨道。从技术上讲，视觉轨道只是一系列连续显示的图像，通常被称为帧。当我们希望大语言模型（LLM）理解视频内容时，大多数情况下可以通过提供一系列帧图像供其处理来实现。\\n\\n在此步骤中，我们使用Python代码节点结合OpenCV（一个计算机视觉库）来提取视频中的帧。出于性能考虑，我们仅从视频中捕获最多90帧，并确保这些帧均匀分布在整段视频中。对于一个3MB大小的视频，此步骤大约需要1-2分钟完成。\"\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"b518461c-13f1-45ae-a156-20ae6051fc19\",\n      \"name\": \"Sticky Note3\",\n      \"type\": \"n8n-nodes-base.stickyNote\",\n      \"position\": [\n        560,\n        660\n      ],\n      \"parameters\": {\n        \"color\": 3,\n        \"width\": 418.11627906976724,\n        \"height\": 132.89534883720933,\n        \"content\": \"### 🚨 性能警告！\\n使用大型视频或捕获大量帧会极大消耗内存，可能导致您的n8n实例崩溃。请确保内存充足，并提前对视频进行优化处理！\"\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"585f7a7f-1676-4bc3-a6fb-eace443aa5da\",\n      \"name\": \"Sticky Note4\",\n      \"type\": \"n8n-nodes-base.stickyNote\",\n      \"position\": [\n        1200,\n        118.69767441860472\n      ],\n      \"parameters\": {\n        \"color\": 7,\n        \"width\": 1264.8139534883715,\n        \"height\": 774.3720930232558,\n        \"content\": \"## 3. 使用Vision AI对批量帧进行旁白生成  \\n[了解更多关于基础LLM节点的信息](https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.chainllm/)  \\n\\n为了保持在LLM的token限制范围内，我们需要将视频帧分批顺序发送，以代表原始视频的片段。我们将使用循环节点创建每批15帧的批次——这是因为我们最多处理90帧，正好可以完美分为6次循环。接下来，我们将每帧转换为二进制图像，以便调整大小并附加到基础LLM节点。需要指出的一点技巧是，在基础LLM节点内部，之前生成的迭代内容会被前置，以形成连贯的脚本。如果不这样做，LLM会认为每批帧都需要从头开始生成。  \\n\\n我们还使用了等待节点来遵守服务的速率限制。这对于仍处于较低使用层级的新用户非常有用。如果你没有此类限制，可以放心移除这个等待节点！\"\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"42c002a3-37f6-4dd7-af14-20391b19cb5a\",\n      \"name\": \"Stay Within Service Limits\",\n      \"type\": \"n8n-nodes-base.wait\",\n      \"position\": [\n        2280,\n        640\n      ],\n      \"webhookId\": \"677fa706-b4dd-4fe3-ba17-feea944c3193\",\n      \"parameters\": {},\n      \"typeVersion\": 1.1\n    },\n    {\n      \"id\": \"5beb17fa-8a57-4c72-9c3b-b7fdf41b545a\",\n      \"name\": \"For Every 15 Frames\",\n      \"type\": \"n8n-nodes-base.splitInBatches\",\n      \"position\": [\n        1320,\n        380\n      ],\n      \"parameters\": {\n        \"options\": {},\n        \"batchSize\": 15\n      },\n      \"typeVersion\": 3\n    },\n    {\n      \"id\": \"9a57256a-076a-4823-8cad-3b64a17ff705\",\n      \"name\": \"Resize Frame\",\n      \"type\": \"n8n-nodes-base.editImage\",\n      \"position\": [\n        1640,\n        500\n      ],\n      \"parameters\": {\n        \"width\": 768,\n        \"height\": 768,\n        \"options\": {\n          \"format\": \"jpeg\"\n        },\n        \"operation\": \"resize\"\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"3e776939-1a25-4ea0-8106-c3072d108106\",\n      \"name\": \"Aggregate Frames\",\n      \"type\": \"n8n-nodes-base.aggregate\",\n      \"position\": [\n        1800,\n        500\n      ],\n      \"parameters\": {\n        \"options\": {\n          \"includeBinaries\": true\n        },\n        \"aggregate\": \"aggregateAllItemData\"\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"3a973a9c-2c7a-43c5-9c45-a14d49b56622\",\n      \"name\": \"Sticky Note5\",\n      \"type\": \"n8n-nodes-base.stickyNote\",\n      \"position\": [\n        2500,\n        120.6860465116277\n      ],\n      \"parameters\": {\n        \"color\": 7,\n        \"width\": 769.1860465116274,\n        \"height\": 487.83720930232533,\n        \"content\": \"## 4. 使用TTS生成配音片段\\n[详细了解OpenAI节点](https://docs.n8n.io/integrations/builtin/app-nodes/n8n-nodes-langchain.openai)\\n\\n最后，我们可以将生成的脚本片段合并为一个整体，并利用OpenAI的音频生成能力，从完整脚本中生成配音。获得输出的mp3文件后，可将其上传至Google Drive等平台供后续使用。\\n\\n点击此处收听成品效果：https://drive.google.com/file/d/1-XCoii0leGB2MffBMPpCZoxboVyeyeIX/view?usp=sharing\"\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"92e07c18-4058-4098-a448-13451bd8a17a\",\n      \"name\": \"Use Text-to-Speech\",\n      \"type\": \"@n8n/n8n-nodes-langchain.openAi\",\n      \"position\": [\n        2840,\n        360\n      ],\n      \"parameters\": {\n        \"input\": \"={{ $json.data.map(item => item.text).join('\\\\n') }}\",\n        \"options\": {\n          \"response_format\": \"mp3\"\n        },\n        \"resource\": \"audio\"\n      },\n      \"credentials\": {\n        \"openAiApi\": {\n          \"id\": \"8gccIjcuf3gvaoEr\",\n          \"name\": \"OpenAi account\"\n        }\n      },\n      \"typeVersion\": 1.5\n    },\n    {\n      \"id\": \"0696c336-1814-4ad4-aa5e-b86489a4231e\",\n      \"name\": \"Sticky Note6\",\n      \"type\": \"n8n-nodes-base.stickyNote\",\n      \"position\": [\n        61,\n        598\n      ],\n      \"parameters\": {\n        \"color\": 7,\n        \"width\": 458.1279069767452,\n        \"height\": 296.8139534883723,\n        \"content\": \"**本演示所使用的视频来源为**  \\n&copy; [Coverr-Free-Footage](https://pixabay.com/users/coverr-free-footage-1281706/) 经由 [Pixabay](https://pixabay.com/videos/india-street-busy-rickshaw-people-3175/) 授权  \\n![](https://res.cloudinary.com/daglih2g8/image/upload/f_auto,q_auto/v1/n8n-workflows/jhx2tma2gxaabkeiqlgp#full-width)\"\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"81185ac4-c7fd-4921-937f-109662d5dfa5\",\n      \"name\": \"Generate Narration Script\",\n      \"type\": \"@n8n/n8n-nodes-langchain.chainLlm\",\n      \"position\": [\n        1960,\n        500\n      ],\n      \"parameters\": {\n        \"text\": \"=These are frames of a video. Create a short voiceover script in the style of David Attenborough. Only include the narration.\\n{{\\n$('Generate Narration Script').isExecuted\\n ? `Continue from this script:\\\\n${$('Generate Narration Script').all().map(item => item.json.text.replace(/\\\\n/g,'')).join('\\\\n')}`\\n : ''\\n}}\",\n        \"messages\": {\n          \"messageValues\": [\n            {\n              \"type\": \"HumanMessagePromptTemplate\",\n              \"messageType\": \"imageBinary\"\n            },\n            {\n              \"type\": \"HumanMessagePromptTemplate\",\n              \"messageType\": \"imageBinary\",\n              \"binaryImageDataKey\": \"data_1\"\n            },\n            {\n              \"type\": \"HumanMessagePromptTemplate\",\n              \"messageType\": \"imageBinary\",\n              \"binaryImageDataKey\": \"data_2\"\n            },\n            {\n              \"type\": \"HumanMessagePromptTemplate\",\n              \"messageType\": \"imageBinary\",\n              \"binaryImageDataKey\": \"data_3\"\n            },\n            {\n              \"type\": \"HumanMessagePromptTemplate\",\n              \"messageType\": \"imageBinary\",\n              \"binaryImageDataKey\": \"data_4\"\n            },\n            {\n              \"type\": \"HumanMessagePromptTemplate\",\n              \"messageType\": \"imageBinary\",\n              \"binaryImageDataKey\": \"data_5\"\n            },\n            {\n              \"type\": \"HumanMessagePromptTemplate\",\n              \"messageType\": \"imageBinary\",\n              \"binaryImageDataKey\": \"data_6\"\n            },\n            {\n              \"type\": \"HumanMessagePromptTemplate\",\n              \"messageType\": \"imageBinary\",\n              \"binaryImageDataKey\": \"data_7\"\n            },\n            {\n              \"type\": \"HumanMessagePromptTemplate\",\n              \"messageType\": \"imageBinary\",\n              \"binaryImageDataKey\": \"data_8\"\n            },\n            {\n              \"type\": \"HumanMessagePromptTemplate\",\n              \"messageType\": \"imageBinary\",\n              \"binaryImageDataKey\": \"data_9\"\n            },\n            {\n              \"type\": \"HumanMessagePromptTemplate\",\n              \"messageType\": \"imageBinary\",\n              \"binaryImageDataKey\": \"data_10\"\n            },\n            {\n              \"type\": \"HumanMessagePromptTemplate\",\n              \"messageType\": \"imageBinary\",\n              \"binaryImageDataKey\": \"data_11\"\n            },\n            {\n              \"type\": \"HumanMessagePromptTemplate\",\n              \"messageType\": \"imageBinary\",\n              \"binaryImageDataKey\": \"data_12\"\n            },\n            {\n              \"type\": \"HumanMessagePromptTemplate\",\n              \"messageType\": \"imageBinary\",\n              \"binaryImageDataKey\": \"data_13\"\n            },\n            {\n              \"type\": \"HumanMessagePromptTemplate\",\n              \"messageType\": \"imageBinary\",\n              \"binaryImageDataKey\": \"data_14\"\n            }\n          ]\n        },\n        \"promptType\": \"define\"\n      },\n      \"typeVersion\": 1.4\n    }\n  ],\n  \"pinData\": {},\n  \"connections\": {\n    \"Resize Frame\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Aggregate Frames\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Capture Frames\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Split Out Frames\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Combine Script\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Use Text-to-Speech\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Download Video\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Capture Frames\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Aggregate Frames\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Generate Narration Script\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Split Out Frames\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"For Every 15 Frames\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Convert to Binary\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Resize Frame\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"OpenAI Chat Model\": {\n      \"ai_languageModel\": [\n        [\n          {\n            \"node\": \"Generate Narration Script\",\n            \"type\": \"ai_languageModel\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Use Text-to-Speech\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Upload to GDrive\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"For Every 15 Frames\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Combine Script\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ],\n        [\n          {\n            \"node\": \"Convert to Binary\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Generate Narration Script\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Stay Within Service Limits\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Stay Within Service Limits\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"For Every 15 Frames\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"When clicking ‘Test workflow’\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Download Video\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    }\n  }\n}"
}