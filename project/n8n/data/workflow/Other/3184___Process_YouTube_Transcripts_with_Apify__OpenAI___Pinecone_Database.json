{
  "title": "üöÄ Process YouTube Transcripts with Apify, OpenAI & Pinecone Database",
  "url": "https://n8n.io/workflows/3184-process-youtube-transcripts-with-apify-openai-and-pinecone-database/",
  "category": "Other",
  "category_url": "https://n8n.io/workflows/categories/other/?sort=createdAt:desc",
  "author": "Adyl Itto",
  "publish_date": "Last update 2 months ago",
  "publish_date_absolute": "",
  "content": "",
  "workflow_json": "{\"id\":\"Vlyhg8yXcCMMVq7k\",\"meta\":{\"instanceId\":\"7e4a2ed9435505e7ac8a1705caf648bc7288d77cc54adb476b4bec35afce8dbd\"},\"name\":\"YT RAG Agent Backend Transcript-format-pineconeUpsert\",\"tags\":[],\"nodes\":[{\"id\":\"308ee339-14ae-4920-8830-0756440f06b3\",\"name\":\"Airtable\",\"type\":\"n8n-nodes-base.airtable\",\"position\":[40,260],\"parameters\":{},\"typeVersion\":2.1},{\"id\":\"4a0324d8-27a0-468b-9a37-d7297c419cc0\",\"name\":\"Loop Over Items\",\"type\":\"n8n-nodes-base.splitInBatches\",\"position\":[240,260],\"parameters\":{},\"typeVersion\":3},{\"id\":\"14e43ff8-16fd-45f6-b7d9-5ac7e4f10dfe\",\"name\":\"Airtable1\",\"type\":\"n8n-nodes-base.airtable\",\"onError\":\"continueErrorOutput\",\"position\":[1180,340],\"parameters\":{},\"typeVersion\":2.1,\"alwaysOutputData\":true},{\"id\":\"f1e16621-7642-42df-baff-fa943f5ec7f9\",\"name\":\"Wait\",\"type\":\"n8n-nodes-base.wait\",\"position\":[540,340],\"webhookId\":\"bb544bfb-251a-497a-8a17-1b70e2a014c9\",\"parameters\":{},\"typeVersion\":1.1},{\"id\":\"2a17f4a3-91be-465d-9318-f889b87b47ca\",\"name\":\"Apify NinjaPost\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[400,340],\"parameters\":{},\"typeVersion\":4.2},{\"id\":\"a3071928-a592-4f26-a548-ac52cfb18ca6\",\"name\":\"Get JSON TS\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[680,340],\"parameters\":{},\"typeVersion\":4.2,\"alwaysOutputData\":true},{\"id\":\"9216991d-e5de-4681-bab0-c20ddd00c77c\",\"name\":\"JSON Stringifier\",\"type\":\"n8n-nodes-base.code\",\"position\":[820,340],\"parameters\":{},\"typeVersion\":2},{\"id\":\"90db5c4e-e23e-4101-8465-10bbb686b658\",\"name\":\"Edit Fields\",\"type\":\"n8n-nodes-base.set\",\"position\":[980,340],\"parameters\":{},\"typeVersion\":3.4},{\"id\":\"c04e0167-8104-4149-8b81-09ebe7957e91\",\"name\":\"Airtable2\",\"type\":\"n8n-nodes-base.airtable\",\"position\":[100,740],\"parameters\":{},\"typeVersion\":2.1},{\"id\":\"b5132329-2385-4fb9-8d8a-ce838974c189\",\"name\":\"Pinecone Vector Store\",\"type\":\"@n8n/n8n-nodes-langchain.vectorStorePinecone\",\"position\":[420,740],\"parameters\":{},\"typeVersion\":1},{\"id\":\"4c68d784-d86e-4f87-ad72-b2085cb998a1\",\"name\":\"Embeddings OpenAI\",\"type\":\"@n8n/n8n-nodes-langchain.embeddingsOpenAi\",\"position\":[420,920],\"parameters\":{},\"typeVersion\":1.2},{\"id\":\"6f641b5d-5716-4283-bd09-2bb746e41939\",\"name\":\"When clicking ‚ÄòTest workflow‚Äô\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[-40,740],\"parameters\":{},\"typeVersion\":1},{\"id\":\"6ed623e3-0256-44ba-b3d2-c74ea7f38399\",\"name\":\"Transcript Processor\",\"type\":\"n8n-nodes-base.code\",\"position\":[260,740],\"parameters\":{},\"typeVersion\":2},{\"id\":\"5f77e165-4288-4e40-a381-23d2ef409752\",\"name\":\"Default Data Loader\",\"type\":\"@n8n/n8n-nodes-langchain.documentDefaultDataLoader\",\"position\":[540,900],\"parameters\":{},\"typeVersion\":1},{\"id\":\"50495550-405a-4ab5-aa1c-dbd5731fa3f1\",\"name\":\"Recursive Character Text Splitter1\",\"type\":\"@n8n/n8n-nodes-langchain.textSplitterRecursiveCharacterTextSplitter\",\"position\":[540,1060],\"parameters\":{},\"typeVersion\":1},{\"id\":\"aafb300a-7311-43a2-8bbe-67dfabfec313\",\"name\":\"Installation Tutorial\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-260,-760],\"parameters\":{\"content\":\"\"},\"typeVersion\":1}],\"active\":false,\"pinData\":{},\"settings\":{\"executionOrder\":\"v1\"},\"versionId\":\"89526f4d-4135-4cc1-87bc-4fcbf0bc213e\",\"connections\":{\"Wait\":{\"main\":[[{\"node\":\"Get JSON TS\",\"type\":\"main\",\"index\":0}]]},\"Airtable\":{\"main\":[[{\"node\":\"Loop Over Items\",\"type\":\"main\",\"index\":0}]]},\"Airtable1\":{\"main\":[[{\"node\":\"Loop Over Items\",\"type\":\"main\",\"index\":0}],[{\"node\":\"Loop Over Items\",\"type\":\"main\",\"index\":0}]]},\"Airtable2\":{\"main\":[[{\"node\":\"Transcript Processor\",\"type\":\"main\",\"index\":0}]]},\"Edit Fields\":{\"main\":[[{\"node\":\"Airtable1\",\"type\":\"main\",\"index\":0}]]},\"Get JSON TS\":{\"main\":[[{\"node\":\"JSON Stringifier\",\"type\":\"main\",\"index\":0}]]},\"Apify NinjaPost\":{\"main\":[[{\"node\":\"Wait\",\"type\":\"main\",\"index\":0}]]},\"Loop Over Items\":{\"main\":[[],[{\"node\":\"Apify NinjaPost\",\"type\":\"main\",\"index\":0}]]},\"JSON Stringifier\":{\"main\":[[{\"node\":\"Edit Fields\",\"type\":\"main\",\"index\":0}]]},\"Embeddings OpenAI\":{\"ai_embedding\":[[{\"node\":\"Pinecone Vector Store\",\"type\":\"ai_embedding\",\"index\":0}]]},\"Default Data Loader\":{\"ai_document\":[[{\"node\":\"Pinecone Vector Store\",\"type\":\"ai_document\",\"index\":0}]]},\"Transcript Processor\":{\"main\":[[{\"node\":\"Pinecone Vector Store\",\"type\":\"main\",\"index\":0}]]},\"When clicking ‚ÄòTest workflow‚Äô\":{\"main\":[[{\"node\":\"Airtable2\",\"type\":\"main\",\"index\":0}]]},\"Recursive Character Text Splitter1\":{\"ai_textSplitter\":[[{\"node\":\"Default Data Loader\",\"type\":\"ai_textSplitter\",\"index\":0}]]}}}",
  "readme": "# üöÄ YouTube Transcript Indexing Backend for Pinecone üé•üíæ\n\nThis tutorial explains how to build the **backend** workflow in n8n that indexes YouTube video transcripts into a Pinecone vector database. **Note:** This workflow handles the processing and indexing of transcripts only‚Äîthe retrieval agent (which searches these embeddings) is implemented separately.\n\n* * *\n\n## üìã Workflow Overview\n\nThis backend workflow performs the following tasks:\n\n  1. **Fetch Video Records from Airtable** üì•  \nRetrieves video URLs and related metadata.\n\n  2. **Scrape YouTube Transcripts Using Apify** üé¨  \nTriggers an Apify actor to scrape transcripts with timestamps from each video.\n\n  3. **Update Airtable with Transcript Data** üîÑ  \nStores the fetched transcript JSON back in Airtable linked via video ID.\n\n  4. **Process & Chunk Transcripts** ‚úÇÔ∏è  \nParses the transcript JSON, converts \"mm:ss\" timestamps to seconds, and groups entries into meaningful chunks. Each chunk is enriched with metadata‚Äîsuch as video title, description, start/end timestamps, and a direct URL linking to that video moment.\n\n  5. **Generate Embeddings & Index in Pinecone** üíæ  \nUses OpenAI to create vector embeddings for each transcript chunk and indexes them in Pinecone. This enables efficient semantic searches later by a separate retrieval agent.\n\n\n\n\n* * *\n\n## üîß Step-by-Step Guide\n\n### Step 1: Retrieve Video Records from Airtable üì•\n\n  * **Airtable Search Node:**\n\n    * **Setup:** Configure the node to fetch video records (with essential fields like `url` and metadata) from your Airtable base.\n  * **Loop Over Items:**\n\n    * Use a **SplitInBatches** node to process each video record individually.\n\n\n\n* * *\n\n### Step 2: Scrape YouTube Transcripts Using Apify üé¨\n\n  * **Trigger Apify Actor:**\n\n    * **HTTP Request Node (\"Apify NinjaPost\"):**\n      * **Method:** POST\n      * **Endpoint:** `https://api.apify.com/v2/acts/topaz_sharingan~youtube-transcript-scraper-1/runs?token=&lt;YOUR_TOKEN&gt;`\n      * **Payload Example:**\n            \n            {\n              \"includeTimestamps\": \"Yes\",\n              \"startUrls\": [\"{{ $json.url }}\"]\n            }\n            \n\n    * **Purpose:** Initiates transcript scraping for each video URL.\n  * **Wait for Processing:**\n\n    * **Wait Node:**\n      * **Duration:** Approximately 1 minute to allow Apify to generate the transcript.\n  * **Retrieve Transcript Data:**\n\n    * **HTTP Request Node (\"Get JSON TS\"):**\n      * **Method:** GET\n      * **Endpoint:** `https://api.apify.com/v2/acts/topaz_sharingan~youtube-transcript-scraper-1/runs/last/dataset/items?token=&lt;YOUR_TOKEN&gt;`\n\n\n\n* * *\n\n### Step 3: Update Airtable with Transcript Data üîÑ\n\n  * **Format Transcript Data:**\n\n    * **Code Node (\"Code\"):**\n      * **Task:** Convert the fetched transcript JSON into a formatted string.\n            \n            const jsonObject = items[0].json;\n            const jsonString = JSON.stringify(jsonObject, null, 2);\n            return { json: { stringifiedJson: jsonString } };\n            \n\n  * **Extract the Video ID:**\n\n    * **Set Node (\"Edit Fields\"):**\n      * **Expression:**\n            \n            {{$json.url.split('v=')[1].split('&')[0]}}\n            \n\n  * **Update Airtable Record:**\n\n    * **Airtable Update Node (\"Airtable1\"):**\n      * **Updates:**\n        * **ts:** Stores the transcript string.\n        * **videoid:** Uses the extracted video ID to match the record.\n\n\n\n* * *\n\n### Step 4: Process Transcripts into Semantic Chunks ‚úÇÔ∏è\n\n  * **Retrieve Updated Records:**\n\n    * **Airtable Search Node (\"Airtable2\"):**\n      * **Purpose:** Fetch records that now contain transcript data.\n  * **Parse and Chunk Transcripts:**\n\n    * **Code Node (\"Code4\"):**\n      * **Functionality:**\n        * Parses transcript JSON.\n        * Converts \"mm:ss\" timestamps to seconds.\n        * Groups transcript entries into chunks based on a 3-second gap.\n        * Creates an object for each chunk that includes: \n          * **Text:** The transcript segment.\n          * **Video Metadata:** Video ID, title, description, published date, thumbnail.\n          * **Chunk Details:** Start and end timestamps.\n          * **Direct URL:** A link to the exact moment in the video (e.g., `https://youtube.com/watch?v=VIDEOID&t=XXs`).\n  * **Enrich & Split Text:**\n\n    * **Default Data Loader Node:**\n      * Attaches additional metadata (e.g., video title, description) to each chunk.\n    * **Recursive Character Text Splitter Node:**\n      * **Settings:** Typically set to 500-character chunks with a 50-character overlap.\n      * **Purpose:** Ensures long transcript texts are broken into manageable segments for embedding.\n\n\n\n* * *\n\n### Step 5: Generate Embeddings & Index in Pinecone üíæ\n\n  * **Generate Embeddings:**\n\n    * **Embeddings OpenAI Node:**\n      * **Task:** Convert each transcript chunk into a vector embedding.\n      * **Tip:** Adjust the batch size (e.g., 512) based on your data volume.\n  * **Index in Pinecone:**\n\n    * **Pinecone Vector Store Node:**\n      * **Configuration:**\n        * **Index:** Specify your Pinecone index (e.g., `\"videos\"`).\n        * **Namespace:** Use a dedicated namespace (e.g., `\"transcripts\"`).\n      * **Outcome:** Each enriched transcript chunk is stored in Pinecone, ready for semantic retrieval by a separate retrieval agent.\n\n\n\n* * *\n\n## üéâ Final Thoughts\n\nThis backend workflow is dedicated to processing and indexing YouTube video transcripts so that a separate retrieval agent can perform efficient semantic searches. With this setup:\n\n  * **Transcripts Are Indexed:**  \nChunks of transcripts are enriched with metadata and stored as vector embeddings.\n\n  * **Instant Topic Retrieval:**  \nA retrieval agent (implemented separately) can later query Pinecone to find the exact moment in a video where a topic is discussed, thanks to the direct URL and metadata stored with each chunk.\n\n  * **Scalable & Modular:**  \nThe separation between indexing and retrieval allows for easy updates and scalability.\n\n\n\n\nHappy automating and enjoy building powerful search capabilities with your YouTube content! üéâ\n",
  "readme_html": "<!--[--><div data-v-50766329=\"\"><h1>üöÄ YouTube Transcript Indexing Backend for Pinecone üé•üíæ</h1>\n<p>This tutorial explains how to build the <strong>backend</strong> workflow in n8n that indexes YouTube video transcripts into a Pinecone vector database. <strong>Note:</strong> This workflow handles the processing and indexing of transcripts only‚Äîthe retrieval agent (which searches these embeddings) is implemented separately.</p>\n<hr>\n<h2>üìã Workflow Overview</h2>\n<p>This backend workflow performs the following tasks:</p>\n<ol>\n<li>\n<p><strong>Fetch Video Records from Airtable</strong> üì•<br>\nRetrieves video URLs and related metadata.</p>\n</li>\n<li>\n<p><strong>Scrape YouTube Transcripts Using Apify</strong> üé¨<br>\nTriggers an Apify actor to scrape transcripts with timestamps from each video.</p>\n</li>\n<li>\n<p><strong>Update Airtable with Transcript Data</strong> üîÑ<br>\nStores the fetched transcript JSON back in Airtable linked via video ID.</p>\n</li>\n<li>\n<p><strong>Process &amp; Chunk Transcripts</strong> ‚úÇÔ∏è<br>\nParses the transcript JSON, converts \"mm:ss\" timestamps to seconds, and groups entries into meaningful chunks. Each chunk is enriched with metadata‚Äîsuch as video title, description, start/end timestamps, and a direct URL linking to that video moment.</p>\n</li>\n<li>\n<p><strong>Generate Embeddings &amp; Index in Pinecone</strong> üíæ<br>\nUses OpenAI to create vector embeddings for each transcript chunk and indexes them in Pinecone. This enables efficient semantic searches later by a separate retrieval agent.</p>\n</li>\n</ol>\n<hr>\n<h2>üîß Step-by-Step Guide</h2>\n<h3>Step 1: Retrieve Video Records from Airtable üì•</h3>\n<ul>\n<li>\n<p><strong>Airtable Search Node:</strong></p>\n<ul>\n<li><strong>Setup:</strong> Configure the node to fetch video records (with essential fields like <code>url</code> and metadata) from your Airtable base.</li>\n</ul>\n</li>\n<li>\n<p><strong>Loop Over Items:</strong></p>\n<ul>\n<li>Use a <strong>SplitInBatches</strong> node to process each video record individually.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3>Step 2: Scrape YouTube Transcripts Using Apify üé¨</h3>\n<ul>\n<li>\n<p><strong>Trigger Apify Actor:</strong></p>\n<ul>\n<li><strong>HTTP Request Node (\"Apify NinjaPost\"):</strong>\n<ul>\n<li><strong>Method:</strong> POST</li>\n<li><strong>Endpoint:</strong> <code>https://api.apify.com/v2/acts/topaz_sharingan~youtube-transcript-scraper-1/runs?token=&amp;lt;YOUR_TOKEN&amp;gt;</code></li>\n<li><strong>Payload Example:</strong><pre><code>{\n  \"includeTimestamps\": \"Yes\",\n  \"startUrls\": [\"{{ $json.url }}\"]\n}\n</code></pre>\n</li>\n</ul>\n</li>\n<li><strong>Purpose:</strong> Initiates transcript scraping for each video URL.</li>\n</ul>\n</li>\n<li>\n<p><strong>Wait for Processing:</strong></p>\n<ul>\n<li><strong>Wait Node:</strong>\n<ul>\n<li><strong>Duration:</strong> Approximately 1 minute to allow Apify to generate the transcript.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><strong>Retrieve Transcript Data:</strong></p>\n<ul>\n<li><strong>HTTP Request Node (\"Get JSON TS\"):</strong>\n<ul>\n<li><strong>Method:</strong> GET</li>\n<li><strong>Endpoint:</strong> <code>https://api.apify.com/v2/acts/topaz_sharingan~youtube-transcript-scraper-1/runs/last/dataset/items?token=&amp;lt;YOUR_TOKEN&amp;gt;</code></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3>Step 3: Update Airtable with Transcript Data üîÑ</h3>\n<ul>\n<li>\n<p><strong>Format Transcript Data:</strong></p>\n<ul>\n<li><strong>Code Node (\"Code\"):</strong>\n<ul>\n<li><strong>Task:</strong> Convert the fetched transcript JSON into a formatted string.<pre><code>const jsonObject = items[0].json;\nconst jsonString = JSON.stringify(jsonObject, null, 2);\nreturn { json: { stringifiedJson: jsonString } };\n</code></pre>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><strong>Extract the Video ID:</strong></p>\n<ul>\n<li><strong>Set Node (\"Edit Fields\"):</strong>\n<ul>\n<li><strong>Expression:</strong><pre><code>{{$json.url.split('v=')[1].split('&amp;')[0]}}\n</code></pre>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><strong>Update Airtable Record:</strong></p>\n<ul>\n<li><strong>Airtable Update Node (\"Airtable1\"):</strong>\n<ul>\n<li><strong>Updates:</strong>\n<ul>\n<li><strong>ts:</strong> Stores the transcript string.</li>\n<li><strong>videoid:</strong> Uses the extracted video ID to match the record.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3>Step 4: Process Transcripts into Semantic Chunks ‚úÇÔ∏è</h3>\n<ul>\n<li>\n<p><strong>Retrieve Updated Records:</strong></p>\n<ul>\n<li><strong>Airtable Search Node (\"Airtable2\"):</strong>\n<ul>\n<li><strong>Purpose:</strong> Fetch records that now contain transcript data.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><strong>Parse and Chunk Transcripts:</strong></p>\n<ul>\n<li><strong>Code Node (\"Code4\"):</strong>\n<ul>\n<li><strong>Functionality:</strong>\n<ul>\n<li>Parses transcript JSON.</li>\n<li>Converts \"mm:ss\" timestamps to seconds.</li>\n<li>Groups transcript entries into chunks based on a 3-second gap.</li>\n<li>Creates an object for each chunk that includes:\n<ul>\n<li><strong>Text:</strong> The transcript segment.</li>\n<li><strong>Video Metadata:</strong> Video ID, title, description, published date, thumbnail.</li>\n<li><strong>Chunk Details:</strong> Start and end timestamps.</li>\n<li><strong>Direct URL:</strong> A link to the exact moment in the video (e.g., <code>https://youtube.com/watch?v=VIDEOID&amp;t=XXs</code>).</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><strong>Enrich &amp; Split Text:</strong></p>\n<ul>\n<li><strong>Default Data Loader Node:</strong>\n<ul>\n<li>Attaches additional metadata (e.g., video title, description) to each chunk.</li>\n</ul>\n</li>\n<li><strong>Recursive Character Text Splitter Node:</strong>\n<ul>\n<li><strong>Settings:</strong> Typically set to 500-character chunks with a 50-character overlap.</li>\n<li><strong>Purpose:</strong> Ensures long transcript texts are broken into manageable segments for embedding.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3>Step 5: Generate Embeddings &amp; Index in Pinecone üíæ</h3>\n<ul>\n<li>\n<p><strong>Generate Embeddings:</strong></p>\n<ul>\n<li><strong>Embeddings OpenAI Node:</strong>\n<ul>\n<li><strong>Task:</strong> Convert each transcript chunk into a vector embedding.</li>\n<li><strong>Tip:</strong> Adjust the batch size (e.g., 512) based on your data volume.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><strong>Index in Pinecone:</strong></p>\n<ul>\n<li><strong>Pinecone Vector Store Node:</strong>\n<ul>\n<li><strong>Configuration:</strong>\n<ul>\n<li><strong>Index:</strong> Specify your Pinecone index (e.g., <code>\"videos\"</code>).</li>\n<li><strong>Namespace:</strong> Use a dedicated namespace (e.g., <code>\"transcripts\"</code>).</li>\n</ul>\n</li>\n<li><strong>Outcome:</strong> Each enriched transcript chunk is stored in Pinecone, ready for semantic retrieval by a separate retrieval agent.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2>üéâ Final Thoughts</h2>\n<p>This backend workflow is dedicated to processing and indexing YouTube video transcripts so that a separate retrieval agent can perform efficient semantic searches. With this setup:</p>\n<ul>\n<li>\n<p><strong>Transcripts Are Indexed:</strong><br>\nChunks of transcripts are enriched with metadata and stored as vector embeddings.</p>\n</li>\n<li>\n<p><strong>Instant Topic Retrieval:</strong><br>\nA retrieval agent (implemented separately) can later query Pinecone to find the exact moment in a video where a topic is discussed, thanks to the direct URL and metadata stored with each chunk.</p>\n</li>\n<li>\n<p><strong>Scalable &amp; Modular:</strong><br>\nThe separation between indexing and retrieval allows for easy updates and scalability.</p>\n</li>\n</ul>\n<p>Happy automating and enjoy building powerful search capabilities with your YouTube content! üéâ</p>\n</div><!--]-->"
}