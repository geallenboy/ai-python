{
  "title": "Chat with local LLMs using n8n and Ollama",
  "url": "https://n8n.io/workflows/2384-chat-with-local-llms-using-n8n-and-ollama/",
  "category": "AI",
  "category_url": "https://n8n.io/workflows/categories/ai/?count=20",
  "author": "Mihai Farcas",
  "publish_date": "Last update 8 months ago",
  "content": "",
  "workflow_json": "{\"id\":\"af8RV5b2TWB2LclA\",\"meta\":{\"instanceId\":\"95f2ab28b3dabb8da5d47aa5145b95fe3845f47b20d6343dd5256b6a28ba8fab\",\"templateCredsSetupCompleted\":true},\"name\":\"Chat with local LLMs using n8n and Ollama\",\"tags\":[],\"nodes\":[{\"id\":\"475385fa-28f3-45c4-bd1a-10dde79f74f2\",\"name\":\"When chat message received\",\"type\":\"@n8n/n8n-nodes-langchain.chatTrigger\",\"position\":[700,460],\"webhookId\":\"ebdeba3f-6b4f-49f3-ba0a-8253dd226161\",\"parameters\":{\"options\":{}},\"typeVersion\":1.1},{\"id\":\"61133dc6-dcd9-44ff-85f2-5d8cc2ce813e\",\"name\":\"Ollama Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOllama\",\"position\":[900,680],\"parameters\":{\"options\":{}},\"credentials\":{\"ollamaApi\":{\"id\":\"MyYvr1tcNQ4e7M6l\",\"name\":\"Local Ollama\"}},\"typeVersion\":1},{\"id\":\"3e89571f-7c87-44c6-8cfd-4903d5e1cdc5\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[160,80],\"parameters\":{\"width\":485,\"height\":473,\"content\":\"## Chat with local LLMs using n8n and Ollama\\nThis n8n workflow allows you to seamlessly interact with your self-hosted Large Language Models (LLMs) through a user-friendly chat interface. By connecting to Ollama, a powerful tool for managing local LLMs, you can send prompts and receive AI-generated responses directly within n8n.\\n\\n### How it works\\n1. When chat message received: Captures the user's input from the chat interface.\\n2. Chat LLM Chain: Sends the input to the Ollama server and receives the AI-generated response.\\n3. Delivers the LLM's response back to the chat interface.\\n\\n### Set up steps\\n* Make sure Ollama is installed and running on your machine before executing this workflow.\\n* Edit the Ollama address if different from the default.\\n\"},\"typeVersion\":1},{\"id\":\"9345cadf-a72e-4d3d-b9f0-d670744065fe\",\"name\":\"Sticky Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1040,660],\"parameters\":{\"color\":6,\"width\":368,\"height\":258,\"content\":\"## Ollama setup\\n* Connect to your local Ollama, usually on http://localhost:11434\\n* If running in Docker, make sure that the n8n container has access to the host's network in order to connect to Ollama. You can do this by passing `--net=host` option when starting the n8n Docker container\"},\"typeVersion\":1},{\"id\":\"eeffdd4e-6795-4ebc-84f7-87b5ac4167d9\",\"name\":\"Chat LLM Chain\",\"type\":\"@n8n/n8n-nodes-langchain.chainLlm\",\"position\":[920,460],\"parameters\":{},\"typeVersion\":1.4}],\"active\":false,\"pinData\":{},\"settings\":{\"executionOrder\":\"v1\"},\"versionId\":\"3af03daa-e085-4774-8676-41578a4cba2d\",\"connections\":{\"Ollama Chat Model\":{\"ai_languageModel\":[[{\"node\":\"Chat LLM Chain\",\"type\":\"ai_languageModel\",\"index\":0}]]},\"When chat message received\":{\"main\":[[{\"node\":\"Chat LLM Chain\",\"type\":\"main\",\"index\":0}]]}}}",
  "readme": "## Chat with local LLMs using n8n and Ollama\n\nThis n8n workflow allows you to seamlessly interact with your self-hosted Large Language Models (LLMs) through a user-friendly chat interface. By connecting to Ollama, a powerful tool for managing local LLMs, you can send prompts and receive AI-generated responses directly within n8n.\n\n### Use cases\n\n  * Private AI Interactions  \nIdeal for scenarios where data privacy and confidentiality are important.\n  * Cost-Effective LLM Usage  \nAvoid ongoing cloud API costs by running models on your own hardware.\n  * Experimentation & Learning  \nA great way to explore and experiment with different LLMs in a local, controlled environment.\n  * Prototyping & Development  \nBuild and test AI-powered applications without relying on external services.\n\n\n\n### How it works\n\n  1. When chat message received: Captures the user's input from the chat interface.\n  2. Chat LLM Chain: Sends the input to the Ollama server and receives the AI-generated response.\n  3. Delivers the LLM's response back to the chat interface.\n\n\n\n### Set up steps\n\n  * Make sure Ollama is installed and running on your machine before executing this workflow.\n  * Edit the Ollama address if different from the default.\n\n\n",
  "readme_html": "<!--[--><div data-v-006f9244=\"\"><h2>Chat with local LLMs using n8n and Ollama</h2>\n<p>This n8n workflow allows you to seamlessly interact with your self-hosted Large Language Models (LLMs) through a user-friendly chat interface. By connecting to Ollama, a powerful tool for managing local LLMs, you can send prompts and receive AI-generated responses directly within n8n.</p>\n<h3>Use cases</h3>\n<ul>\n<li>Private AI Interactions<br>\nIdeal for scenarios where data privacy and confidentiality are important.</li>\n<li>Cost-Effective LLM Usage<br>\nAvoid ongoing cloud API costs by running models on your own hardware.</li>\n<li>Experimentation &amp; Learning<br>\nA great way to explore and experiment with different LLMs in a local, controlled environment.</li>\n<li>Prototyping &amp; Development<br>\nBuild and test AI-powered applications without relying on external services.</li>\n</ul>\n<h3>How it works</h3>\n<ol>\n<li>When chat message received: Captures the user's input from the chat interface.</li>\n<li>Chat LLM Chain: Sends the input to the Ollama server and receives the AI-generated response.</li>\n<li>Delivers the LLM's response back to the chat interface.</li>\n</ol>\n<h3>Set up steps</h3>\n<ul>\n<li>Make sure Ollama is installed and running on your machine before executing this workflow.</li>\n<li>Edit the Ollama address if different from the default.</li>\n</ul>\n</div><!--]-->",
  "readme_zh": "## 使用n8n与Ollama实现本地大语言模型对话\n\n该n8n工作流让您能通过友好聊天界面，无缝对接自托管的大语言模型。通过连接Ollama（一款强大的本地LLM管理工具），您可以直接在n8n平台发送提示词并获取AI生成的响应。\n\n### 核心应用场景\n\n  * 私有化AI对话  \n适用于注重数据隐私与机密性的场景\n  * 经济高效的模型调用  \n通过本地硬件运行模型，避免持续产生的云端API费用\n  * 实验与学习  \n在本地可控环境中探索不同大语言模型的理想方式\n  * 原型开发  \n不依赖外部服务即可构建测试AI驱动型应用\n\n### 实现原理\n\n  1. 接收聊天消息：从聊天界面捕获用户输入\n  2. 大模型对话链：将输入发送至Ollama服务器并接收AI响应\n  3. 将模型响应返回至聊天界面\n\n### 配置步骤\n\n  * 执行工作流前请确保已安装并运行Ollama\n  * 若Ollama地址非默认值，请手动修改配置\n\n（注：LLM在中文技术社区常译为\"大语言模型\"，故采用此译法；\"prompts\"根据上下文译为\"提示词\"符合AI领域术语；\"user-friendly chat interface\"采用\"友好聊天界面\"的意译更符合中文表达习惯；保持\"Ollama\"作为专有名词不翻译；\"self-hosted\"译为\"自托管\"是行业通用译法）",
  "title_zh": "使用n8n和Ollama与本地大语言模型对话",
  "publish_date_absolute": "2024-09-07",
  "publish_date_zh": "最后更新于8个月前",
  "workflow_json_zh": "{\"id\":\"af8RV5b2TWB2LclA\",\"meta\":{\"instanceId\":\"95f2ab28b3dabb8da5d47aa5145b95fe3845f47b20d6343dd5256b6a28ba8fab\",\"templateCredsSetupCompleted\":true},\"name\":\"Chat with local LLMs using n8n and Ollama\",\"tags\":[],\"nodes\":[{\"id\":\"475385fa-28f3-45c4-bd1a-10dde79f74f2\",\"name\":\"When chat message received\",\"type\":\"@n8n/n8n-nodes-langchain.chatTrigger\",\"position\":[700,460],\"webhookId\":\"ebdeba3f-6b4f-49f3-ba0a-8253dd226161\",\"parameters\":{\"options\":{}},\"typeVersion\":1.1},{\"id\":\"61133dc6-dcd9-44ff-85f2-5d8cc2ce813e\",\"name\":\"Ollama Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOllama\",\"position\":[900,680],\"parameters\":{\"options\":{}},\"credentials\":{\"ollamaApi\":{\"id\":\"MyYvr1tcNQ4e7M6l\",\"name\":\"Local Ollama\"}},\"typeVersion\":1},{\"id\":\"3e89571f-7c87-44c6-8cfd-4903d5e1cdc5\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[160,80],\"parameters\":{\"width\":485,\"height\":473,\"content\":\"## 使用n8n与Ollama实现本地大语言模型对话  \\n该n8n工作流让您能通过友好聊天界面，无缝对接自托管的大语言模型（LLMs）。借助Ollama这一强大的本地LLM管理工具，您可以直接在n8n内发送提示词并获取AI生成的回复。\\n\\n### 实现原理  \\n1. 接收聊天消息：从聊天界面捕获用户输入  \\n2. 大语言模型链：将输入发送至Ollama服务器并接收AI生成的响应  \\n3. 将模型响应传回聊天界面  \\n\\n### 配置步骤  \\n* 执行工作流前请确保Ollama已在本地安装并运行  \\n* 若Ollama地址非默认值，请手动修改配置\"},\"typeVersion\":1},{\"id\":\"9345cadf-a72e-4d3d-b9f0-d670744065fe\",\"name\":\"Sticky Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1040,660],\"parameters\":{\"color\":6,\"width\":368,\"height\":258,\"content\":\"## Ollama 设置\\n* 连接到本地Ollama服务，通常地址为 http://localhost:11434\\n* 若在Docker环境中运行，请确保n8n容器能访问宿主机网络以便连接Ollama。启动n8n Docker容器时可通过添加`--net=host`参数实现\"},\"typeVersion\":1},{\"id\":\"eeffdd4e-6795-4ebc-84f7-87b5ac4167d9\",\"name\":\"Chat LLM Chain\",\"type\":\"@n8n/n8n-nodes-langchain.chainLlm\",\"position\":[920,460],\"parameters\":{},\"typeVersion\":1.4}],\"active\":false,\"pinData\":{},\"settings\":{\"executionOrder\":\"v1\"},\"versionId\":\"3af03daa-e085-4774-8676-41578a4cba2d\",\"connections\":{\"Ollama Chat Model\":{\"ai_languageModel\":[[{\"node\":\"Chat LLM Chain\",\"type\":\"ai_languageModel\",\"index\":0}]]},\"When chat message received\":{\"main\":[[{\"node\":\"Chat LLM Chain\",\"type\":\"main\",\"index\":0}]]}}}"
}