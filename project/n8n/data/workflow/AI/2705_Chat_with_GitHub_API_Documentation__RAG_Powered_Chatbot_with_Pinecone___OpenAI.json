{
  "title": "Chat with GitHub API Documentation: RAG-Powered Chatbot with Pinecone & OpenAI",
  "url": "https://n8n.io/workflows/2705-chat-with-github-api-documentation-rag-powered-chatbot-with-pinecone-and-openai/",
  "category": "AI",
  "category_url": "https://n8n.io/workflows/categories/ai/?count=20",
  "author": "Mihai Farcas",
  "publish_date": "Last update 4 months ago",
  "content": "",
  "workflow_json": "{\"id\":\"FD0bHNaehP3LzCNN\",\"meta\":{\"instanceId\":\"69133932b9ba8e1ef14816d0b63297bb44feb97c19f759b5d153ff6b0c59e18d\"},\"name\":\"Chat with GitHub OpenAPI Specification using RAG (Pinecone and OpenAI)\",\"tags\":[],\"nodes\":[{\"id\":\"362cb773-7540-4753-a401-e585cdf4af8a\",\"name\":\"When clicking ‘Test workflow’\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[0,0],\"parameters\":{},\"typeVersion\":1},{\"id\":\"45470036-cae6-48d0-ac66-addc8999e776\",\"name\":\"HTTP Request\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[300,0],\"parameters\":{\"url\":\"https://raw.githubusercontent.com/github/rest-api-description/refs/heads/main/descriptions/api.github.com/api.github.com.json\",\"options\":{}},\"typeVersion\":4.2},{\"id\":\"a9e65897-52c9-4941-bf49-e1a659e442ef\",\"name\":\"Pinecone Vector Store\",\"type\":\"@n8n/n8n-nodes-langchain.vectorStorePinecone\",\"position\":[520,0],\"parameters\":{\"mode\":\"insert\",\"options\":{},\"pineconeIndex\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"n8n-demo\",\"cachedResultName\":\"n8n-demo\"}},\"credentials\":{\"pineconeApi\":{\"id\":\"bQTNry52ypGLqt47\",\"name\":\"PineconeApi account\"}},\"typeVersion\":1},{\"id\":\"c2a2354b-5457-4ceb-abfc-9a58e8593b81\",\"name\":\"Default Data Loader\",\"type\":\"@n8n/n8n-nodes-langchain.documentDefaultDataLoader\",\"position\":[660,180],\"parameters\":{\"options\":{}},\"typeVersion\":1},{\"id\":\"7338d9ea-ae8f-46eb-807f-a15dc7639fc9\",\"name\":\"Recursive Character Text Splitter\",\"type\":\"@n8n/n8n-nodes-langchain.textSplitterRecursiveCharacterTextSplitter\",\"position\":[740,360],\"parameters\":{\"options\":{}},\"typeVersion\":1},{\"id\":\"44fd7a59-f208-4d5d-a22d-e9f8ca9badf1\",\"name\":\"When chat message received\",\"type\":\"@n8n/n8n-nodes-langchain.chatTrigger\",\"position\":[-20,760],\"webhookId\":\"089e38ab-4eee-4c34-aa5d-54cf4a8f53b7\",\"parameters\":{\"options\":{}},\"typeVersion\":1.1},{\"id\":\"51d819d6-70ff-428d-aa56-1d7e06490dee\",\"name\":\"AI Agent\",\"type\":\"@n8n/n8n-nodes-langchain.agent\",\"position\":[320,760],\"parameters\":{\"options\":{\"systemMessage\":\"You are a helpful assistant providing information about the GitHub API and how to use it based on the OpenAPI V3 specifications.\"}},\"typeVersion\":1.7},{\"id\":\"aed548bf-7083-44ad-a3e0-163dee7423ef\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[220,980],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"tQLWnWRzD8aebYvp\",\"name\":\"OpenAi account\"}},\"typeVersion\":1.1},{\"id\":\"dfe9f356-2225-4f4b-86c7-e56a230b4193\",\"name\":\"Window Buffer Memory\",\"type\":\"@n8n/n8n-nodes-langchain.memoryBufferWindow\",\"position\":[420,1020],\"parameters\":{},\"typeVersion\":1.3},{\"id\":\"4cf672ee-13b8-4355-b8e0-c2e7381671bc\",\"name\":\"Vector Store Tool\",\"type\":\"@n8n/n8n-nodes-langchain.toolVectorStore\",\"position\":[580,980],\"parameters\":{\"name\":\"GitHub_OpenAPI_Specification\",\"description\":\"Use this tool to get information about the GitHub   API. This database contains OpenAPI v3 specifications.\"},\"typeVersion\":1},{\"id\":\"1df7fb85-9d4a-4db5-9bed-41d28e2e4643\",\"name\":\"OpenAI Chat Model1\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[840,1160],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"tQLWnWRzD8aebYvp\",\"name\":\"OpenAi account\"}},\"typeVersion\":1.1},{\"id\":\"7b52ef7a-5935-451e-8747-efe16ce288af\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-40,-260],\"parameters\":{\"width\":640,\"height\":200,\"content\":\"## Indexing content in the vector database\\nThis part of the workflow is responsible for extracting content, generating embeddings and sending them to the Pinecone vector store.\\n\\nIt requests the OpenAPI specifications from GitHub using a HTTP request. Then, it splits the file in chunks, generating embeddings for each chunk using OpenAI, and saving them in Pinecone vector DB.\"},\"typeVersion\":1},{\"id\":\"3508d602-56d4-4818-84eb-ca75cdeec1d0\",\"name\":\"Sticky Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-20,560],\"parameters\":{\"width\":580,\"content\":\"## Querying and response generation \\n\\nThis part of the workflow is responsible for the chat interface, querying the vector store and generating relevant responses.\\n\\nIt uses OpenAI GPT 4o-mini to generate responses.\"},\"typeVersion\":1},{\"id\":\"5a9808ef-4edd-4ec9-ba01-2fe50b2dbf4b\",\"name\":\"Generate User Query Embedding\",\"type\":\"@n8n/n8n-nodes-langchain.embeddingsOpenAi\",\"position\":[480,1400],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"tQLWnWRzD8aebYvp\",\"name\":\"OpenAi account\"}},\"typeVersion\":1.2},{\"id\":\"f703dc8e-9d4b-45e3-8994-789b3dfe8631\",\"name\":\"Pinecone Vector Store (Querying)\",\"type\":\"@n8n/n8n-nodes-langchain.vectorStorePinecone\",\"position\":[440,1220],\"parameters\":{\"options\":{},\"pineconeIndex\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"n8n-demo\",\"cachedResultName\":\"n8n-demo\"}},\"credentials\":{\"pineconeApi\":{\"id\":\"bQTNry52ypGLqt47\",\"name\":\"PineconeApi account\"}},\"typeVersion\":1},{\"id\":\"ea64a7a5-1fa5-4938-83a9-271929733a8e\",\"name\":\"Generate Embeddings\",\"type\":\"@n8n/n8n-nodes-langchain.embeddingsOpenAi\",\"position\":[480,220],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"tQLWnWRzD8aebYvp\",\"name\":\"OpenAi account\"}},\"typeVersion\":1.2},{\"id\":\"65cbd4e3-91f6-441a-9ef1-528c3019e238\",\"name\":\"Sticky Note2\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-820,-260],\"parameters\":{\"width\":620,\"height\":320,\"content\":\"## RAG workflow in n8n\\n\\nThis is an example of how to use RAG techniques to create a chatbot with n8n. It is an API documentation chatbot that can answer questions about the GitHub API. It uses OpenAI for generating embeddings, the gpt-4o-mini LLM for generating responses and Pinecone as a vector database.\\n\\n### Before using this template\\n* create OpenAI and Pinecone accounts\\n* obtain API keys OpenAI and Pinecone \\n* configure credentials in n8n for both\\n* ensure you have a Pinecone index named \\\"n8n-demo\\\" or adjust the workflow accordingly.\"},\"typeVersion\":1}],\"active\":false,\"pinData\":{},\"settings\":{\"executionOrder\":\"v1\"},\"versionId\":\"2908105f-c20c-4183-bb9d-26e3559b9911\",\"connections\":{\"HTTP Request\":{\"main\":[[{\"node\":\"Pinecone Vector Store\",\"type\":\"main\",\"index\":0}]]},\"OpenAI Chat Model\":{\"ai_languageModel\":[[{\"node\":\"AI Agent\",\"type\":\"ai_languageModel\",\"index\":0}]]},\"Vector Store Tool\":{\"ai_tool\":[[{\"node\":\"AI Agent\",\"type\":\"ai_tool\",\"index\":0}]]},\"OpenAI Chat Model1\":{\"ai_languageModel\":[[{\"node\":\"Vector Store Tool\",\"type\":\"ai_languageModel\",\"index\":0}]]},\"Default Data Loader\":{\"ai_document\":[[{\"node\":\"Pinecone Vector Store\",\"type\":\"ai_document\",\"index\":0}]]},\"Generate Embeddings\":{\"ai_embedding\":[[{\"node\":\"Pinecone Vector Store\",\"type\":\"ai_embedding\",\"index\":0}]]},\"Window Buffer Memory\":{\"ai_memory\":[[{\"node\":\"AI Agent\",\"type\":\"ai_memory\",\"index\":0}]]},\"When chat message received\":{\"main\":[[{\"node\":\"AI Agent\",\"type\":\"main\",\"index\":0}]]},\"Generate User Query Embedding\":{\"ai_embedding\":[[{\"node\":\"Pinecone Vector Store (Querying)\",\"type\":\"ai_embedding\",\"index\":0}]]},\"Pinecone Vector Store (Querying)\":{\"ai_vectorStore\":[[{\"node\":\"Vector Store Tool\",\"type\":\"ai_vectorStore\",\"index\":0}]]},\"Recursive Character Text Splitter\":{\"ai_textSplitter\":[[{\"node\":\"Default Data Loader\",\"type\":\"ai_textSplitter\",\"index\":0}]]},\"When clicking ‘Test workflow’\":{\"main\":[[{\"node\":\"HTTP Request\",\"type\":\"main\",\"index\":0}]]}}}",
  "readme": "This workflow demonstrates a Retrieval Augmented Generation (RAG) chatbot that lets you chat with the GitHub API Specification (documentation) using natural language. Built with n8n, OpenAI's LLMs and the Pinecone vector database, it provides accurate and context-aware responses to your questions about how to use the GitHub API.  \nYou could adapt this to any OpenAPI specification for any public or private API, thus creating a documentation chatbout that anyone in your company can use.\n\n## How it works:\n\n  * Data Ingestion: The workflow fetches the complete GitHub API OpenAPI 3 specification directly from the GitHub repository.  \nChunking and Embeddings: It splits the large API spec into smaller, manageable chunks. OpenAI's embedding models then generate vector embeddings for each chunk, capturing their semantic meaning.\n  * Vector Database Storage: These embeddings, along with the corresponding text chunks, are stored in a Pinecone vector database.\n  * Chat Interface and Query Processing: The workflow provides a simple chat interface. When you ask a question, it generates an embedding for your query using the same OpenAI model.\n  * Semantic Search and Retrieval: Pinecone is queried to find the most relevant text chunks from the API spec based on the query embedding.\n  * Response Generation: The retrieved chunks and your original question are fed to OpenAI's `gpt-4o-mini` LLM, which generates a concise, informative, and contextually relevant answer, including code snippets when applicable.\n\n\n\n## Set up steps:\n\n  * Create accounts: You'll need accounts with OpenAI and Pinecone.\n  * API keys: Obtain API keys for both services.  \nConfigure credentials: In your n8n environment, configure credentials for OpenAI and Pinecone using your API keys.\n  * Import the workflow: Import this workflow into your n8n instance.\n  * Pinecone Index: Ensure you have a Pinecone index named \"n8n-demo\" or adjust the workflow accordingly. The workflow is set up to work with this index out of the box.\n\n\n\n### Setup Time: Approximately 15-20 minutes.\n\n## Why use this workflow?\n\n  * Learn RAG in Action: This is a practical, hands-on example of how to build a RAG-powered chatbot.\n  * Adaptable Template: Easily modify this workflow to create chatbots for other APIs or knowledge bases.\n  * n8n Made Easy: See how n8n simplifies complex integrations between data sources, vector databases, and LLMs.\n\n\n",
  "readme_html": "<!--[--><div data-v-006f9244=\"\"><p>This workflow demonstrates a Retrieval Augmented Generation (RAG) chatbot that lets you chat with the GitHub API Specification (documentation) using natural language. Built with n8n, OpenAI's LLMs and the Pinecone vector database, it provides accurate and context-aware responses to your questions about how to use the GitHub API.<br>\nYou could adapt this to any OpenAPI specification for any public or private API, thus creating a documentation chatbout that anyone in your company can use.</p>\n<h2>How it works:</h2>\n<ul>\n<li>Data Ingestion: The workflow fetches the complete GitHub API OpenAPI 3 specification directly from the GitHub repository.<br>\nChunking and Embeddings: It splits the large API spec into smaller, manageable chunks. OpenAI's embedding models then generate vector embeddings for each chunk, capturing their semantic meaning.</li>\n<li>Vector Database Storage: These embeddings, along with the corresponding text chunks, are stored in a Pinecone vector database.</li>\n<li>Chat Interface and Query Processing: The workflow provides a simple chat interface. When you ask a question, it generates an embedding for your query using the same OpenAI model.</li>\n<li>Semantic Search and Retrieval: Pinecone is queried to find the most relevant text chunks from the API spec based on the query embedding.</li>\n<li>Response Generation: The retrieved chunks and your original question are fed to OpenAI's <code>gpt-4o-mini</code> LLM, which generates a concise, informative, and contextually relevant answer, including code snippets when applicable.</li>\n</ul>\n<h2>Set up steps:</h2>\n<ul>\n<li>Create accounts: You'll need accounts with OpenAI and Pinecone.</li>\n<li>API keys: Obtain API keys for both services.<br>\nConfigure credentials: In your n8n environment, configure credentials for OpenAI and Pinecone using your API keys.</li>\n<li>Import the workflow: Import this workflow into your n8n instance.</li>\n<li>Pinecone Index: Ensure you have a Pinecone index named \"n8n-demo\" or adjust the workflow accordingly. The workflow is set up to work with this index out of the box.</li>\n</ul>\n<h3>Setup Time: Approximately 15-20 minutes.</h3>\n<h2>Why use this workflow?</h2>\n<ul>\n<li>Learn RAG in Action: This is a practical, hands-on example of how to build a RAG-powered chatbot.</li>\n<li>Adaptable Template: Easily modify this workflow to create chatbots for other APIs or knowledge bases.</li>\n<li>n8n Made Easy: See how n8n simplifies complex integrations between data sources, vector databases, and LLMs.</li>\n</ul>\n</div><!--]-->",
  "readme_zh": "该工作流展示了一个基于检索增强生成（RAG）技术的聊天机器人，允许您通过自然语言与GitHub API规范（文档）进行对话。它由n8n、OpenAI大语言模型和Pinecone向量数据库构建而成，能针对GitHub API使用问题提供精准且上下文感知的响应。  \n您可将其适配至任何公开或私有API的OpenAPI规范，从而创建供企业内部全员使用的文档问答机器人。\n\n## 实现原理：\n\n  * 数据摄取：直接从GitHub仓库获取完整的OpenAPI 3规范文档  \n分块与向量化：将庞大的API规范切割为可管理的小块，通过OpenAI嵌入模型为每个文本块生成表征语义的向量\n  * 向量数据库存储：将文本块及其对应向量存入Pinecone向量数据库\n  * 聊天交互与查询处理：提供简易聊天界面，用户提问时使用相同OpenAI模型生成查询向量\n  * 语义搜索与检索：基于查询向量在Pinecone中查找API规范中最相关的文本块\n  * 响应生成：将检索到的文本块与原始问题输入OpenAI的`gpt-4o-mini`模型，生成简明扼要、包含上下文关联的答案（适用时自动附加代码片段）",
  "title_zh": "与GitHub API文档对话：基于Pinecone和OpenAI的RAG驱动聊天机器人",
  "publish_date_absolute": "2024-12-30",
  "publish_date_zh": "最后更新于4个月前",
  "workflow_json_zh": "{\n  \"id\": \"FD0bHNaehP3LzCNN\",\n  \"meta\": {\n    \"instanceId\": \"69133932b9ba8e1ef14816d0b63297bb44feb97c19f759b5d153ff6b0c59e18d\"\n  },\n  \"name\": \"Chat with GitHub OpenAPI Specification using RAG (Pinecone and OpenAI)\",\n  \"tags\": [],\n  \"nodes\": [\n    {\n      \"id\": \"362cb773-7540-4753-a401-e585cdf4af8a\",\n      \"name\": \"When clicking ‘Test workflow’\",\n      \"type\": \"n8n-nodes-base.manualTrigger\",\n      \"position\": [\n        0,\n        0\n      ],\n      \"parameters\": {},\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"45470036-cae6-48d0-ac66-addc8999e776\",\n      \"name\": \"HTTP Request\",\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"position\": [\n        300,\n        0\n      ],\n      \"parameters\": {\n        \"url\": \"https://raw.githubusercontent.com/github/rest-api-description/refs/heads/main/descriptions/api.github.com/api.github.com.json\",\n        \"options\": {}\n      },\n      \"typeVersion\": 4.2\n    },\n    {\n      \"id\": \"a9e65897-52c9-4941-bf49-e1a659e442ef\",\n      \"name\": \"Pinecone Vector Store\",\n      \"type\": \"@n8n/n8n-nodes-langchain.vectorStorePinecone\",\n      \"position\": [\n        520,\n        0\n      ],\n      \"parameters\": {\n        \"mode\": \"insert\",\n        \"options\": {},\n        \"pineconeIndex\": {\n          \"__rl\": true,\n          \"mode\": \"list\",\n          \"value\": \"n8n-demo\",\n          \"cachedResultName\": \"n8n-demo\"\n        }\n      },\n      \"credentials\": {\n        \"pineconeApi\": {\n          \"id\": \"bQTNry52ypGLqt47\",\n          \"name\": \"PineconeApi account\"\n        }\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"c2a2354b-5457-4ceb-abfc-9a58e8593b81\",\n      \"name\": \"Default Data Loader\",\n      \"type\": \"@n8n/n8n-nodes-langchain.documentDefaultDataLoader\",\n      \"position\": [\n        660,\n        180\n      ],\n      \"parameters\": {\n        \"options\": {}\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"7338d9ea-ae8f-46eb-807f-a15dc7639fc9\",\n      \"name\": \"Recursive Character Text Splitter\",\n      \"type\": \"@n8n/n8n-nodes-langchain.textSplitterRecursiveCharacterTextSplitter\",\n      \"position\": [\n        740,\n        360\n      ],\n      \"parameters\": {\n        \"options\": {}\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"44fd7a59-f208-4d5d-a22d-e9f8ca9badf1\",\n      \"name\": \"When chat message received\",\n      \"type\": \"@n8n/n8n-nodes-langchain.chatTrigger\",\n      \"position\": [\n        -20,\n        760\n      ],\n      \"webhookId\": \"089e38ab-4eee-4c34-aa5d-54cf4a8f53b7\",\n      \"parameters\": {\n        \"options\": {}\n      },\n      \"typeVersion\": 1.1\n    },\n    {\n      \"id\": \"51d819d6-70ff-428d-aa56-1d7e06490dee\",\n      \"name\": \"AI Agent\",\n      \"type\": \"@n8n/n8n-nodes-langchain.agent\",\n      \"position\": [\n        320,\n        760\n      ],\n      \"parameters\": {\n        \"options\": {\n          \"systemMessage\": \"You are a helpful assistant providing information about the GitHub API and how to use it based on the OpenAPI V3 specifications.\"\n        }\n      },\n      \"typeVersion\": 1.7\n    },\n    {\n      \"id\": \"aed548bf-7083-44ad-a3e0-163dee7423ef\",\n      \"name\": \"OpenAI Chat Model\",\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\n      \"position\": [\n        220,\n        980\n      ],\n      \"parameters\": {\n        \"options\": {}\n      },\n      \"credentials\": {\n        \"openAiApi\": {\n          \"id\": \"tQLWnWRzD8aebYvp\",\n          \"name\": \"OpenAi account\"\n        }\n      },\n      \"typeVersion\": 1.1\n    },\n    {\n      \"id\": \"dfe9f356-2225-4f4b-86c7-e56a230b4193\",\n      \"name\": \"Window Buffer Memory\",\n      \"type\": \"@n8n/n8n-nodes-langchain.memoryBufferWindow\",\n      \"position\": [\n        420,\n        1020\n      ],\n      \"parameters\": {},\n      \"typeVersion\": 1.3\n    },\n    {\n      \"id\": \"4cf672ee-13b8-4355-b8e0-c2e7381671bc\",\n      \"name\": \"Vector Store Tool\",\n      \"type\": \"@n8n/n8n-nodes-langchain.toolVectorStore\",\n      \"position\": [\n        580,\n        980\n      ],\n      \"parameters\": {\n        \"name\": \"GitHub_OpenAPI_Specification\",\n        \"description\": \"Use this tool to get information about the GitHub   API. This database contains OpenAPI v3 specifications.\"\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"1df7fb85-9d4a-4db5-9bed-41d28e2e4643\",\n      \"name\": \"OpenAI Chat Model1\",\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\n      \"position\": [\n        840,\n        1160\n      ],\n      \"parameters\": {\n        \"options\": {}\n      },\n      \"credentials\": {\n        \"openAiApi\": {\n          \"id\": \"tQLWnWRzD8aebYvp\",\n          \"name\": \"OpenAi account\"\n        }\n      },\n      \"typeVersion\": 1.1\n    },\n    {\n      \"id\": \"7b52ef7a-5935-451e-8747-efe16ce288af\",\n      \"name\": \"Sticky Note\",\n      \"type\": \"n8n-nodes-base.stickyNote\",\n      \"position\": [\n        -40,\n        -260\n      ],\n      \"parameters\": {\n        \"width\": 640,\n        \"height\": 200,\n        \"content\": \"## 在向量数据库中索引内容\\n该工作流环节负责提取内容、生成嵌入向量并将其发送至Pinecone向量存储库。\\n\\n通过HTTP请求从GitHub获取OpenAPI规范文件后，系统会将文件分割成多个文本块，利用OpenAI为每个文本块生成嵌入向量，最终将这些向量存入Pinecone向量数据库。\"\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"3508d602-56d4-4818-84eb-ca75cdeec1d0\",\n      \"name\": \"Sticky Note1\",\n      \"type\": \"n8n-nodes-base.stickyNote\",\n      \"position\": [\n        -20,\n        560\n      ],\n      \"parameters\": {\n        \"width\": 580,\n        \"content\": \"## 查询与响应生成\\n\\n该工作流模块负责实现聊天交互界面、查询向量数据库并生成相关响应。\\n\\n系统采用OpenAI GPT 4o-mini模型进行智能应答生成。\"\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"5a9808ef-4edd-4ec9-ba01-2fe50b2dbf4b\",\n      \"name\": \"Generate User Query Embedding\",\n      \"type\": \"@n8n/n8n-nodes-langchain.embeddingsOpenAi\",\n      \"position\": [\n        480,\n        1400\n      ],\n      \"parameters\": {\n        \"options\": {}\n      },\n      \"credentials\": {\n        \"openAiApi\": {\n          \"id\": \"tQLWnWRzD8aebYvp\",\n          \"name\": \"OpenAi account\"\n        }\n      },\n      \"typeVersion\": 1.2\n    },\n    {\n      \"id\": \"f703dc8e-9d4b-45e3-8994-789b3dfe8631\",\n      \"name\": \"Pinecone Vector Store (Querying)\",\n      \"type\": \"@n8n/n8n-nodes-langchain.vectorStorePinecone\",\n      \"position\": [\n        440,\n        1220\n      ],\n      \"parameters\": {\n        \"options\": {},\n        \"pineconeIndex\": {\n          \"__rl\": true,\n          \"mode\": \"list\",\n          \"value\": \"n8n-demo\",\n          \"cachedResultName\": \"n8n-demo\"\n        }\n      },\n      \"credentials\": {\n        \"pineconeApi\": {\n          \"id\": \"bQTNry52ypGLqt47\",\n          \"name\": \"PineconeApi account\"\n        }\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"ea64a7a5-1fa5-4938-83a9-271929733a8e\",\n      \"name\": \"Generate Embeddings\",\n      \"type\": \"@n8n/n8n-nodes-langchain.embeddingsOpenAi\",\n      \"position\": [\n        480,\n        220\n      ],\n      \"parameters\": {\n        \"options\": {}\n      },\n      \"credentials\": {\n        \"openAiApi\": {\n          \"id\": \"tQLWnWRzD8aebYvp\",\n          \"name\": \"OpenAi account\"\n        }\n      },\n      \"typeVersion\": 1.2\n    },\n    {\n      \"id\": \"65cbd4e3-91f6-441a-9ef1-528c3019e238\",\n      \"name\": \"Sticky Note2\",\n      \"type\": \"n8n-nodes-base.stickyNote\",\n      \"position\": [\n        -820,\n        -260\n      ],\n      \"parameters\": {\n        \"width\": 620,\n        \"height\": 320,\n        \"content\": \"## n8n中的RAG工作流\\n\\n这是一个展示如何运用RAG技术通过n8n构建聊天机器人的示例。该机器人专门用于回答GitHub API相关问题的文档咨询，采用OpenAI生成嵌入向量，gpt-4o-mini大语言模型生成响应，并使用Pinecone作为向量数据库。\\n\\n### 使用前准备\\n* 注册OpenAI和Pinecone账户\\n* 获取OpenAI和Pinecone的API密钥\\n* 在n8n中配置两者的访问凭证\\n* 确保已创建名为\\\"n8n-demo\\\"的Pinecone索引（或相应调整工作流配置）\"\n      },\n      \"typeVersion\": 1\n    }\n  ],\n  \"active\": false,\n  \"pinData\": {},\n  \"settings\": {\n    \"executionOrder\": \"v1\"\n  },\n  \"versionId\": \"2908105f-c20c-4183-bb9d-26e3559b9911\",\n  \"connections\": {\n    \"HTTP Request\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Pinecone Vector Store\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"OpenAI Chat Model\": {\n      \"ai_languageModel\": [\n        [\n          {\n            \"node\": \"AI Agent\",\n            \"type\": \"ai_languageModel\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Vector Store Tool\": {\n      \"ai_tool\": [\n        [\n          {\n            \"node\": \"AI Agent\",\n            \"type\": \"ai_tool\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"OpenAI Chat Model1\": {\n      \"ai_languageModel\": [\n        [\n          {\n            \"node\": \"Vector Store Tool\",\n            \"type\": \"ai_languageModel\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Default Data Loader\": {\n      \"ai_document\": [\n        [\n          {\n            \"node\": \"Pinecone Vector Store\",\n            \"type\": \"ai_document\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Generate Embeddings\": {\n      \"ai_embedding\": [\n        [\n          {\n            \"node\": \"Pinecone Vector Store\",\n            \"type\": \"ai_embedding\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Window Buffer Memory\": {\n      \"ai_memory\": [\n        [\n          {\n            \"node\": \"AI Agent\",\n            \"type\": \"ai_memory\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"When chat message received\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"AI Agent\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Generate User Query Embedding\": {\n      \"ai_embedding\": [\n        [\n          {\n            \"node\": \"Pinecone Vector Store (Querying)\",\n            \"type\": \"ai_embedding\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Pinecone Vector Store (Querying)\": {\n      \"ai_vectorStore\": [\n        [\n          {\n            \"node\": \"Vector Store Tool\",\n            \"type\": \"ai_vectorStore\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Recursive Character Text Splitter\": {\n      \"ai_textSplitter\": [\n        [\n          {\n            \"node\": \"Default Data Loader\",\n            \"type\": \"ai_textSplitter\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"When clicking ‘Test workflow’\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"HTTP Request\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    }\n  }\n}"
}