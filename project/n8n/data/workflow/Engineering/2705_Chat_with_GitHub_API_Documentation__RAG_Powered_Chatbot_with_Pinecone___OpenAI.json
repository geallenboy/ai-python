{
  "title": "Chat with GitHub API Documentation: RAG-Powered Chatbot with Pinecone & OpenAI",
  "url": "https://n8n.io/workflows/2705-chat-with-github-api-documentation-rag-powered-chatbot-with-pinecone-and-openai/",
  "category": "Engineering",
  "category_url": "https://n8n.io/workflows/categories/engineering/?count=20",
  "author": "Mihai Farcas",
  "publish_date": "Last update 4 months ago",
  "publish_date_absolute": "2025-01-06",
  "content": "",
  "workflow_json": "{\"id\":\"FD0bHNaehP3LzCNN\",\"meta\":{\"instanceId\":\"69133932b9ba8e1ef14816d0b63297bb44feb97c19f759b5d153ff6b0c59e18d\"},\"name\":\"Chat with GitHub OpenAPI Specification using RAG (Pinecone and OpenAI)\",\"tags\":[],\"nodes\":[{\"id\":\"362cb773-7540-4753-a401-e585cdf4af8a\",\"name\":\"When clicking ‘Test workflow’\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[0,0],\"parameters\":{},\"typeVersion\":1},{\"id\":\"45470036-cae6-48d0-ac66-addc8999e776\",\"name\":\"HTTP Request\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[300,0],\"parameters\":{\"url\":\"https://raw.githubusercontent.com/github/rest-api-description/refs/heads/main/descriptions/api.github.com/api.github.com.json\",\"options\":{}},\"typeVersion\":4.2},{\"id\":\"a9e65897-52c9-4941-bf49-e1a659e442ef\",\"name\":\"Pinecone Vector Store\",\"type\":\"@n8n/n8n-nodes-langchain.vectorStorePinecone\",\"position\":[520,0],\"parameters\":{\"mode\":\"insert\",\"options\":{},\"pineconeIndex\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"n8n-demo\",\"cachedResultName\":\"n8n-demo\"}},\"credentials\":{\"pineconeApi\":{\"id\":\"bQTNry52ypGLqt47\",\"name\":\"PineconeApi account\"}},\"typeVersion\":1},{\"id\":\"c2a2354b-5457-4ceb-abfc-9a58e8593b81\",\"name\":\"Default Data Loader\",\"type\":\"@n8n/n8n-nodes-langchain.documentDefaultDataLoader\",\"position\":[660,180],\"parameters\":{\"options\":{}},\"typeVersion\":1},{\"id\":\"7338d9ea-ae8f-46eb-807f-a15dc7639fc9\",\"name\":\"Recursive Character Text Splitter\",\"type\":\"@n8n/n8n-nodes-langchain.textSplitterRecursiveCharacterTextSplitter\",\"position\":[740,360],\"parameters\":{\"options\":{}},\"typeVersion\":1},{\"id\":\"44fd7a59-f208-4d5d-a22d-e9f8ca9badf1\",\"name\":\"When chat message received\",\"type\":\"@n8n/n8n-nodes-langchain.chatTrigger\",\"position\":[-20,760],\"webhookId\":\"089e38ab-4eee-4c34-aa5d-54cf4a8f53b7\",\"parameters\":{\"options\":{}},\"typeVersion\":1.1},{\"id\":\"51d819d6-70ff-428d-aa56-1d7e06490dee\",\"name\":\"AI Agent\",\"type\":\"@n8n/n8n-nodes-langchain.agent\",\"position\":[320,760],\"parameters\":{\"options\":{\"systemMessage\":\"You are a helpful assistant providing information about the GitHub API and how to use it based on the OpenAPI V3 specifications.\"}},\"typeVersion\":1.7},{\"id\":\"aed548bf-7083-44ad-a3e0-163dee7423ef\",\"name\":\"OpenAI Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[220,980],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"tQLWnWRzD8aebYvp\",\"name\":\"OpenAi account\"}},\"typeVersion\":1.1},{\"id\":\"dfe9f356-2225-4f4b-86c7-e56a230b4193\",\"name\":\"Window Buffer Memory\",\"type\":\"@n8n/n8n-nodes-langchain.memoryBufferWindow\",\"position\":[420,1020],\"parameters\":{},\"typeVersion\":1.3},{\"id\":\"4cf672ee-13b8-4355-b8e0-c2e7381671bc\",\"name\":\"Vector Store Tool\",\"type\":\"@n8n/n8n-nodes-langchain.toolVectorStore\",\"position\":[580,980],\"parameters\":{\"name\":\"GitHub_OpenAPI_Specification\",\"description\":\"Use this tool to get information about the GitHub   API. This database contains OpenAPI v3 specifications.\"},\"typeVersion\":1},{\"id\":\"1df7fb85-9d4a-4db5-9bed-41d28e2e4643\",\"name\":\"OpenAI Chat Model1\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\"position\":[840,1160],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"tQLWnWRzD8aebYvp\",\"name\":\"OpenAi account\"}},\"typeVersion\":1.1},{\"id\":\"7b52ef7a-5935-451e-8747-efe16ce288af\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-40,-260],\"parameters\":{\"width\":640,\"height\":200,\"content\":\"## Indexing content in the vector database\\nThis part of the workflow is responsible for extracting content, generating embeddings and sending them to the Pinecone vector store.\\n\\nIt requests the OpenAPI specifications from GitHub using a HTTP request. Then, it splits the file in chunks, generating embeddings for each chunk using OpenAI, and saving them in Pinecone vector DB.\"},\"typeVersion\":1},{\"id\":\"3508d602-56d4-4818-84eb-ca75cdeec1d0\",\"name\":\"Sticky Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-20,560],\"parameters\":{\"width\":580,\"content\":\"## Querying and response generation \\n\\nThis part of the workflow is responsible for the chat interface, querying the vector store and generating relevant responses.\\n\\nIt uses OpenAI GPT 4o-mini to generate responses.\"},\"typeVersion\":1},{\"id\":\"5a9808ef-4edd-4ec9-ba01-2fe50b2dbf4b\",\"name\":\"Generate User Query Embedding\",\"type\":\"@n8n/n8n-nodes-langchain.embeddingsOpenAi\",\"position\":[480,1400],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"tQLWnWRzD8aebYvp\",\"name\":\"OpenAi account\"}},\"typeVersion\":1.2},{\"id\":\"f703dc8e-9d4b-45e3-8994-789b3dfe8631\",\"name\":\"Pinecone Vector Store (Querying)\",\"type\":\"@n8n/n8n-nodes-langchain.vectorStorePinecone\",\"position\":[440,1220],\"parameters\":{\"options\":{},\"pineconeIndex\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"n8n-demo\",\"cachedResultName\":\"n8n-demo\"}},\"credentials\":{\"pineconeApi\":{\"id\":\"bQTNry52ypGLqt47\",\"name\":\"PineconeApi account\"}},\"typeVersion\":1},{\"id\":\"ea64a7a5-1fa5-4938-83a9-271929733a8e\",\"name\":\"Generate Embeddings\",\"type\":\"@n8n/n8n-nodes-langchain.embeddingsOpenAi\",\"position\":[480,220],\"parameters\":{\"options\":{}},\"credentials\":{\"openAiApi\":{\"id\":\"tQLWnWRzD8aebYvp\",\"name\":\"OpenAi account\"}},\"typeVersion\":1.2},{\"id\":\"65cbd4e3-91f6-441a-9ef1-528c3019e238\",\"name\":\"Sticky Note2\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-820,-260],\"parameters\":{\"width\":620,\"height\":320,\"content\":\"## RAG workflow in n8n\\n\\nThis is an example of how to use RAG techniques to create a chatbot with n8n. It is an API documentation chatbot that can answer questions about the GitHub API. It uses OpenAI for generating embeddings, the gpt-4o-mini LLM for generating responses and Pinecone as a vector database.\\n\\n### Before using this template\\n* create OpenAI and Pinecone accounts\\n* obtain API keys OpenAI and Pinecone \\n* configure credentials in n8n for both\\n* ensure you have a Pinecone index named \\\"n8n-demo\\\" or adjust the workflow accordingly.\"},\"typeVersion\":1}],\"active\":false,\"pinData\":{},\"settings\":{\"executionOrder\":\"v1\"},\"versionId\":\"2908105f-c20c-4183-bb9d-26e3559b9911\",\"connections\":{\"HTTP Request\":{\"main\":[[{\"node\":\"Pinecone Vector Store\",\"type\":\"main\",\"index\":0}]]},\"OpenAI Chat Model\":{\"ai_languageModel\":[[{\"node\":\"AI Agent\",\"type\":\"ai_languageModel\",\"index\":0}]]},\"Vector Store Tool\":{\"ai_tool\":[[{\"node\":\"AI Agent\",\"type\":\"ai_tool\",\"index\":0}]]},\"OpenAI Chat Model1\":{\"ai_languageModel\":[[{\"node\":\"Vector Store Tool\",\"type\":\"ai_languageModel\",\"index\":0}]]},\"Default Data Loader\":{\"ai_document\":[[{\"node\":\"Pinecone Vector Store\",\"type\":\"ai_document\",\"index\":0}]]},\"Generate Embeddings\":{\"ai_embedding\":[[{\"node\":\"Pinecone Vector Store\",\"type\":\"ai_embedding\",\"index\":0}]]},\"Window Buffer Memory\":{\"ai_memory\":[[{\"node\":\"AI Agent\",\"type\":\"ai_memory\",\"index\":0}]]},\"When chat message received\":{\"main\":[[{\"node\":\"AI Agent\",\"type\":\"main\",\"index\":0}]]},\"Generate User Query Embedding\":{\"ai_embedding\":[[{\"node\":\"Pinecone Vector Store (Querying)\",\"type\":\"ai_embedding\",\"index\":0}]]},\"Pinecone Vector Store (Querying)\":{\"ai_vectorStore\":[[{\"node\":\"Vector Store Tool\",\"type\":\"ai_vectorStore\",\"index\":0}]]},\"Recursive Character Text Splitter\":{\"ai_textSplitter\":[[{\"node\":\"Default Data Loader\",\"type\":\"ai_textSplitter\",\"index\":0}]]},\"When clicking ‘Test workflow’\":{\"main\":[[{\"node\":\"HTTP Request\",\"type\":\"main\",\"index\":0}]]}}}",
  "readme": "This workflow demonstrates a Retrieval Augmented Generation (RAG) chatbot that lets you chat with the GitHub API Specification (documentation) using natural language. Built with n8n, OpenAI's LLMs and the Pinecone vector database, it provides accurate and context-aware responses to your questions about how to use the GitHub API.  \nYou could adapt this to any OpenAPI specification for any public or private API, thus creating a documentation chatbout that anyone in your company can use.\n\n## How it works:\n\n  * Data Ingestion: The workflow fetches the complete GitHub API OpenAPI 3 specification directly from the GitHub repository.  \nChunking and Embeddings: It splits the large API spec into smaller, manageable chunks. OpenAI's embedding models then generate vector embeddings for each chunk, capturing their semantic meaning.\n  * Vector Database Storage: These embeddings, along with the corresponding text chunks, are stored in a Pinecone vector database.\n  * Chat Interface and Query Processing: The workflow provides a simple chat interface. When you ask a question, it generates an embedding for your query using the same OpenAI model.\n  * Semantic Search and Retrieval: Pinecone is queried to find the most relevant text chunks from the API spec based on the query embedding.\n  * Response Generation: The retrieved chunks and your original question are fed to OpenAI's `gpt-4o-mini` LLM, which generates a concise, informative, and contextually relevant answer, including code snippets when applicable.\n\n\n\n## Set up steps:\n\n  * Create accounts: You'll need accounts with OpenAI and Pinecone.\n  * API keys: Obtain API keys for both services.  \nConfigure credentials: In your n8n environment, configure credentials for OpenAI and Pinecone using your API keys.\n  * Import the workflow: Import this workflow into your n8n instance.\n  * Pinecone Index: Ensure you have a Pinecone index named \"n8n-demo\" or adjust the workflow accordingly. The workflow is set up to work with this index out of the box.\n\n\n\n### Setup Time: Approximately 15-20 minutes.\n\n## Why use this workflow?\n\n  * Learn RAG in Action: This is a practical, hands-on example of how to build a RAG-powered chatbot.\n  * Adaptable Template: Easily modify this workflow to create chatbots for other APIs or knowledge bases.\n  * n8n Made Easy: See how n8n simplifies complex integrations between data sources, vector databases, and LLMs.\n\n\n",
  "readme_html": "<!--[--><div data-v-006f9244=\"\"><p>This workflow demonstrates a Retrieval Augmented Generation (RAG) chatbot that lets you chat with the GitHub API Specification (documentation) using natural language. Built with n8n, OpenAI's LLMs and the Pinecone vector database, it provides accurate and context-aware responses to your questions about how to use the GitHub API.<br>\nYou could adapt this to any OpenAPI specification for any public or private API, thus creating a documentation chatbout that anyone in your company can use.</p>\n<h2>How it works:</h2>\n<ul>\n<li>Data Ingestion: The workflow fetches the complete GitHub API OpenAPI 3 specification directly from the GitHub repository.<br>\nChunking and Embeddings: It splits the large API spec into smaller, manageable chunks. OpenAI's embedding models then generate vector embeddings for each chunk, capturing their semantic meaning.</li>\n<li>Vector Database Storage: These embeddings, along with the corresponding text chunks, are stored in a Pinecone vector database.</li>\n<li>Chat Interface and Query Processing: The workflow provides a simple chat interface. When you ask a question, it generates an embedding for your query using the same OpenAI model.</li>\n<li>Semantic Search and Retrieval: Pinecone is queried to find the most relevant text chunks from the API spec based on the query embedding.</li>\n<li>Response Generation: The retrieved chunks and your original question are fed to OpenAI's <code>gpt-4o-mini</code> LLM, which generates a concise, informative, and contextually relevant answer, including code snippets when applicable.</li>\n</ul>\n<h2>Set up steps:</h2>\n<ul>\n<li>Create accounts: You'll need accounts with OpenAI and Pinecone.</li>\n<li>API keys: Obtain API keys for both services.<br>\nConfigure credentials: In your n8n environment, configure credentials for OpenAI and Pinecone using your API keys.</li>\n<li>Import the workflow: Import this workflow into your n8n instance.</li>\n<li>Pinecone Index: Ensure you have a Pinecone index named \"n8n-demo\" or adjust the workflow accordingly. The workflow is set up to work with this index out of the box.</li>\n</ul>\n<h3>Setup Time: Approximately 15-20 minutes.</h3>\n<h2>Why use this workflow?</h2>\n<ul>\n<li>Learn RAG in Action: This is a practical, hands-on example of how to build a RAG-powered chatbot.</li>\n<li>Adaptable Template: Easily modify this workflow to create chatbots for other APIs or knowledge bases.</li>\n<li>n8n Made Easy: See how n8n simplifies complex integrations between data sources, vector databases, and LLMs.</li>\n</ul>\n</div><!--]-->",
  "readme_zh": "This workflow demonstrates a Retrieval Augmented Generation (RAG) chatbot that lets you chat with the GitHub API Specification (documentation) using natural language. Built with n8n, OpenAI's LLMs and the Pinecone vector database, it provides accurate and context-aware responses to your questions about how to use the GitHub API.  \nYou could adapt this to any OpenAPI specification for any public or private API, thus creating a documentation chatbout that anyone in your company can use.\n\n## How it works:\n\n  * Data Ingestion: The workflow fetches the complete GitHub API OpenAPI 3 specification directly from the GitHub repository.  \nChunking and Embeddings: It splits the large API spec into smaller, manageable chunks. OpenAI's embedding models then generate vector embeddings for each chunk, capturing their semantic meaning.\n  * Vector Database Storage: These embeddings, along with the corresponding text chunks, are stored in a Pinecone vector database.\n  * Chat Interface and Query Processing: The workflow provides a simple chat interface. When you ask a question, it generates an embedding for your query using the same OpenAI model.\n  * Semantic Search and Retrieval: Pinecone is queried to find the most relevant text chunks from the API spec based on the query embedding.\n  * Response Generation: The retrieved chunks and your original question are fed to OpenAI's `gpt-4o-mini` LLM, which generates a concise, informative, and contextually relevant answer, including code snippets when applicable.\n\n\n\n## Set up steps:\n\n  * Create accounts: You'll need accounts with OpenAI and Pinecone.\n  * API keys: Obtain API keys for both services.  \nConfigure credentials: In your n8n environment, configure credentials for OpenAI and Pinecone using your API keys.\n  * Import the workflow: Import this workflow into your n8n instance.\n  * Pinecone Index: Ensure you have a Pinecone index named \"n8n-demo\" or adjust the workflow accordingly. The workflow is set up to work with this index out of the box.\n\n\n\n### Setup Time: Approximately 15-20 minutes.\n\n## Why use this workflow?\n\n  * Learn RAG in Action: This is a practical, hands-on example of how to build a RAG-powered chatbot.\n  * Adaptable Template: Easily modify this workflow to create chatbots for other APIs or knowledge bases.\n  * n8n Made Easy: See how n8n simplifies complex integrations between data sources, vector databases, and LLMs.\n\n\n",
  "title_zh": "Chat with GitHub API Documentation: RAG-Powered Chatbot with Pinecone & OpenAI",
  "publish_date_zh": "Last update 4 months ago",
  "workflow_json_zh": "{\n  \"id\": \"FD0bHNaehP3LzCNN\",\n  \"meta\": {\n    \"instanceId\": \"69133932b9ba8e1ef14816d0b63297bb44feb97c19f759b5d153ff6b0c59e18d\"\n  },\n  \"name\": \"Chat with GitHub OpenAPI Specification using RAG (Pinecone and OpenAI)\",\n  \"tags\": [],\n  \"nodes\": [\n    {\n      \"id\": \"362cb773-7540-4753-a401-e585cdf4af8a\",\n      \"name\": \"When clicking ‘Test workflow’\",\n      \"type\": \"n8n-nodes-base.manualTrigger\",\n      \"position\": [\n        0,\n        0\n      ],\n      \"parameters\": {},\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"45470036-cae6-48d0-ac66-addc8999e776\",\n      \"name\": \"HTTP Request\",\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"position\": [\n        300,\n        0\n      ],\n      \"parameters\": {\n        \"url\": \"https://raw.githubusercontent.com/github/rest-api-description/refs/heads/main/descriptions/api.github.com/api.github.com.json\",\n        \"options\": {}\n      },\n      \"typeVersion\": 4.2\n    },\n    {\n      \"id\": \"a9e65897-52c9-4941-bf49-e1a659e442ef\",\n      \"name\": \"Pinecone Vector Store\",\n      \"type\": \"@n8n/n8n-nodes-langchain.vectorStorePinecone\",\n      \"position\": [\n        520,\n        0\n      ],\n      \"parameters\": {\n        \"mode\": \"insert\",\n        \"options\": {},\n        \"pineconeIndex\": {\n          \"__rl\": true,\n          \"mode\": \"list\",\n          \"value\": \"n8n-demo\",\n          \"cachedResultName\": \"n8n-demo\"\n        }\n      },\n      \"credentials\": {\n        \"pineconeApi\": {\n          \"id\": \"bQTNry52ypGLqt47\",\n          \"name\": \"PineconeApi account\"\n        }\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"c2a2354b-5457-4ceb-abfc-9a58e8593b81\",\n      \"name\": \"Default Data Loader\",\n      \"type\": \"@n8n/n8n-nodes-langchain.documentDefaultDataLoader\",\n      \"position\": [\n        660,\n        180\n      ],\n      \"parameters\": {\n        \"options\": {}\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"7338d9ea-ae8f-46eb-807f-a15dc7639fc9\",\n      \"name\": \"Recursive Character Text Splitter\",\n      \"type\": \"@n8n/n8n-nodes-langchain.textSplitterRecursiveCharacterTextSplitter\",\n      \"position\": [\n        740,\n        360\n      ],\n      \"parameters\": {\n        \"options\": {}\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"44fd7a59-f208-4d5d-a22d-e9f8ca9badf1\",\n      \"name\": \"When chat message received\",\n      \"type\": \"@n8n/n8n-nodes-langchain.chatTrigger\",\n      \"position\": [\n        -20,\n        760\n      ],\n      \"webhookId\": \"089e38ab-4eee-4c34-aa5d-54cf4a8f53b7\",\n      \"parameters\": {\n        \"options\": {}\n      },\n      \"typeVersion\": 1.1\n    },\n    {\n      \"id\": \"51d819d6-70ff-428d-aa56-1d7e06490dee\",\n      \"name\": \"AI Agent\",\n      \"type\": \"@n8n/n8n-nodes-langchain.agent\",\n      \"position\": [\n        320,\n        760\n      ],\n      \"parameters\": {\n        \"options\": {\n          \"systemMessage\": \"You are a helpful assistant providing information about the GitHub API and how to use it based on the OpenAPI V3 specifications.\"\n        }\n      },\n      \"typeVersion\": 1.7\n    },\n    {\n      \"id\": \"aed548bf-7083-44ad-a3e0-163dee7423ef\",\n      \"name\": \"OpenAI Chat Model\",\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\n      \"position\": [\n        220,\n        980\n      ],\n      \"parameters\": {\n        \"options\": {}\n      },\n      \"credentials\": {\n        \"openAiApi\": {\n          \"id\": \"tQLWnWRzD8aebYvp\",\n          \"name\": \"OpenAi account\"\n        }\n      },\n      \"typeVersion\": 1.1\n    },\n    {\n      \"id\": \"dfe9f356-2225-4f4b-86c7-e56a230b4193\",\n      \"name\": \"Window Buffer Memory\",\n      \"type\": \"@n8n/n8n-nodes-langchain.memoryBufferWindow\",\n      \"position\": [\n        420,\n        1020\n      ],\n      \"parameters\": {},\n      \"typeVersion\": 1.3\n    },\n    {\n      \"id\": \"4cf672ee-13b8-4355-b8e0-c2e7381671bc\",\n      \"name\": \"Vector Store Tool\",\n      \"type\": \"@n8n/n8n-nodes-langchain.toolVectorStore\",\n      \"position\": [\n        580,\n        980\n      ],\n      \"parameters\": {\n        \"name\": \"GitHub_OpenAPI_Specification\",\n        \"description\": \"Use this tool to get information about the GitHub   API. This database contains OpenAPI v3 specifications.\"\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"1df7fb85-9d4a-4db5-9bed-41d28e2e4643\",\n      \"name\": \"OpenAI Chat Model1\",\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatOpenAi\",\n      \"position\": [\n        840,\n        1160\n      ],\n      \"parameters\": {\n        \"options\": {}\n      },\n      \"credentials\": {\n        \"openAiApi\": {\n          \"id\": \"tQLWnWRzD8aebYvp\",\n          \"name\": \"OpenAi account\"\n        }\n      },\n      \"typeVersion\": 1.1\n    },\n    {\n      \"id\": \"7b52ef7a-5935-451e-8747-efe16ce288af\",\n      \"name\": \"Sticky Note\",\n      \"type\": \"n8n-nodes-base.stickyNote\",\n      \"position\": [\n        -40,\n        -260\n      ],\n      \"parameters\": {\n        \"width\": 640,\n        \"height\": 200,\n        \"content\": \"## Indexing content in the vector database\\nThis part of the workflow is responsible for extracting content, generating embeddings and sending them to the Pinecone vector store.\\n\\nIt requests the OpenAPI specifications from GitHub using a HTTP request. Then, it splits the file in chunks, generating embeddings for each chunk using OpenAI, and saving them in Pinecone vector DB.\"\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"3508d602-56d4-4818-84eb-ca75cdeec1d0\",\n      \"name\": \"Sticky Note1\",\n      \"type\": \"n8n-nodes-base.stickyNote\",\n      \"position\": [\n        -20,\n        560\n      ],\n      \"parameters\": {\n        \"width\": 580,\n        \"content\": \"## 查询与响应生成\\n\\n该工作流模块负责实现聊天交互界面、查询向量数据库以及生成相关回复功能。\\n\\n系统采用OpenAI GPT 4o-mini模型进行智能回复生成。\"\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"5a9808ef-4edd-4ec9-ba01-2fe50b2dbf4b\",\n      \"name\": \"Generate User Query Embedding\",\n      \"type\": \"@n8n/n8n-nodes-langchain.embeddingsOpenAi\",\n      \"position\": [\n        480,\n        1400\n      ],\n      \"parameters\": {\n        \"options\": {}\n      },\n      \"credentials\": {\n        \"openAiApi\": {\n          \"id\": \"tQLWnWRzD8aebYvp\",\n          \"name\": \"OpenAi account\"\n        }\n      },\n      \"typeVersion\": 1.2\n    },\n    {\n      \"id\": \"f703dc8e-9d4b-45e3-8994-789b3dfe8631\",\n      \"name\": \"Pinecone Vector Store (Querying)\",\n      \"type\": \"@n8n/n8n-nodes-langchain.vectorStorePinecone\",\n      \"position\": [\n        440,\n        1220\n      ],\n      \"parameters\": {\n        \"options\": {},\n        \"pineconeIndex\": {\n          \"__rl\": true,\n          \"mode\": \"list\",\n          \"value\": \"n8n-demo\",\n          \"cachedResultName\": \"n8n-demo\"\n        }\n      },\n      \"credentials\": {\n        \"pineconeApi\": {\n          \"id\": \"bQTNry52ypGLqt47\",\n          \"name\": \"PineconeApi account\"\n        }\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"ea64a7a5-1fa5-4938-83a9-271929733a8e\",\n      \"name\": \"Generate Embeddings\",\n      \"type\": \"@n8n/n8n-nodes-langchain.embeddingsOpenAi\",\n      \"position\": [\n        480,\n        220\n      ],\n      \"parameters\": {\n        \"options\": {}\n      },\n      \"credentials\": {\n        \"openAiApi\": {\n          \"id\": \"tQLWnWRzD8aebYvp\",\n          \"name\": \"OpenAi account\"\n        }\n      },\n      \"typeVersion\": 1.2\n    },\n    {\n      \"id\": \"65cbd4e3-91f6-441a-9ef1-528c3019e238\",\n      \"name\": \"Sticky Note2\",\n      \"type\": \"n8n-nodes-base.stickyNote\",\n      \"position\": [\n        -820,\n        -260\n      ],\n      \"parameters\": {\n        \"width\": 620,\n        \"height\": 320,\n        \"content\": \"## 在n8n中实现RAG工作流\\n\\n这是一个使用RAG技术通过n8n构建聊天机器人的示例。该机器人专门用于回答GitHub API相关文档问题，采用OpenAI生成嵌入向量，gpt-4o-mini大语言模型生成回答，并以Pinecone作为向量数据库。\\n\\n### 使用前准备\\n* 注册OpenAI和Pinecone账户\\n* 获取OpenAI和Pinecone的API密钥\\n* 在n8n中配置这两个服务的凭证\\n* 确保已创建名为\\\"n8n-demo\\\"的Pinecone索引，或根据实际情况调整工作流配置\"\n      },\n      \"typeVersion\": 1\n    }\n  ],\n  \"active\": false,\n  \"pinData\": {},\n  \"settings\": {\n    \"executionOrder\": \"v1\"\n  },\n  \"versionId\": \"2908105f-c20c-4183-bb9d-26e3559b9911\",\n  \"connections\": {\n    \"HTTP Request\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Pinecone Vector Store\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"OpenAI Chat Model\": {\n      \"ai_languageModel\": [\n        [\n          {\n            \"node\": \"AI Agent\",\n            \"type\": \"ai_languageModel\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Vector Store Tool\": {\n      \"ai_tool\": [\n        [\n          {\n            \"node\": \"AI Agent\",\n            \"type\": \"ai_tool\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"OpenAI Chat Model1\": {\n      \"ai_languageModel\": [\n        [\n          {\n            \"node\": \"Vector Store Tool\",\n            \"type\": \"ai_languageModel\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Default Data Loader\": {\n      \"ai_document\": [\n        [\n          {\n            \"node\": \"Pinecone Vector Store\",\n            \"type\": \"ai_document\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Generate Embeddings\": {\n      \"ai_embedding\": [\n        [\n          {\n            \"node\": \"Pinecone Vector Store\",\n            \"type\": \"ai_embedding\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Window Buffer Memory\": {\n      \"ai_memory\": [\n        [\n          {\n            \"node\": \"AI Agent\",\n            \"type\": \"ai_memory\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"When chat message received\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"AI Agent\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Generate User Query Embedding\": {\n      \"ai_embedding\": [\n        [\n          {\n            \"node\": \"Pinecone Vector Store (Querying)\",\n            \"type\": \"ai_embedding\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Pinecone Vector Store (Querying)\": {\n      \"ai_vectorStore\": [\n        [\n          {\n            \"node\": \"Vector Store Tool\",\n            \"type\": \"ai_vectorStore\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Recursive Character Text Splitter\": {\n      \"ai_textSplitter\": [\n        [\n          {\n            \"node\": \"Default Data Loader\",\n            \"type\": \"ai_textSplitter\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"When clicking ‘Test workflow’\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"HTTP Request\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    }\n  }\n}"
}