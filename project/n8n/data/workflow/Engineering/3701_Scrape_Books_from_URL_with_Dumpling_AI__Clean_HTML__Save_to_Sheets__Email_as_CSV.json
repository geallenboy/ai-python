{
  "title": "Scrape Books from URL with Dumpling AI, Clean HTML, Save to Sheets, Email as CSV",
  "url": "https://n8n.io/workflows/3701-scrape-books-from-url-with-dumpling-ai-clean-html-save-to-sheets-email-as-csv/",
  "category": "Engineering",
  "category_url": "https://n8n.io/workflows/categories/engineering/?count=20",
  "author": "Yang",
  "publish_date": "Last update 2 days ago",
  "publish_date_absolute": "",
  "content": "",
  "workflow_json": "{\"id\":\"DswhuYzoemjA6iNN\",\"meta\":{\"instanceId\":\"a1ae5c8dc6c65e674f9c3947d083abcc749ef2546dff9f4ff01de4d6a36ebfe6\",\"templateCredsSetupCompleted\":true},\"name\":\"Scrape Books from URL with Dumpling AI, Clean HTML, Save to Sheets, Email as CSV\",\"tags\":[{\"id\":\"TlcNkmb96fUfZ2eA\",\"name\":\"Tutorials\",\"createdAt\":\"2025-04-15T17:02:00.249Z\",\"updatedAt\":\"2025-04-15T17:02:00.249Z\"}],\"nodes\":[{\"id\":\"2e4f64a5-353c-4dd3-9822-62df795d4940\",\"name\":\"Convert to CSV File\",\"type\":\"n8n-nodes-base.convertToFile\",\"position\":[1640,340],\"parameters\":{\"options\":{}},\"typeVersion\":1.1},{\"id\":\"472442d3-a691-4310-93f8-019579d0c473\",\"name\":\"Extract all books from the page\",\"type\":\"n8n-nodes-base.html\",\"position\":[760,340],\"parameters\":{\"options\":{},\"operation\":\"extractHtmlContent\",\"dataPropertyName\":\"content\",\"extractionValues\":{\"values\":[{\"key\":\"books\",\"cssSelector\":\".row > li\",\"returnArray\":true,\"returnValue\":\"html\"}]}},\"typeVersion\":1.2},{\"id\":\"92765257-d64d-47c9-bd57-50914342138b\",\"name\":\"Sort by price\",\"type\":\"n8n-nodes-base.sort\",\"position\":[1420,340],\"parameters\":{\"options\":{},\"sortFieldsUi\":{\"sortField\":[{\"order\":\"descending\",\"fieldName\":\"price\"}]}},\"typeVersion\":1},{\"id\":\"efc2f33f-1bef-4906-b3b7-b02868080a54\",\"name\":\"Extract individual book price\",\"type\":\"n8n-nodes-base.html\",\"position\":[1200,340],\"parameters\":{\"options\":{},\"operation\":\"extractHtmlContent\",\"dataPropertyName\":\"books\",\"extractionValues\":{\"values\":[{\"key\":\"title\",\"attribute\":\"title\",\"cssSelector\":\"h3 > a\",\"returnValue\":\"attribute\"},{\"key\":\"price\",\"cssSelector\":\".price_color\"}]}},\"typeVersion\":1.2},{\"id\":\"74c7c3af-d63c-4b6c-95a0-15f45b19134b\",\"name\":\"Send CSV via e-mail\",\"type\":\"n8n-nodes-base.gmail\",\"position\":[1860,340],\"webhookId\":\"40f2d609-52ed-40bf-b190-1f1cebbe3fb7\",\"parameters\":{\"sendTo\":\"\",\"message\":\"Hey, here's the scraped data from the online bookstore!\",\"options\":{\"attachmentsUi\":{\"attachmentsBinary\":[{}]}},\"subject\":\"bookstore csv\",\"emailType\":\"text\"},\"credentials\":{\"gmailOAuth2\":{\"id\":\"j70r3RTMED1pgN3R\",\"name\":\"Gmail account 2\"}},\"typeVersion\":2.1},{\"id\":\"95c7998b-ece0-4dea-b99e-97ac22fb8a59\",\"name\":\"Sticky Note3\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[140,-260],\"parameters\":{\"width\":619,\"height\":297,\"content\":\"### Scrape Books from URL with Dumpling AI, Clean HTML, Save to Sheets, Email as CSV\\n\\nüìå This workflow scrapes book data from a website, turns it into a CSV, saves it, and sends it by email.\\n\\nüîß It starts from a Google Sheets trigger, fetches the page using DumplingAI, extracts books, sorts by price, and emails the CSV.\\n\\n‚úÖ Make sure APIs for Gmail, Sheets & Drive are enabled in Google Cloud. Update the URL in the \\\"Fetch website content\\\" node.\\n\"},\"typeVersion\":1},{\"id\":\"f599028a-49a9-4b85-b484-5abf1229e373\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[140,60],\"parameters\":{\"color\":4,\"width\":900,\"height\":300,\"content\":\"### üîÅ Trigger to Raw Book HTML\\n\\n1. **Google Sheets Trigger**  \\n   Watches a sheet for new row entries. Once a new URL is added, the workflow starts.\\n\\n2. **Fetch Website Content (Dumpling AI)**  \\n   Makes an HTTP POST request to Dumpling AI to scrape and return the full HTML of the target URL.\\n\\n3. **Extract All Books**  \\n   Uses CSS selectors to isolate the list items (`li.row > li`) containing book entries.\\n\\n4. **Split Out Node**  \\n   Breaks the array of book HTML blocks into individual items, so each book can be processed separately in the next steps.\\n\"},\"typeVersion\":1},{\"id\":\"bc6ab72c-de03-4e79-9da0-ca12ddf31811\",\"name\":\"Sticky Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1140,60],\"parameters\":{\"color\":6,\"width\":840,\"height\":300,\"content\":\"### üì¶ Parse, Sort, Export & Email\\n\\n5. **Extract Individual Book Data**  \\n   From each book, extract the title (`<h3>a` title attribute) and price (`.price_color` content).\\n\\n6. **Sort by Price**  \\n   Organizes the extracted data in descending order using the price field.\\n\\n7. **Convert to CSV File**  \\n   Transforms the sorted JSON data into a downloadable CSV file format.\\n\\n8. **Send CSV via Gmail**  \\n   Automatically sends an email with the CSV file attached to the predefined address.\\n\"},\"typeVersion\":1},{\"id\":\"a1246b4e-212f-4bd3-970b-b0ff8db2f834\",\"name\":\"Trigger- Watches For new URL in Spreadsheet\",\"type\":\"n8n-nodes-base.googleSheetsTrigger\",\"position\":[320,340],\"parameters\":{\"event\":\"rowAdded\",\"options\":{},\"pollTimes\":{\"item\":[{\"mode\":\"everyMinute\"}]},\"sheetName\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"\",\"cachedResultUrl\":\"https://docs.google.com/spreadsheets/d/1pb4WLqv2EruLM1z9-utehcINolSj0vlUqZionyLoRUs/edit#gid=0\",\"cachedResultName\":\"Sheet1\"},\"documentId\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"\",\"cachedResultUrl\":\"https://docs.google.com/spreadsheets/d/1pb4WLqv2EruLM1z9-utehcINolSj0vlUqZionyLoRUs/edit?usp=drivesdk\",\"cachedResultName\":\"URLs\"}},\"credentials\":{\"googleSheetsTriggerOAuth2Api\":{\"id\":\"qDzHSzTkclwDHpSR\",\"name\":\"Google Sheets Trigger account\"}},\"typeVersion\":1},{\"id\":\"b19aa287-3be4-4e16-908d-b0cb484519e3\",\"name\":\"Scrape Website Content with Dumpling AI\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[540,340],\"parameters\":{\"url\":\"https://app.dumplingai.com/api/v1/scrape\",\"method\":\"POST\",\"options\":{\"allowUnauthorizedCerts\":true},\"jsonBody\":\"={\\n  \\\"url\\\": \\\"{{ $('Trigger- Watches For new URL in Spreadsheet')}}\\\", \\n  \\\"format\\\": \\\"html\\\",\\n  \\\"cleaned\\\": \\\"True\\\"\\n  }\",\"sendBody\":true,\"sendHeaders\":true,\"specifyBody\":\"json\",\"authentication\":\"genericCredentialType\",\"genericAuthType\":\"httpHeaderAuth\",\"headerParameters\":{\"parameters\":[{\"name\":\"Content-Type\",\"value\":\"application/json\"}]}},\"credentials\":{\"httpBasicAuth\":{\"id\":\"mznexGH3YDtrUTAk\",\"name\":\"Unnamed credential\"},\"httpHeaderAuth\":{\"id\":\"xamyMqCpAech5BeT\",\"name\":\"Header Auth account\"}},\"typeVersion\":4.1},{\"id\":\"02cbc6f9-bdcb-45fc-9973-ded42346ffbc\",\"name\":\"Split HTML Array into Individual Books\",\"type\":\"n8n-nodes-base.splitOut\",\"position\":[980,340],\"parameters\":{\"options\":{},\"fieldToSplitOut\":\"books\"},\"typeVersion\":1}],\"active\":false,\"pinData\":{},\"settings\":{\"executionOrder\":\"v1\"},\"versionId\":\"264412ff-9d74-443c-a2ff-69be1e042a82\",\"connections\":{\"Sort by price\":{\"main\":[[{\"node\":\"Convert to CSV File\",\"type\":\"main\",\"index\":0}]]},\"Convert to CSV File\":{\"main\":[[{\"node\":\"Send CSV via e-mail\",\"type\":\"main\",\"index\":0}]]},\"Extract individual book price\":{\"main\":[[{\"node\":\"Sort by price\",\"type\":\"main\",\"index\":0}]]},\"Extract all books from the page\":{\"main\":[[{\"node\":\"Split HTML Array into Individual Books\",\"type\":\"main\",\"index\":0}]]},\"Split HTML Array into Individual Books\":{\"main\":[[{\"node\":\"Extract individual book price\",\"type\":\"main\",\"index\":0}]]},\"Scrape Website Content with Dumpling AI\":{\"main\":[[{\"node\":\"Extract all books from the page\",\"type\":\"main\",\"index\":0}]]},\"Trigger- Watches For new URL in Spreadsheet\":{\"main\":[[{\"node\":\"Scrape Website Content with Dumpling AI\",\"type\":\"main\",\"index\":0}]]}}}",
  "readme": "### üë• Who is this for?\n\nThis workflow is ideal for virtual assistants, researchers, developers, automation specialists, and data analysts who need to regularly extract and organize structured product information (like books) from a website. It‚Äôs especially useful for those working with catalog-based websites who want to automate extraction and delivery of clean, sorted data.\n\n* * *\n\n### üß© What problem is this solving?\n\nManually copying product listings like book titles and prices from a website into a spreadsheet is slow and repetitive. This automation solves that problem by scraping content using Dumpling AI, extracting the right data using CSS selectors, and formatting it into a clean CSV file that is sent to your email‚Äîall triggered automatically when a new URL is added to Google Sheets.\n\n* * *\n\n### ‚öôÔ∏è What this workflow does\n\nThis template automates an entire content scraping and delivery process:\n\n  * Watches a Google Sheet for new URLs\n  * Scrapes the HTML content of the given webpage using Dumpling AI\n  * Uses CSS selectors in the HTML node to extract each book from the page\n  * Splits the HTML array into individual items\n  * Extracts the book title and price from each HTML block\n  * Sorts the books in descending order based on price\n  * Converts the sorted data to a CSV file\n  * Sends the CSV via email using Gmail\n\n\n\n* * *\n\n### üõ†Ô∏è Setup\n\n  1. **Google Sheets**\n\n     * Create a sheet titled something like `URLs`\n     * Add your product listing URLs (e.g., <http://books.toscrape.com>)\n     * Connect the Google Sheets trigger node to your sheet\n     * Ensure you have proper credentials connected\n  2. **Dumpling AI**\n\n     * Create an account at [Dumpling AI](https://app.dumplingai.com%5D) \\- Generate your API key\n\n     * Set the HTTP Method to `POST` and pass the URL dynamically from the Google Sheet\n\n     * Use `Header Auth` to include your API key in the request header\n\n     * Make sure `\"cleaned\": \"True\"` is included in the body for optimized HTML output\n\n  3. **HTML Node**\n\n     * The first HTML node extracts the main book container blocks using:  \n`.row &gt; li`\n     * The second HTML node parses out the individual fields: \n       * `title`: `h3 &gt; a` (via the `title` attribute)\n       * `price`: `.price_color`\n  4. **Sort Node**\n\n     * Sorts books by `price` in descending order\n     * Note: price is extracted as a string, ensure it's parsable if you plan to use numeric filtering later\n  5. **Convert to CSV**\n\n     * The JSON data is passed into a Convert node and transformed into a CSV file\n  6. **Gmail**\n\n     * Sends the CSV as an attachment to a designated email\n\n\n\n* * *\n\n### üîÑ How to customize this workflow\n\n  * **Extract more data** : Add more CSS selectors in the second HTML node to pull fields like author, availability, or product links\n  * **Switch destinations** : Replace Gmail with Slack, Google Drive, Dropbox, or another platform\n  * **Adjust sorting** : Sort alphabetically or based on another extracted value\n  * **Use a different source** : As long as the site structure is consistent, this can scrape any listing-like page\n  * **Trigger differently** : Use a webhook, form submission, or schedule trigger instead of Google Sheets\n\n\n\n* * *\n\n### ‚ö†Ô∏è Dependencies and Notes\n\n  * This workflow uses **Dumpling AI** to perform the web scraping. This requires an API key and uses credits per request.\n  * The **HTML node** depends on valid CSS selectors. If the site layout changes, the selectors may need to be updated.\n  * Ensure you‚Äôre not scraping content from websites that prohibit automated scraping.\n\n\n",
  "readme_html": "<!--[--><div data-v-006f9244=\"\"><h3>üë• Who is this for?</h3>\n<p>This workflow is ideal for virtual assistants, researchers, developers, automation specialists, and data analysts who need to regularly extract and organize structured product information (like books) from a website. It‚Äôs especially useful for those working with catalog-based websites who want to automate extraction and delivery of clean, sorted data.</p>\n<hr>\n<h3>üß© What problem is this solving?</h3>\n<p>Manually copying product listings like book titles and prices from a website into a spreadsheet is slow and repetitive. This automation solves that problem by scraping content using Dumpling AI, extracting the right data using CSS selectors, and formatting it into a clean CSV file that is sent to your email‚Äîall triggered automatically when a new URL is added to Google Sheets.</p>\n<hr>\n<h3>‚öôÔ∏è What this workflow does</h3>\n<p>This template automates an entire content scraping and delivery process:</p>\n<ul>\n<li>Watches a Google Sheet for new URLs</li>\n<li>Scrapes the HTML content of the given webpage using Dumpling AI</li>\n<li>Uses CSS selectors in the HTML node to extract each book from the page</li>\n<li>Splits the HTML array into individual items</li>\n<li>Extracts the book title and price from each HTML block</li>\n<li>Sorts the books in descending order based on price</li>\n<li>Converts the sorted data to a CSV file</li>\n<li>Sends the CSV via email using Gmail</li>\n</ul>\n<hr>\n<h3>üõ†Ô∏è Setup</h3>\n<ol>\n<li>\n<p><strong>Google Sheets</strong></p>\n<ul>\n<li>Create a sheet titled something like <code>URLs</code></li>\n<li>Add your product listing URLs (e.g., <a href=\"http://books.toscrape.com\" rel=\"ugc nofollow\" target=\"_blank\">http://books.toscrape.com</a>)</li>\n<li>Connect the Google Sheets trigger node to your sheet</li>\n<li>Ensure you have proper credentials connected</li>\n</ul>\n</li>\n<li>\n<p><strong>Dumpling AI</strong></p>\n<ul>\n<li>\n<p>Create an account at <a href=\"https://app.dumplingai.com%5D\" rel=\"ugc nofollow\" target=\"_blank\">Dumpling AI</a>   - Generate your API key</p>\n</li>\n<li>\n<p>Set the HTTP Method to <code>POST</code> and pass the URL dynamically from the Google Sheet</p>\n</li>\n<li>\n<p>Use <code>Header Auth</code> to include your API key in the request header</p>\n</li>\n<li>\n<p>Make sure <code>\"cleaned\": \"True\"</code> is included in the body for optimized HTML output</p>\n</li>\n</ul>\n</li>\n<li>\n<p><strong>HTML Node</strong></p>\n<ul>\n<li>The first HTML node extracts the main book container blocks using:<br>\n<code>.row &amp;gt; li</code></li>\n<li>The second HTML node parses out the individual fields:\n<ul>\n<li><code>title</code>: <code>h3 &amp;gt; a</code> (via the <code>title</code> attribute)</li>\n<li><code>price</code>: <code>.price_color</code></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><strong>Sort Node</strong></p>\n<ul>\n<li>Sorts books by <code>price</code> in descending order</li>\n<li>Note: price is extracted as a string, ensure it's parsable if you plan to use numeric filtering later</li>\n</ul>\n</li>\n<li>\n<p><strong>Convert to CSV</strong></p>\n<ul>\n<li>The JSON data is passed into a Convert node and transformed into a CSV file</li>\n</ul>\n</li>\n<li>\n<p><strong>Gmail</strong></p>\n<ul>\n<li>Sends the CSV as an attachment to a designated email</li>\n</ul>\n</li>\n</ol>\n<hr>\n<h3>üîÑ How to customize this workflow</h3>\n<ul>\n<li><strong>Extract more data</strong>: Add more CSS selectors in the second HTML node to pull fields like author, availability, or product links</li>\n<li><strong>Switch destinations</strong>: Replace Gmail with Slack, Google Drive, Dropbox, or another platform</li>\n<li><strong>Adjust sorting</strong>: Sort alphabetically or based on another extracted value</li>\n<li><strong>Use a different source</strong>: As long as the site structure is consistent, this can scrape any listing-like page</li>\n<li><strong>Trigger differently</strong>: Use a webhook, form submission, or schedule trigger instead of Google Sheets</li>\n</ul>\n<hr>\n<h3>‚ö†Ô∏è Dependencies and Notes</h3>\n<ul>\n<li>This workflow uses <strong>Dumpling AI</strong> to perform the web scraping. This requires an API key and uses credits per request.</li>\n<li>The <strong>HTML node</strong> depends on valid CSS selectors. If the site layout changes, the selectors may need to be updated.</li>\n<li>Ensure you‚Äôre not scraping content from websites that prohibit automated scraping.</li>\n</ul>\n</div><!--]-->"
}