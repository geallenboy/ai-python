{
  "title": "Prompt-based Object Detection with Gemini 2.0",
  "url": "https://n8n.io/workflows/2649-prompt-based-object-detection-with-gemini-20/",
  "category": "Engineering",
  "category_url": "https://n8n.io/workflows/categories/engineering/?count=20",
  "author": "Jimleuk",
  "publish_date": "Last update 4 months ago",
  "publish_date_absolute": "2025-01-06",
  "content": "",
  "workflow_json": "{\"nodes\":[{\"id\":\"bae5d407-9210-4bd0-99a3-3637ee893065\",\"name\":\"When clicking ‘Test workflow’\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[-1440,-280],\"parameters\":{},\"typeVersion\":1},{\"id\":\"c5a14c8e-4aeb-4a4e-b202-f88e837b6efb\",\"name\":\"Get Variables\",\"type\":\"n8n-nodes-base.set\",\"position\":[-200,-180],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"b455afe0-2311-4d3f-8751-269624d76cf1\",\"name\":\"coords\",\"type\":\"array\",\"value\":\"={{ $json.candidates[0].content.parts[0].text.parseJson() }}\"},{\"id\":\"92f09465-9a0b-443c-aa72-6d208e4df39c\",\"name\":\"width\",\"type\":\"string\",\"value\":\"={{ $('Get Image Info').item.json.size.width }}\"},{\"id\":\"da98ce2a-4600-46a6-b4cb-159ea515cb50\",\"name\":\"height\",\"type\":\"string\",\"value\":\"={{ $('Get Image Info').item.json.size.height }}\"}]}},\"typeVersion\":3.4},{\"id\":\"f24017c9-05bc-4f75-a18c-29efe99bfe0e\",\"name\":\"Get Test Image\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[-1260,-280],\"parameters\":{\"url\":\"https://www.stonhambarns.co.uk/wp-content/uploads/jennys-ark-petting-zoo-for-website-6.jpg\",\"options\":{}},\"typeVersion\":4.2},{\"id\":\"c0f6a9f7-ba65-48a3-8752-ce5d80fe33cf\",\"name\":\"Gemini 2.0 Object Detection\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[-680,-180],\"parameters\":{\"url\":\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent\",\"method\":\"POST\",\"options\":{},\"jsonBody\":\"={{\\n{\\n  \\\"contents\\\": [{\\n    \\\"parts\\\":[\\n        {\\\"text\\\": \\\"I want to see all bounding boxes of rabbits in this image.\\\"},\\n        {\\n          \\\"inline_data\\\": {\\n            \\\"mime_type\\\":\\\"image/jpeg\\\",\\n            \\\"data\\\": $input.item.binary.data.data\\n          }\\n        }\\n    ]\\n  }],\\n  \\\"generationConfig\\\": {\\n    \\\"response_mime_type\\\": \\\"application/json\\\",\\n    \\\"response_schema\\\": {\\n      \\\"type\\\": \\\"ARRAY\\\",\\n      \\\"items\\\": {\\n        \\\"type\\\": \\\"OBJECT\\\",\\n        \\\"properties\\\": {\\n          \\\"box_2d\\\": {\\\"type\\\":\\\"ARRAY\\\", \\\"items\\\": { \\\"type\\\": \\\"NUMBER\\\" } },\\n          \\\"label\\\": { \\\"type\\\": \\\"STRING\\\"}\\n        }\\n      }\\n    }\\n  }\\n}\\n}}\",\"sendBody\":true,\"specifyBody\":\"json\",\"authentication\":\"predefinedCredentialType\",\"nodeCredentialType\":\"googlePalmApi\"},\"credentials\":{\"googlePalmApi\":{\"id\":\"dSxo6ns5wn658r8N\",\"name\":\"Google Gemini(PaLM) Api account\"}},\"typeVersion\":4.2},{\"id\":\"edbc1152-4642-4656-9a3a-308dae42bac6\",\"name\":\"Scale Normalised Coords\",\"type\":\"n8n-nodes-base.code\",\"position\":[-20,-180],\"parameters\":{\"jsCode\":\"const { coords, width, height } = $input.first().json;\\n\\nconst scale = 1000;\\nconst scaleCoordX = (val) => (val * width) / scale;\\nconst scaleCoordY = (val) => (val * height) / scale;\\n  \\nconst normalisedOutput = coords\\n  .filter(coord => coord.box_2d.length === 4)\\n  .map(coord => {\\n    return {\\n      xmin: coord.box_2d[1] ? scaleCoordX(coord.box_2d[1]) : coord.box_2d[1],\\n      xmax: coord.box_2d[3] ? scaleCoordX(coord.box_2d[3]) : coord.box_2d[3],\\n      ymin: coord.box_2d[0] ? scaleCoordY(coord.box_2d[0]) : coord.box_2d[0],\\n      ymax: coord.box_2d[2] ? scaleCoordY(coord.box_2d[2]) : coord.box_2d[2],\\n    }\\n  });\\n\\nreturn {\\n  json: {\\n    coords: normalisedOutput\\n  },\\n  binary: $('Get Test Image').first().binary\\n}\"},\"typeVersion\":2},{\"id\":\"e0380611-ac7d-48d8-8eeb-35de35dbe56a\",\"name\":\"Draw Bounding Boxes\",\"type\":\"n8n-nodes-base.editImage\",\"position\":[400,-180],\"parameters\":{\"options\":{},\"operation\":\"multiStep\",\"operations\":{\"operations\":[{\"color\":\"#ff00f277\",\"operation\":\"draw\",\"endPositionX\":\"={{ $json.coords[0].xmax }}\",\"endPositionY\":\"={{ $json.coords[0].ymax }}\",\"startPositionX\":\"={{ $json.coords[0].xmin }}\",\"startPositionY\":\"={{ $json.coords[0].ymin }}\"},{\"color\":\"#ff00f277\",\"operation\":\"draw\",\"endPositionX\":\"={{ $json.coords[1].xmax }}\",\"endPositionY\":\"={{ $json.coords[1].ymax }}\",\"startPositionX\":\"={{ $json.coords[1].xmin }}\",\"startPositionY\":\"={{ $json.coords[1].ymin }}\"},{\"color\":\"#ff00f277\",\"operation\":\"draw\",\"endPositionX\":\"={{ $json.coords[2].xmax }}\",\"endPositionY\":\"={{ $json.coords[2].ymax }}\",\"startPositionX\":\"={{ $json.coords[2].xmin }}\",\"startPositionY\":\"={{ $json.coords[2].ymin }}\"},{\"color\":\"#ff00f277\",\"operation\":\"draw\",\"endPositionX\":\"={{ $json.coords[3].xmax }}\",\"endPositionY\":\"={{ $json.coords[3].ymax }}\",\"startPositionX\":\"={{ $json.coords[3].xmin }}\",\"startPositionY\":\"={{ $json.coords[3].ymin }}\"},{\"color\":\"#ff00f277\",\"operation\":\"draw\",\"endPositionX\":\"={{ $json.coords[4].xmax }}\",\"endPositionY\":\"={{ $json.coords[4].ymax }}\",\"startPositionX\":\"={{ $json.coords[4].xmin }}\",\"startPositionY\":\"={{ $json.coords[4].ymin }}\"},{\"color\":\"#ff00f277\",\"operation\":\"draw\",\"cornerRadius\":\"=0\",\"endPositionX\":\"={{ $json.coords[5].xmax }}\",\"endPositionY\":\"={{ $json.coords[5].ymax }}\",\"startPositionX\":\"={{ $json.coords[5].xmin }}\",\"startPositionY\":\"={{ $json.coords[5].ymin }}\"}]}},\"typeVersion\":1},{\"id\":\"52daac1b-5ba3-4302-b47b-df3f410b40fc\",\"name\":\"Get Image Info\",\"type\":\"n8n-nodes-base.editImage\",\"position\":[-1080,-280],\"parameters\":{\"operation\":\"information\"},\"typeVersion\":1},{\"id\":\"0d2ab96a-3323-472d-82ff-2af5e7d815a1\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[740,-460],\"parameters\":{\"width\":440,\"height\":380,\"content\":\"Fig 1. Output of Object Detection\\n![](https://res.cloudinary.com/daglih2g8/image/upload/f_auto,q_auto/v1/n8n-workflows/download_1_qmqyyo#full-width)\"},\"typeVersion\":1},{\"id\":\"c1806400-57da-4ef2-a50d-6ed211d5df29\",\"name\":\"Sticky Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-1520,-480],\"parameters\":{\"color\":7,\"width\":600,\"height\":420,\"content\":\"## 1. Download Test Image\\n[Read more about the HTTP node](https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.httprequest)\\n\\nAny compatible image will do ([see docs](https://ai.google.dev/gemini-api/docs/vision?lang=rest#technical-details-image)) but best if it isn't too busy or the subjects too obscure. Most importantly, you are able to retrieve the width and height as this is required for a later step.\"},\"typeVersion\":1},{\"id\":\"3ae12a7c-a20f-4087-868e-b118cc09fa9a\",\"name\":\"Sticky Note2\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-900,-480],\"parameters\":{\"color\":7,\"width\":560,\"height\":540,\"content\":\"## 2. Use Prompt-Based Object Detection\\n[Read more about the HTTP node](https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.httprequest)\\n\\nWe've had generalised object detection before ([see my other template using ResNet](https://n8n.io/workflows/2331-build-your-own-image-search-using-ai-object-detection-cdn-and-elasticsearch/)) but being able to prompt for what you're looking for is a very exciting proposition! Not only could this reduce the effort in post-detection filtering but also introduce contextual use-cases such as searching by \\\"emotion\\\", \\\"locality\\\", \\\"anomolies\\\" and many more!\\n\\nI found the the output json schema of `{ \\\"box_2d\\\": { \\\"type\\\": \\\"array\\\", ... } }` works best for Gemini to return coordinates. \"},\"typeVersion\":1},{\"id\":\"35673272-7207-41d1-985e-08032355846e\",\"name\":\"Sticky Note3\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-320,-400],\"parameters\":{\"color\":7,\"width\":520,\"height\":440,\"content\":\"## 3. Scale Coords to Fit Original Image\\n[Read more about the Code node](https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.code/)\\n\\nAccording to the Gemini 2.0 overview on [how it calculates bounding boxes](https://ai.google.dev/gemini-api/docs/models/gemini-v2?_gl=1*187cb6v*_up*MQ..*_ga*MTU1ODkzMDc0Mi4xNzM0NDM0NDg2*_ga_P1DBVKWT6V*MTczNDQzNDQ4Ni4xLjAuMTczNDQzNDQ4Ni4wLjAuMjEzNzc5MjU0Ng..#bounding-box), we'll have to rescale the coordinate values as they are normalised to a 0-1000 range. Nothing a little code node can't help with!\"},\"typeVersion\":1},{\"id\":\"d3d4470d-0fe1-47fd-a892-10a19b6a6ecc\",\"name\":\"Sticky Note4\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-660,80],\"parameters\":{\"color\":5,\"width\":340,\"height\":100,\"content\":\"### Q. Why not use the Basic LLM node?\\nAt time of writing, Langchain version does not recognise Gemini 2.0 to be a multimodal model.\"},\"typeVersion\":1},{\"id\":\"5b2c1eff-6329-4d9a-9d3d-3a48fb3bd753\",\"name\":\"Sticky Note5\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[220,-400],\"parameters\":{\"color\":7,\"width\":500,\"height\":440,\"content\":\"## 4. Draw!\\n[Read more about the Edit Image node](https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.editimage/)\\n\\nFinally for this demonstration, we can use the \\\"Edit Image\\\" node to draw the bounding boxes on top of the original image. In my test run, I can see Gemini did miss out one of the bunnies but seeing how this is the experimental version we're playing with, it's pretty good to see it doesn't do too bad of a job.\"},\"typeVersion\":1},{\"id\":\"965d791b-a183-46b0-b2a6-dd961d630c13\",\"name\":\"Sticky Note6\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[-1960,-740],\"parameters\":{\"width\":420,\"height\":680,\"content\":\"## Try it out!\\n### This n8n template demonstrates how to use Gemini 2.0's new Bounding Box detection capabilities your workflows.\\n\\nThe key difference being this enables prompt-based object detection for images which is pretty powerful for things like contextual search over an image. eg. \\\"Put a bounding box around all adults with children in this image\\\" or \\\"Put a bounding box around cars parked out of bounds of a parking space\\\".\\n\\n## How it works\\n* An image is downloaded via the HTTP node and an \\\"Edit Image\\\" node is used to extract the file's width and height.\\n* The image is then given to the Gemini 2.0 API to parse and return coordinates of the bounding box of the requested subjects. In this demo, we've asked for the AI to identify all bunnies.\\n* The coordinates are then rescaled with the original image's width and height to correctl align them.\\n* Finally to measure the accuracy of the object detection, we use the \\\"Edit Image\\\" node to draw the bounding boxes onto the original image.\\n\\n\\n### Need Help?\\nJoin the [Discord](https://discord.com/invite/XPKeKXeB7d) or ask in the [Forum](https://community.n8n.io/)!\\n\\nHappy Hacking!\"},\"typeVersion\":1}],\"pinData\":{},\"connections\":{\"Get Variables\":{\"main\":[[{\"node\":\"Scale Normalised Coords\",\"type\":\"main\",\"index\":0}]]},\"Get Image Info\":{\"main\":[[{\"node\":\"Gemini 2.0 Object Detection\",\"type\":\"main\",\"index\":0}]]},\"Get Test Image\":{\"main\":[[{\"node\":\"Get Image Info\",\"type\":\"main\",\"index\":0}]]},\"Draw Bounding Boxes\":{\"main\":[[]]},\"Scale Normalised Coords\":{\"main\":[[{\"node\":\"Draw Bounding Boxes\",\"type\":\"main\",\"index\":0}]]},\"Gemini 2.0 Object Detection\":{\"main\":[[{\"node\":\"Get Variables\",\"type\":\"main\",\"index\":0}]]},\"When clicking ‘Test workflow’\":{\"main\":[[{\"node\":\"Get Test Image\",\"type\":\"main\",\"index\":0}]]}}}",
  "readme": "This n8n template demonstrates how to get started with Gemini 2.0's new Bounding Box detection capabilities in your workflows.\n\nThe key difference being this enables prompt-based object detection for images which is pretty powerful for things like contextual search over an image. eg. \"Put a bounding box around all adults with children in this image\" or \"Put a bounding box around cars parked out of bounds of a parking space\".\n\n## How it works\n\n  * An image is downloaded via the HTTP node and an \"Edit Image\" node is used to extract the file's width and height.\n  * The image is then given to the Gemini 2.0 API to parse and return coordinates of the bounding box of the requested subjects. In this demo, we've asked for the AI to identify all bunnies.\n  * The coordinates are then rescaled with the original image's width and height to correctl align them.\n  * Finally to measure the accuracy of the object detection, we use the \"Edit Image\" node to draw the bounding boxes onto the original image.\n\n\n\n## How to use\n\n  * Really up to the imagination! Perhaps a form of grounding for evidence based workflows or a higher form of image search can be built.\n\n\n\n## Requirements\n\n  * Google Gemini for LLM\n\n\n\n## Customising the workflow\n\n  * This template is just a demonstration of an experimental version of Gemini 2.0. It is recommended to wait for Gemini 2.0 to come out of this stage before using in production.\n\n\n",
  "readme_html": "<!--[--><div data-v-006f9244=\"\"><p>This n8n template demonstrates how to get started with Gemini 2.0's new Bounding Box detection capabilities in your workflows.</p>\n<p>The key difference being this enables prompt-based object detection for images which is pretty powerful for things like contextual search over an image. eg. \"Put a bounding box around all adults with children in this image\" or \"Put a bounding box around cars parked out of bounds of a parking space\".</p>\n<h2>How it works</h2>\n<ul>\n<li>An image is downloaded via the HTTP node and an \"Edit Image\" node is used to extract the file's width and height.</li>\n<li>The image is then given to the Gemini 2.0 API to parse and return coordinates of the bounding box of the requested subjects. In this demo, we've asked for the AI to identify all bunnies.</li>\n<li>The coordinates are then rescaled with the original image's width and height to correctl align them.</li>\n<li>Finally to measure the accuracy of the object detection, we use the \"Edit Image\" node to draw the bounding boxes onto the original image.</li>\n</ul>\n<h2>How to use</h2>\n<ul>\n<li>Really up to the imagination! Perhaps a form of grounding for evidence based workflows or a higher form of image search can be built.</li>\n</ul>\n<h2>Requirements</h2>\n<ul>\n<li>Google Gemini for LLM</li>\n</ul>\n<h2>Customising the workflow</h2>\n<ul>\n<li>This template is just a demonstration of an experimental version of Gemini 2.0. It is recommended to wait for Gemini 2.0 to come out of this stage before using in production.</li>\n</ul>\n</div><!--]-->",
  "readme_zh": "该n8n模板展示了如何在您的工作流程中初步运用Gemini 2.0全新的边界框检测功能。\n\n其核心突破在于实现了基于提示词的图像目标检测，这对于图像上下文搜索等场景极具价值。例如：\"为图中所有带小孩的成人添加边界框\"或\"标记停车场内违规停放的车辆\"。\n\n## 实现原理\n\n  * 通过HTTP节点下载图像后，\"编辑图像\"节点将提取文件原始宽高\n  * 图像送入Gemini 2.0 API解析，返回指定目标的边界框坐标（本示例检测所有兔子）\n  * 根据原始图像尺寸对坐标进行等比缩放校准\n  * 最后通过\"编辑图像\"节点将边界框绘制于原图，直观验证检测精度\n\n## 应用场景\n\n  * 全凭想象！可用于构建证据溯源工作流，或开发高阶图像搜索系统\n\n## 环境要求\n\n  * 需配置Google Gemini大语言模型\n\n## 定制说明\n\n  * 本模板仅演示Gemini 2.0实验版本功能，建议待正式发布后再投入生产环境使用\n\n（注：根据技术文档特点，采用以下处理：\n1. 专业术语保留英文原名如\"n8n/Gemini 2.0/API\"\n2. 交互式操作说明使用中文技术文档常见句式\n3. 长难句拆分为符合中文阅读习惯的短句\n4. 保留技术文档特有的项目符号层级结构\n5. 口语化提示词翻译时保留其交互特征）",
  "title_zh": "基于提示的物体检测与Gemini 2.0",
  "publish_date_zh": "最后更新于4个月前",
  "workflow_json_zh": "{\n  \"nodes\": [\n    {\n      \"id\": \"bae5d407-9210-4bd0-99a3-3637ee893065\",\n      \"name\": \"When clicking ‘Test workflow’\",\n      \"type\": \"n8n-nodes-base.manualTrigger\",\n      \"position\": [\n        -1440,\n        -280\n      ],\n      \"parameters\": {},\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"c5a14c8e-4aeb-4a4e-b202-f88e837b6efb\",\n      \"name\": \"Get Variables\",\n      \"type\": \"n8n-nodes-base.set\",\n      \"position\": [\n        -200,\n        -180\n      ],\n      \"parameters\": {\n        \"options\": {},\n        \"assignments\": {\n          \"assignments\": [\n            {\n              \"id\": \"b455afe0-2311-4d3f-8751-269624d76cf1\",\n              \"name\": \"coords\",\n              \"type\": \"array\",\n              \"value\": \"={{ $json.candidates[0].content.parts[0].text.parseJson() }}\"\n            },\n            {\n              \"id\": \"92f09465-9a0b-443c-aa72-6d208e4df39c\",\n              \"name\": \"width\",\n              \"type\": \"string\",\n              \"value\": \"={{ $('Get Image Info').item.json.size.width }}\"\n            },\n            {\n              \"id\": \"da98ce2a-4600-46a6-b4cb-159ea515cb50\",\n              \"name\": \"height\",\n              \"type\": \"string\",\n              \"value\": \"={{ $('Get Image Info').item.json.size.height }}\"\n            }\n          ]\n        }\n      },\n      \"typeVersion\": 3.4\n    },\n    {\n      \"id\": \"f24017c9-05bc-4f75-a18c-29efe99bfe0e\",\n      \"name\": \"Get Test Image\",\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"position\": [\n        -1260,\n        -280\n      ],\n      \"parameters\": {\n        \"url\": \"https://www.stonhambarns.co.uk/wp-content/uploads/jennys-ark-petting-zoo-for-website-6.jpg\",\n        \"options\": {}\n      },\n      \"typeVersion\": 4.2\n    },\n    {\n      \"id\": \"c0f6a9f7-ba65-48a3-8752-ce5d80fe33cf\",\n      \"name\": \"Gemini 2.0 Object Detection\",\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"position\": [\n        -680,\n        -180\n      ],\n      \"parameters\": {\n        \"url\": \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent\",\n        \"method\": \"POST\",\n        \"options\": {},\n        \"jsonBody\": \"={{\\n{\\n  \\\"contents\\\": [{\\n    \\\"parts\\\":[\\n        {\\\"text\\\": \\\"I want to see all bounding boxes of rabbits in this image.\\\"},\\n        {\\n          \\\"inline_data\\\": {\\n            \\\"mime_type\\\":\\\"image/jpeg\\\",\\n            \\\"data\\\": $input.item.binary.data.data\\n          }\\n        }\\n    ]\\n  }],\\n  \\\"generationConfig\\\": {\\n    \\\"response_mime_type\\\": \\\"application/json\\\",\\n    \\\"response_schema\\\": {\\n      \\\"type\\\": \\\"ARRAY\\\",\\n      \\\"items\\\": {\\n        \\\"type\\\": \\\"OBJECT\\\",\\n        \\\"properties\\\": {\\n          \\\"box_2d\\\": {\\\"type\\\":\\\"ARRAY\\\", \\\"items\\\": { \\\"type\\\": \\\"NUMBER\\\" } },\\n          \\\"label\\\": { \\\"type\\\": \\\"STRING\\\"}\\n        }\\n      }\\n    }\\n  }\\n}\\n}}\",\n        \"sendBody\": true,\n        \"specifyBody\": \"json\",\n        \"authentication\": \"predefinedCredentialType\",\n        \"nodeCredentialType\": \"googlePalmApi\"\n      },\n      \"credentials\": {\n        \"googlePalmApi\": {\n          \"id\": \"dSxo6ns5wn658r8N\",\n          \"name\": \"Google Gemini(PaLM) Api account\"\n        }\n      },\n      \"typeVersion\": 4.2\n    },\n    {\n      \"id\": \"edbc1152-4642-4656-9a3a-308dae42bac6\",\n      \"name\": \"Scale Normalised Coords\",\n      \"type\": \"n8n-nodes-base.code\",\n      \"position\": [\n        -20,\n        -180\n      ],\n      \"parameters\": {\n        \"jsCode\": \"const { coords, width, height } = $input.first().json;\\n\\nconst scale = 1000;\\nconst scaleCoordX = (val) => (val * width) / scale;\\nconst scaleCoordY = (val) => (val * height) / scale;\\n  \\nconst normalisedOutput = coords\\n  .filter(coord => coord.box_2d.length === 4)\\n  .map(coord => {\\n    return {\\n      xmin: coord.box_2d[1] ? scaleCoordX(coord.box_2d[1]) : coord.box_2d[1],\\n      xmax: coord.box_2d[3] ? scaleCoordX(coord.box_2d[3]) : coord.box_2d[3],\\n      ymin: coord.box_2d[0] ? scaleCoordY(coord.box_2d[0]) : coord.box_2d[0],\\n      ymax: coord.box_2d[2] ? scaleCoordY(coord.box_2d[2]) : coord.box_2d[2],\\n    }\\n  });\\n\\nreturn {\\n  json: {\\n    coords: normalisedOutput\\n  },\\n  binary: $('Get Test Image').first().binary\\n}\"\n      },\n      \"typeVersion\": 2\n    },\n    {\n      \"id\": \"e0380611-ac7d-48d8-8eeb-35de35dbe56a\",\n      \"name\": \"Draw Bounding Boxes\",\n      \"type\": \"n8n-nodes-base.editImage\",\n      \"position\": [\n        400,\n        -180\n      ],\n      \"parameters\": {\n        \"options\": {},\n        \"operation\": \"multiStep\",\n        \"operations\": {\n          \"operations\": [\n            {\n              \"color\": \"#ff00f277\",\n              \"operation\": \"draw\",\n              \"endPositionX\": \"={{ $json.coords[0].xmax }}\",\n              \"endPositionY\": \"={{ $json.coords[0].ymax }}\",\n              \"startPositionX\": \"={{ $json.coords[0].xmin }}\",\n              \"startPositionY\": \"={{ $json.coords[0].ymin }}\"\n            },\n            {\n              \"color\": \"#ff00f277\",\n              \"operation\": \"draw\",\n              \"endPositionX\": \"={{ $json.coords[1].xmax }}\",\n              \"endPositionY\": \"={{ $json.coords[1].ymax }}\",\n              \"startPositionX\": \"={{ $json.coords[1].xmin }}\",\n              \"startPositionY\": \"={{ $json.coords[1].ymin }}\"\n            },\n            {\n              \"color\": \"#ff00f277\",\n              \"operation\": \"draw\",\n              \"endPositionX\": \"={{ $json.coords[2].xmax }}\",\n              \"endPositionY\": \"={{ $json.coords[2].ymax }}\",\n              \"startPositionX\": \"={{ $json.coords[2].xmin }}\",\n              \"startPositionY\": \"={{ $json.coords[2].ymin }}\"\n            },\n            {\n              \"color\": \"#ff00f277\",\n              \"operation\": \"draw\",\n              \"endPositionX\": \"={{ $json.coords[3].xmax }}\",\n              \"endPositionY\": \"={{ $json.coords[3].ymax }}\",\n              \"startPositionX\": \"={{ $json.coords[3].xmin }}\",\n              \"startPositionY\": \"={{ $json.coords[3].ymin }}\"\n            },\n            {\n              \"color\": \"#ff00f277\",\n              \"operation\": \"draw\",\n              \"endPositionX\": \"={{ $json.coords[4].xmax }}\",\n              \"endPositionY\": \"={{ $json.coords[4].ymax }}\",\n              \"startPositionX\": \"={{ $json.coords[4].xmin }}\",\n              \"startPositionY\": \"={{ $json.coords[4].ymin }}\"\n            },\n            {\n              \"color\": \"#ff00f277\",\n              \"operation\": \"draw\",\n              \"cornerRadius\": \"=0\",\n              \"endPositionX\": \"={{ $json.coords[5].xmax }}\",\n              \"endPositionY\": \"={{ $json.coords[5].ymax }}\",\n              \"startPositionX\": \"={{ $json.coords[5].xmin }}\",\n              \"startPositionY\": \"={{ $json.coords[5].ymin }}\"\n            }\n          ]\n        }\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"52daac1b-5ba3-4302-b47b-df3f410b40fc\",\n      \"name\": \"Get Image Info\",\n      \"type\": \"n8n-nodes-base.editImage\",\n      \"position\": [\n        -1080,\n        -280\n      ],\n      \"parameters\": {\n        \"operation\": \"information\"\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"0d2ab96a-3323-472d-82ff-2af5e7d815a1\",\n      \"name\": \"Sticky Note\",\n      \"type\": \"n8n-nodes-base.stickyNote\",\n      \"position\": [\n        740,\n        -460\n      ],\n      \"parameters\": {\n        \"width\": 440,\n        \"height\": 380,\n        \"content\": \"图1. 目标检测输出结果\\n![](https://res.cloudinary.com/daglih2g8/image/upload/f_auto,q_auto/v1/n8n-workflows/download_1_qmqyyo#full-width)\"\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"c1806400-57da-4ef2-a50d-6ed211d5df29\",\n      \"name\": \"Sticky Note1\",\n      \"type\": \"n8n-nodes-base.stickyNote\",\n      \"position\": [\n        -1520,\n        -480\n      ],\n      \"parameters\": {\n        \"color\": 7,\n        \"width\": 600,\n        \"height\": 420,\n        \"content\": \"## 1. 下载测试图片\\n[详细了解HTTP节点](https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.httprequest)\\n\\n任何兼容的图片均可使用（[参阅文档](https://ai.google.dev/gemini-api/docs/vision?lang=rest#technical-details-image)），但最好避免选择元素过于杂乱或主体过于晦涩的图片。最关键的是需确保能获取图片的宽度和高度信息，这是后续步骤的必要条件。\"\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"3ae12a7c-a20f-4087-868e-b118cc09fa9a\",\n      \"name\": \"Sticky Note2\",\n      \"type\": \"n8n-nodes-base.stickyNote\",\n      \"position\": [\n        -900,\n        -480\n      ],\n      \"parameters\": {\n        \"color\": 7,\n        \"width\": 560,\n        \"height\": 540,\n        \"content\": \"## 2. 使用基于提示词的目标检测技术\\n[详细了解HTTP节点](https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.httprequest)\\n\\n我们此前已实现通用目标检测（[参见我使用ResNet的另一个模板](https://n8n.io/workflows/2331-build-your-own-image-search-using-ai-object-detection-cdn-and-elasticsearch/)），但能够通过自然语言提示指定检测目标堪称革命性突破！这不仅可降低检测后过滤的工作量，更能开启\\\"情绪识别\\\"、\\\"区域特征检测\\\"、\\\"异常识别\\\"等场景化应用！\\n\\n实践发现，当输出JSON采用`{ \\\"box_2d\\\": { \\\"type\\\": \\\"array\\\", ... } }`结构时，Gemini模型能最准确地返回坐标数据。\"\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"35673272-7207-41d1-985e-08032355846e\",\n      \"name\": \"Sticky Note3\",\n      \"type\": \"n8n-nodes-base.stickyNote\",\n      \"position\": [\n        -320,\n        -400\n      ],\n      \"parameters\": {\n        \"color\": 7,\n        \"width\": 520,\n        \"height\": 440,\n        \"content\": \"## 3. 将坐标缩放至适配原始图像\\n[详细了解代码节点](https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.code/)\\n\\n根据Gemini 2.0文档中关于[边界框计算方式](https://ai.google.dev/gemini-api/docs/models/gemini-v2?_gl=1*187cb6v*_up*MQ..*_ga*MTU1ODkzMDc0Mi4xNzM0NDM0NDg2*_ga_P1DBVKWT6V*MTczNDQzNDQ4Ni4xLjAuMTczNDQzNDQ4Ni4wLjAuMjEzNzc5MjU0Ng..#bounding-box)的说明，由于返回的坐标值被归一化到0-1000范围内，我们需要对其进行重新缩放。这种小问题用代码节点就能轻松解决！\"\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"d3d4470d-0fe1-47fd-a892-10a19b6a6ecc\",\n      \"name\": \"Sticky Note4\",\n      \"type\": \"n8n-nodes-base.stickyNote\",\n      \"position\": [\n        -660,\n        80\n      ],\n      \"parameters\": {\n        \"color\": 5,\n        \"width\": 340,\n        \"height\": 100,\n        \"content\": \"### 问：为何不使用基础LLM节点？\\n截至撰写本文时，Langchain版本尚未将Gemini 2.0识别为多模态模型。\"\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"5b2c1eff-6329-4d9a-9d3d-3a48fb3bd753\",\n      \"name\": \"Sticky Note5\",\n      \"type\": \"n8n-nodes-base.stickyNote\",\n      \"position\": [\n        220,\n        -400\n      ],\n      \"parameters\": {\n        \"color\": 7,\n        \"width\": 500,\n        \"height\": 440,\n        \"content\": \"## 4. 绘图！\\n[详细了解编辑图像节点](https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.editimage/)\\n\\n最后在这个演示中，我们可以使用\\\"编辑图像\\\"节点在原始图像上绘制边界框。在我的测试运行中，我发现Gemini确实漏掉了一只兔子，但考虑到我们使用的是实验版本，能取得这样的效果已经相当不错了。\"\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"965d791b-a183-46b0-b2a6-dd961d630c13\",\n      \"name\": \"Sticky Note6\",\n      \"type\": \"n8n-nodes-base.stickyNote\",\n      \"position\": [\n        -1960,\n        -740\n      ],\n      \"parameters\": {\n        \"width\": 420,\n        \"height\": 680,\n        \"content\": \"## 试试看吧！\\n### 本n8n模板演示了如何在工作流中运用Gemini 2.0全新的边界框检测功能\\n\\n其核心突破在于实现了基于提示词的图像物体检测，这为图像上下文搜索等场景提供了强大支持。例如：\\\"为图中所有带小孩的成人添加边界框\\\"或\\\"标记停车场内违规停放的车辆\\\"。\\n\\n## 工作原理\\n* 通过HTTP节点下载图像后，\\\"编辑图像\\\"节点将提取文件原始宽高\\n* 将图像送入Gemini 2.0 API解析，返回指定目标的边界框坐标（本示例要求AI识别所有兔子）\\n* 根据原图尺寸重新缩放坐标以确保精准对齐\\n* 最后通过\\\"编辑图像\\\"节点将边界框绘制到原图上，以此验证物体检测准确度\\n\\n### 需要帮助？\\n加入[Discord社区](https://discord.com/invite/XPKeKXeB7d) 或访问[官方论坛](https://community.n8n.io/)提问！\\n\\n祝您探索愉快！\"\n      },\n      \"typeVersion\": 1\n    }\n  ],\n  \"pinData\": {},\n  \"connections\": {\n    \"Get Variables\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Scale Normalised Coords\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Get Image Info\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Gemini 2.0 Object Detection\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Get Test Image\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Get Image Info\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Draw Bounding Boxes\": {\n      \"main\": [\n        []\n      ]\n    },\n    \"Scale Normalised Coords\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Draw Bounding Boxes\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Gemini 2.0 Object Detection\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Get Variables\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"When clicking ‘Test workflow’\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Get Test Image\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    }\n  }\n}"
}