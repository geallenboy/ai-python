{
  "title": "Pulling data from services that n8n doesn’t have a pre-built integration for",
  "url": "https://n8n.io/workflows/1748-pulling-data-from-services-that-n8n-doesnt-have-a-pre-built-integration-for/",
  "category": "BuildingBlocks",
  "category_url": "https://n8n.io/workflows/categories/building-blocks/?sort=createdAt:desc",
  "author": "Jonathan",
  "publish_date": "Last update 10 months ago",
  "publish_date_absolute": "2024-07-06",
  "content": "",
  "workflow_json": "{\"meta\":{\"instanceId\":\"8c8c5237b8e37b006a7adce87f4369350c58e41f3ca9de16196d3197f69eabcd\"},\"nodes\":[{\"id\":\"25ac6cda-31fb-474a-b6b6-083ec03b9273\",\"name\":\"On clicking 'execute'\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[925,285],\"parameters\":{},\"typeVersion\":1},{\"id\":\"93eaee43-7a39-4c83-aeaa-9ca14d0f4b4b\",\"name\":\"Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[380,240],\"parameters\":{\"width\":440,\"height\":200,\"content\":\"## HTTP Request\\n### This workflow shows the most common use cases of the HTTP request node, and how to handle its output\\n\\n\\n### Click the `Execute Workflow` button and double click on the nodes to see the input and output items.\"},\"typeVersion\":1},{\"id\":\"3ccdc45b-aae1-4760-b45e-5b8dca2a9fcf\",\"name\":\"Note2\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1280,480],\"parameters\":{\"width\":986.3743856726365,\"height\":460.847917534361,\"content\":\"## 3. Handle Pagination\\n### Sometimes you need to make the same request multiple times to get all the data you need (pagination).\\n\\n### The pagination process goes as follow:\\n### 1. Loop through the pages of the input source (`HTTP Request` node named \\\"Get my Starts\\\")\\n### 2. Increment the page at the end of each loop (done with the `set` node named \\\"Increment Page\\\") \\n### 3. Stop looping when there are no pages left (checked at the `If` node named \\\"Are we Finished?\\\")\\n\\n\\n\\n\"},\"typeVersion\":1},{\"id\":\"af19bb6d-5f0a-41ca-93b2-dbd27c3fd07e\",\"name\":\"Set\",\"type\":\"n8n-nodes-base.set\",\"position\":[1345,725],\"parameters\":{\"values\":{\"number\":[{\"name\":\"page\"},{\"name\":\"perpage\",\"value\":15}],\"string\":[{\"name\":\"githubUser\",\"value\":\"that-one-tom\"}]},\"options\":{}},\"typeVersion\":1},{\"id\":\"dad6055d-e06b-4f8c-ab90-deb196fce277\",\"name\":\"Note6\",\"type\":\"n8n-nodes-base.stickyNote\",\"disabled\":true,\"position\":[1280,180],\"parameters\":{\"width\":680,\"height\":280,\"content\":\"## 2. Data Scraping\\n### In this example we fetch the titles from the n8n blog using the `HTTP request` node and then we use the `HTML extract` node to pass.\"},\"typeVersion\":1},{\"id\":\"a7d4b9db-4d38-4b8d-9585-fe65c379e381\",\"name\":\"Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1280,-120],\"parameters\":{\"width\":500,\"height\":280,\"content\":\"## 1. Split into items\\n### In this example, we take the body from an `HTTP Request` node and split it out into items that are easier to manage.\"},\"typeVersion\":1},{\"id\":\"d8402820-fa72-4957-8cf6-432f928ae799\",\"name\":\"Item Lists - Create Items from Body\",\"type\":\"n8n-nodes-base.itemLists\",\"notes\":\"Create Items from Body\",\"position\":[1525,-15],\"parameters\":{\"options\":{},\"fieldToSplitOut\":\"body\"},\"notesInFlow\":false,\"typeVersion\":1},{\"id\":\"598939cd-e4c0-4a90-bd1f-f2b13ccbe072\",\"name\":\"HTML Extract - Extract Article Title\",\"type\":\"n8n-nodes-base.htmlExtract\",\"position\":[1505,285],\"parameters\":{\"options\":{},\"sourceData\":\"binary\",\"extractionValues\":{\"values\":[{\"key\":\"ArticleTitle\",\"cssSelector\":\"#firstHeading\"}]}},\"typeVersion\":1},{\"id\":\"1c9b609c-5e41-4444-ade7-e1069943c904\",\"name\":\"Item Lists - Fetch Body\",\"type\":\"n8n-nodes-base.itemLists\",\"position\":[1705,725],\"parameters\":{\"options\":{},\"fieldToSplitOut\":\"body\"},\"typeVersion\":1,\"alwaysOutputData\":true},{\"id\":\"15dfab42-440c-4d06-9ba2-b7b17371d009\",\"name\":\"If - Are we finished?\",\"type\":\"n8n-nodes-base.if\",\"position\":[1885,725],\"parameters\":{\"conditions\":{\"string\":[{\"value1\":\"={{$node[\\\"HTTP Request - Get my Stars\\\"].json[\\\"body\\\"]}}\",\"operation\":\"isEmpty\"}]}},\"executeOnce\":true,\"typeVersion\":1},{\"id\":\"ba6e6904-6749-4ea2-84c1-8409b795bcf5\",\"name\":\"Set - Increment Page\",\"type\":\"n8n-nodes-base.set\",\"position\":[2105,745],\"parameters\":{\"values\":{\"string\":[{\"name\":\"page\",\"value\":\"={{$node[\\\"Set\\\"].json[\\\"page\\\"]++}}\"}]},\"options\":{}},\"executeOnce\":true,\"typeVersion\":1},{\"id\":\"9f0df828-27d7-4994-8934-c8fe88af8566\",\"name\":\"HTTP Request - Get Mock Albums\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[1345,-15],\"parameters\":{\"url\":\"https://jsonplaceholder.typicode.com/albums\",\"options\":{\"response\":{\"response\":{\"fullResponse\":true}}}},\"typeVersion\":3},{\"id\":\"cbc64010-f6f4-4c35-b4e2-9e1d4a748308\",\"name\":\"HTTP Request - Get Wikipedia Page\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[1325,285],\"parameters\":{\"url\":\"https://en.wikipedia.org/wiki/Special:Random\",\"options\":{\"redirect\":{\"redirect\":{\"followRedirects\":true}},\"response\":{\"response\":{\"responseFormat\":\"file\"}}}},\"typeVersion\":3},{\"id\":\"a1a19268-0be8-4379-99a4-4285c68691b5\",\"name\":\"HTTP Request - Get my Stars\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[1525,725],\"parameters\":{\"url\":\"=https://api.github.com/users/{{$node[\\\"Set\\\"].json[\\\"githubUser\\\"]}}/starred\",\"options\":{\"response\":{\"response\":{\"fullResponse\":true}}},\"sendQuery\":true,\"queryParameters\":{\"parameters\":[{\"name\":\"per_page\",\"value\":\"={{$node[\\\"Set\\\"].json[\\\"perpage\\\"]}}\"},{\"name\":\"page\",\"value\":\"={{$node[\\\"Set\\\"].json[\\\"page\\\"]}}\"}]}},\"typeVersion\":3}],\"connections\":{\"Set\":{\"main\":[[{\"node\":\"HTTP Request - Get my Stars\",\"type\":\"main\",\"index\":0}]]},\"Set - Increment Page\":{\"main\":[[{\"node\":\"HTTP Request - Get my Stars\",\"type\":\"main\",\"index\":0}]]},\"If - Are we finished?\":{\"main\":[null,[{\"node\":\"Set - Increment Page\",\"type\":\"main\",\"index\":0}]]},\"On clicking 'execute'\":{\"main\":[[{\"node\":\"Set\",\"type\":\"main\",\"index\":0},{\"node\":\"HTTP Request - Get Mock Albums\",\"type\":\"main\",\"index\":0},{\"node\":\"HTTP Request - Get Wikipedia Page\",\"type\":\"main\",\"index\":0}]]},\"Item Lists - Fetch Body\":{\"main\":[[{\"node\":\"If - Are we finished?\",\"type\":\"main\",\"index\":0}]]},\"HTTP Request - Get my Stars\":{\"main\":[[{\"node\":\"Item Lists - Fetch Body\",\"type\":\"main\",\"index\":0}]]},\"HTTP Request - Get Mock Albums\":{\"main\":[[{\"node\":\"Item Lists - Create Items from Body\",\"type\":\"main\",\"index\":0}]]},\"HTTP Request - Get Wikipedia Page\":{\"main\":[[{\"node\":\"HTML Extract - Extract Article Title\",\"type\":\"main\",\"index\":0}]]}}}",
  "readme": "You still can use the app in a workflow even if we don’t have a node for that or the existing operation for that. With the HTTP Request node, it is possible to call any API point and use the incoming data in your workflow\n\n**Main use cases:**\n\n  * Connect with apps and services that n8n doesn’t have integration with\n  * Web scraping\n\n\n\n**How it works**  \nThis workflow can be divided into three branches, each serving a distinct purpose:\n\n1.Splitting into Items (HTTP Request - Get Mock Albums):\n\n  * The workflow initiates with a manual trigger (On clicking 'execute').\n  * It performs an HTTP request to retrieve mock albums data from \"<https://jsonplaceholder.typicode.com/albums.>\"\n  * The obtained data is split into items using the Item Lists node, facilitating easier management.\n\n\n\n2.Data Scraping (HTTP Request - Get Wikipedia Page and HTML Extract):\n\n  * Another branch of the workflow involves fetching a random Wikipedia page using an HTTP request to \"<https://en.wikipedia.org/wiki/Special:Random.>\"\n  * The HTML Extract node extracts the article title from the fetched Wikipedia page.\n\n\n\n3.Handling Pagination (The final branch deals with handling pagination for a GitHub API request):\n\n  * It sends an HTTP request to \"<https://api.github.com/users/that-one-tom/starred,>\" with parameters like the page number and items per page dynamically set by the Set node.\n  * The workflow uses conditions (If - Are we finished?) to check if there are more pages to retrieve and increments the page number accordingly (Set - Increment Page).\n  * This process repeats until all pages are fetched, allowing for comprehensive data retrieval.\n\n\n",
  "readme_html": "<!--[--><div data-v-50766329=\"\"><p>You still can use the app in a workflow even if we don’t have a node for that or the existing operation for that. With the HTTP Request node, it is possible to call any API point and use the incoming data in your workflow</p>\n<p><strong>Main use cases:</strong></p>\n<ul>\n<li>Connect with apps and services that n8n doesn’t have integration with</li>\n<li>Web scraping</li>\n</ul>\n<p><strong>How it works</strong><br>\nThis workflow can be divided into three branches, each serving a distinct purpose:</p>\n<p>1.Splitting into Items (HTTP Request - Get Mock Albums):</p>\n<ul>\n<li>The workflow initiates with a manual trigger (On clicking 'execute').</li>\n<li>It performs an HTTP request to retrieve mock albums data from \"<a href=\"https://jsonplaceholder.typicode.com/albums.\" rel=\"ugc nofollow\" target=\"_blank\">https://jsonplaceholder.typicode.com/albums.</a>\"</li>\n<li>The obtained data is split into items using the Item Lists node, facilitating easier management.</li>\n</ul>\n<p>2.Data Scraping (HTTP Request - Get Wikipedia Page and HTML Extract):</p>\n<ul>\n<li>Another branch of the workflow involves fetching a random Wikipedia page using an HTTP request to \"<a href=\"https://en.wikipedia.org/wiki/Special:Random.\" rel=\"ugc nofollow\" target=\"_blank\">https://en.wikipedia.org/wiki/Special:Random.</a>\"</li>\n<li>The HTML Extract node extracts the article title from the fetched Wikipedia page.</li>\n</ul>\n<p>3.Handling Pagination (The final branch deals with handling pagination for a GitHub API request):</p>\n<ul>\n<li>It sends an HTTP request to \"<a href=\"https://api.github.com/users/that-one-tom/starred,\" rel=\"ugc nofollow\" target=\"_blank\">https://api.github.com/users/that-one-tom/starred,</a>\" with parameters like the page number and items per page dynamically set by the Set node.</li>\n<li>The workflow uses conditions (If - Are we finished?) to check if there are more pages to retrieve and increments the page number accordingly (Set - Increment Page).</li>\n<li>This process repeats until all pages are fetched, allowing for comprehensive data retrieval.</li>\n</ul>\n</div><!--]-->",
  "readme_zh": "即使我们没有现成的节点或对应操作，您仍可在工作流中使用该应用。通过HTTP请求节点，您可以调用任何API接口，并将返回数据用于工作流中。\n\n**主要应用场景：**\n  * 连接n8n尚未集成的应用程序和服务\n  * 网络数据抓取\n\n**运作原理**  \n本工作流可分为三个功能分支：\n\n1.数据项拆分（HTTP请求-获取模拟专辑）：\n  * 工作流始于手动触发（点击\"执行\"按钮）\n  * 向<https://jsonplaceholder.typicode.com/albums>发起HTTP请求获取模拟专辑数据\n  * 通过\"项目列表\"节点将数据拆分为独立项以便管理\n\n2.网页抓取（HTTP请求-获取维基百科页面及HTML提取）：\n  * 另一分支向<https://en.wikipedia.org/wiki/Special:Random>发起请求获取随机维基百科页面\n  * 使用HTML提取节点从页面源码中抓取文章标题\n\n3.分页处理（最终分支处理GitHub API的分页请求）：\n  * 向<https://api.github.com/users/that-one-tom/starred>发送请求，其中页码和每页项数由Set节点动态设置\n  * 通过条件判断（If-是否完成？）检查是否还有后续页面，并相应递增页码（Set-页码递增）\n  * 该循环持续直至获取所有页面，实现完整数据采集",
  "title_zh": "从n8n尚未内置集成的服务中提取数据",
  "publish_date_zh": "最后更新于10个月前",
  "workflow_json_zh": "{\"meta\":{\"instanceId\":\"8c8c5237b8e37b006a7adce87f4369350c58e41f3ca9de16196d3197f69eabcd\"},\"nodes\":[{\"id\":\"25ac6cda-31fb-474a-b6b6-083ec03b9273\",\"name\":\"On clicking 'execute'\",\"type\":\"n8n-nodes-base.manualTrigger\",\"position\":[925,285],\"parameters\":{},\"typeVersion\":1},{\"id\":\"93eaee43-7a39-4c83-aeaa-9ca14d0f4b4b\",\"name\":\"Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[380,240],\"parameters\":{\"width\":440,\"height\":200,\"content\":\"## HTTP请求\\n### 本工作流展示了HTTP请求节点最常见的用例，以及如何处理其输出\\n\\n\\n### 点击`执行工作流`按钮并双击节点查看输入和输出项\"},\"typeVersion\":1},{\"id\":\"3ccdc45b-aae1-4760-b45e-5b8dca2a9fcf\",\"name\":\"Note2\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1280,480],\"parameters\":{\"width\":986.3743856726365,\"height\":460.847917534361,\"content\":\"## 3. 处理分页\\n### 有时您需要多次发起相同请求以获取全部所需数据（分页场景）\\n\\n### 分页处理流程如下：\\n### 1. 遍历输入源的分页（名为\\\"获取我的收藏\\\"的`HTTP请求`节点）\\n### 2. 每次循环结束时递增页码（通过名为\\\"页码递增\\\"的`set`节点实现）\\n### 3. 当无剩余页码时终止循环（由名为\\\"是否完成？\\\"的`If`节点进行判断）\"},\"typeVersion\":1},{\"id\":\"af19bb6d-5f0a-41ca-93b2-dbd27c3fd07e\",\"name\":\"Set\",\"type\":\"n8n-nodes-base.set\",\"position\":[1345,725],\"parameters\":{\"values\":{\"number\":[{\"name\":\"page\"},{\"name\":\"perpage\",\"value\":15}],\"string\":[{\"name\":\"githubUser\",\"value\":\"that-one-tom\"}]},\"options\":{}},\"typeVersion\":1},{\"id\":\"dad6055d-e06b-4f8c-ab90-deb196fce277\",\"name\":\"Note6\",\"type\":\"n8n-nodes-base.stickyNote\",\"disabled\":true,\"position\":[1280,180],\"parameters\":{\"width\":680,\"height\":280,\"content\":\"## 2. 数据抓取\\n### 在本示例中，我们使用`HTTP请求`节点获取n8n博客的标题，然后通过`HTML提取`节点进行解析传递。\"},\"typeVersion\":1},{\"id\":\"a7d4b9db-4d38-4b8d-9585-fe65c379e381\",\"name\":\"Note1\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[1280,-120],\"parameters\":{\"width\":500,\"height\":280,\"content\":\"## 1. 拆分为条目\\n### 在本示例中，我们从`HTTP请求`节点获取主体内容，并将其拆分为更易于管理的条目。\"},\"typeVersion\":1},{\"id\":\"d8402820-fa72-4957-8cf6-432f928ae799\",\"name\":\"Item Lists - Create Items from Body\",\"type\":\"n8n-nodes-base.itemLists\",\"notes\":\"Create Items from Body\",\"position\":[1525,-15],\"parameters\":{\"options\":{},\"fieldToSplitOut\":\"body\"},\"notesInFlow\":false,\"typeVersion\":1},{\"id\":\"598939cd-e4c0-4a90-bd1f-f2b13ccbe072\",\"name\":\"HTML Extract - Extract Article Title\",\"type\":\"n8n-nodes-base.htmlExtract\",\"position\":[1505,285],\"parameters\":{\"options\":{},\"sourceData\":\"binary\",\"extractionValues\":{\"values\":[{\"key\":\"ArticleTitle\",\"cssSelector\":\"#firstHeading\"}]}},\"typeVersion\":1},{\"id\":\"1c9b609c-5e41-4444-ade7-e1069943c904\",\"name\":\"Item Lists - Fetch Body\",\"type\":\"n8n-nodes-base.itemLists\",\"position\":[1705,725],\"parameters\":{\"options\":{},\"fieldToSplitOut\":\"body\"},\"typeVersion\":1,\"alwaysOutputData\":true},{\"id\":\"15dfab42-440c-4d06-9ba2-b7b17371d009\",\"name\":\"If - Are we finished?\",\"type\":\"n8n-nodes-base.if\",\"position\":[1885,725],\"parameters\":{\"conditions\":{\"string\":[{\"value1\":\"={{$node[\\\"HTTP Request - Get my Stars\\\"].json[\\\"body\\\"]}}\",\"operation\":\"isEmpty\"}]}},\"executeOnce\":true,\"typeVersion\":1},{\"id\":\"ba6e6904-6749-4ea2-84c1-8409b795bcf5\",\"name\":\"Set - Increment Page\",\"type\":\"n8n-nodes-base.set\",\"position\":[2105,745],\"parameters\":{\"values\":{\"string\":[{\"name\":\"page\",\"value\":\"={{$node[\\\"Set\\\"].json[\\\"page\\\"]++}}\"}]},\"options\":{}},\"executeOnce\":true,\"typeVersion\":1},{\"id\":\"9f0df828-27d7-4994-8934-c8fe88af8566\",\"name\":\"HTTP Request - Get Mock Albums\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[1345,-15],\"parameters\":{\"url\":\"https://jsonplaceholder.typicode.com/albums\",\"options\":{\"response\":{\"response\":{\"fullResponse\":true}}}},\"typeVersion\":3},{\"id\":\"cbc64010-f6f4-4c35-b4e2-9e1d4a748308\",\"name\":\"HTTP Request - Get Wikipedia Page\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[1325,285],\"parameters\":{\"url\":\"https://en.wikipedia.org/wiki/Special:Random\",\"options\":{\"redirect\":{\"redirect\":{\"followRedirects\":true}},\"response\":{\"response\":{\"responseFormat\":\"file\"}}}},\"typeVersion\":3},{\"id\":\"a1a19268-0be8-4379-99a4-4285c68691b5\",\"name\":\"HTTP Request - Get my Stars\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[1525,725],\"parameters\":{\"url\":\"=https://api.github.com/users/{{$node[\\\"Set\\\"].json[\\\"githubUser\\\"]}}/starred\",\"options\":{\"response\":{\"response\":{\"fullResponse\":true}}},\"sendQuery\":true,\"queryParameters\":{\"parameters\":[{\"name\":\"per_page\",\"value\":\"={{$node[\\\"Set\\\"].json[\\\"perpage\\\"]}}\"},{\"name\":\"page\",\"value\":\"={{$node[\\\"Set\\\"].json[\\\"page\\\"]}}\"}]}},\"typeVersion\":3}],\"connections\":{\"Set\":{\"main\":[[{\"node\":\"HTTP Request - Get my Stars\",\"type\":\"main\",\"index\":0}]]},\"Set - Increment Page\":{\"main\":[[{\"node\":\"HTTP Request - Get my Stars\",\"type\":\"main\",\"index\":0}]]},\"If - Are we finished?\":{\"main\":[null,[{\"node\":\"Set - Increment Page\",\"type\":\"main\",\"index\":0}]]},\"On clicking 'execute'\":{\"main\":[[{\"node\":\"Set\",\"type\":\"main\",\"index\":0},{\"node\":\"HTTP Request - Get Mock Albums\",\"type\":\"main\",\"index\":0},{\"node\":\"HTTP Request - Get Wikipedia Page\",\"type\":\"main\",\"index\":0}]]},\"Item Lists - Fetch Body\":{\"main\":[[{\"node\":\"If - Are we finished?\",\"type\":\"main\",\"index\":0}]]},\"HTTP Request - Get my Stars\":{\"main\":[[{\"node\":\"Item Lists - Fetch Body\",\"type\":\"main\",\"index\":0}]]},\"HTTP Request - Get Mock Albums\":{\"main\":[[{\"node\":\"Item Lists - Create Items from Body\",\"type\":\"main\",\"index\":0}]]},\"HTTP Request - Get Wikipedia Page\":{\"main\":[[{\"node\":\"HTML Extract - Extract Article Title\",\"type\":\"main\",\"index\":0}]]}}}"
}