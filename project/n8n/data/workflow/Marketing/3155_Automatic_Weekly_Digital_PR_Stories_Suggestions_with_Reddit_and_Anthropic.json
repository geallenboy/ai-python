{
  "title": "Automatic Weekly Digital PR Stories Suggestions with Reddit and Anthropic",
  "url": "https://n8n.io/workflows/3155-automatic-weekly-digital-pr-stories-suggestions-with-reddit-and-anthropic/",
  "category": "Marketing",
  "category_url": "https://n8n.io/workflows/categories/marketing/?sort=createdAt:desc",
  "author": "Custom Workflows AI",
  "publish_date": "Last update 2 months ago",
  "publish_date_absolute": "2025-03-19",
  "content": "",
  "workflow_json": "{\"id\":\"h2uiciRa1D3ntSTT\",\"meta\":{\"instanceId\":\"ddfdf733df99a65c801a91865dba5b7c087c95cc22a459ff3647e6deddf2aee6\"},\"name\":\"My workflow\",\"tags\":[],\"nodes\":[{\"id\":\"4b885b7d-0976-4dd3-bc1c-091ab0dff437\",\"name\":\"Split Topics into Items\",\"type\":\"n8n-nodes-base.code\",\"position\":[420,420],\"parameters\":{\"jsCode\":\"// Input data (from $json.Topics)\\nconst topicsString = $json.Topics;\\n\\n// Split the string by newlines and trim whitespace\\nconst topicsArray = topicsString.split('\\\\n').map(topic => topic.trim());\\n\\n// Create an array of items for each topic\\nconst items = topicsArray.map(topic => {\\n  return { json: { Topic: topic } };\\n});\\n\\n// Output the new array of items\\nreturn items;\\n\"},\"typeVersion\":2},{\"id\":\"935d0266-feda-48cb-b441-b4da19d8b163\",\"name\":\"Search Posts\",\"type\":\"n8n-nodes-base.reddit\",\"position\":[620,420],\"parameters\":{\"keyword\":\"meta\",\"location\":\"allReddit\",\"operation\":\"search\",\"returnAll\":true,\"additionalFields\":{\"sort\":\"hot\"}},\"typeVersion\":1},{\"id\":\"cea577c8-c025-4132-926a-74d6946d81b8\",\"name\":\"Upvotes Requirement Filtering\",\"type\":\"n8n-nodes-base.if\",\"position\":[800,420],\"parameters\":{\"options\":{},\"conditions\":{\"options\":{\"version\":2,\"leftValue\":\"\",\"caseSensitive\":true,\"typeValidation\":\"strict\"},\"combinator\":\"and\",\"conditions\":[{\"id\":\"f767f7a8-a2e8-4566-be80-bd735249e069\",\"operator\":{\"type\":\"number\",\"operation\":\"gt\"},\"leftValue\":\"={{ $json.ups }}\",\"rightValue\":100},{\"id\":\"3af82bef-5a78-4e6e-91ef-a5bd0141c87f\",\"operator\":{\"name\":\"filter.operator.equals\",\"type\":\"string\",\"operation\":\"equals\"},\"leftValue\":\"={{ $json.post_hint }}\",\"rightValue\":\"link\"},{\"id\":\"980a84ed-d640-47a7-b49a-bf638e811f20\",\"operator\":{\"type\":\"string\",\"operation\":\"notContains\"},\"leftValue\":\"={{ $json.url }}\",\"rightValue\":\"bsky.app\"}]}},\"typeVersion\":2.2},{\"id\":\"eec2d833-9a63-4cf6-a6bd-56b300ede5e0\",\"name\":\"Set Reddit Posts\",\"type\":\"n8n-nodes-base.set\",\"position\":[1040,420],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"8d5ae4fa-2f54-48d7-8f61-766f4ecf9d96\",\"name\":\"Title\",\"type\":\"string\",\"value\":\"={{ $json.title }}\"},{\"id\":\"8eb33a06-d8e7-4eea-bcd3-f956e20e06e6\",\"name\":\"Subreddit\",\"type\":\"string\",\"value\":\"={{ $json.subreddit }}\"},{\"id\":\"5ff8c76e-a8d5-4f76-a7d0-faa69b7960e4\",\"name\":\"Upvotes\",\"type\":\"string\",\"value\":\"={{ $json.ups }}\"},{\"id\":\"05a2b453-0e29-4a81-8f10-5934ae721f64\",\"name\":\"Comments\",\"type\":\"string\",\"value\":\"={{ $json.num_comments }}\"},{\"id\":\"78f73e89-19a7-4dd5-9db0-ead55dfd5606\",\"name\":\"Reddit URL\",\"type\":\"string\",\"value\":\"=https://www.reddit.com{{ $json.permalink }}\"},{\"id\":\"6f92bce7-2dc5-4dfd-b216-efc12c5411bb\",\"name\":\"URL\",\"type\":\"string\",\"value\":\"={{ $json.url }}\"},{\"id\":\"0b20d78c-1d6b-4c84-99ef-978ee39fd35e\",\"name\":\"Is_URL\",\"type\":\"string\",\"value\":\"={{ $json.post_hint }}\"},{\"id\":\"489807f6-25ef-47d5-bd47-711ca75dedea\",\"name\":\"Date\",\"type\":\"string\",\"value\":\"={{ new Date($json.created * 1000).toISOString().split('T')[0] }}\"},{\"id\":\"0a9fb817-bfb7-4ea7-9182-1eddc404035f\",\"name\":\"Post ID\",\"type\":\"string\",\"value\":\"={{ $json.id }}\"}]}},\"typeVersion\":3.4},{\"id\":\"9b45abb0-866a-47f4-b2b3-03e4cf41c988\",\"name\":\"Remove Duplicates\",\"type\":\"n8n-nodes-base.code\",\"position\":[1220,420],\"parameters\":{\"jsCode\":\"// Get all input items\\nconst inputItems = $input.all();\\n\\n// Create a Map to store the most upvoted item for each URL\\nconst uniqueItemsMap = new Map();\\n\\nfor (const item of inputItems) {\\n  const url = item.json.URL;\\n  \\n  // Skip items where URL contains \\\"redd.it\\\"\\n  if (url && url.includes(\\\"redd.it\\\")) {\\n    continue;\\n  }\\n  \\n  const upvotes = parseInt(item.json.Upvotes, 10) || 0; // Ensure upvotes is a number\\n\\n  if (!uniqueItemsMap.has(url)) {\\n    // Add the first occurrence of the URL\\n    uniqueItemsMap.set(url, item);\\n  } else {\\n    // Compare upvotes and keep the item with the most upvotes\\n    const existingItem = uniqueItemsMap.get(url);\\n    const existingUpvotes = parseInt(existingItem.json.Upvotes, 10) || 0;\\n    if (upvotes > existingUpvotes) {\\n      uniqueItemsMap.set(url, item);\\n    }\\n  }\\n}\\n\\n// Extract all unique items\\nconst uniqueItems = Array.from(uniqueItemsMap.values());\\n\\n// Return each unique item as a separate output\\nreturn uniqueItems;\"},\"typeVersion\":2},{\"id\":\"39672fd4-3f8c-4cdb-acd5-bb862ae5eddd\",\"name\":\"Loop Over Items\",\"type\":\"n8n-nodes-base.splitInBatches\",\"position\":[40,660],\"parameters\":{\"options\":{}},\"typeVersion\":3},{\"id\":\"ad70aec7-a610-42f8-b87c-0d3dbee00e7b\",\"name\":\"Get Comments\",\"type\":\"n8n-nodes-base.reddit\",\"position\":[480,640],\"parameters\":{\"postId\":\"={{ $json[\\\"Post ID\\\"] }}\",\"resource\":\"postComment\",\"operation\":\"getAll\",\"subreddit\":\"={{ $json.Subreddit }}\"},\"typeVersion\":1},{\"id\":\"af7f0b35-4250-49e5-afa7-608155df0fd5\",\"name\":\"Extract Top Comments\",\"type\":\"n8n-nodes-base.code\",\"position\":[660,640],\"parameters\":{\"jsCode\":\"/**\\n * n8n Code Node for filtering top 30 Reddit-style comments by score/ups\\n * and ensuring replies are included in the comment tree.\\n * Excludes deleted comments.\\n */\\n\\n// Get all input items\\nconst inputItems = $input.all();\\nconst commentsArray = inputItems.flatMap(item => item.json);\\n\\n/**\\n * Checks if a comment is deleted.\\n * @param {Object} commentObj - The comment to check.\\n * @returns {boolean} - True if the comment is deleted, false otherwise.\\n */\\nfunction isDeletedComment(commentObj) {\\n  return commentObj.author === \\\"[deleted]\\\" && commentObj.body === \\\"[removed]\\\";\\n}\\n\\n// Function to recursively flatten a comment and its replies\\nfunction flattenCommentTree(commentObj) {\\n  // Skip deleted comments\\n  if (isDeletedComment(commentObj)) {\\n    return null;\\n  }\\n\\n  const { body, ups, score, replies, author } = commentObj;\\n\\n  // Calculate score\\n  const finalScore = typeof ups === 'number' ? ups : (score || 0);\\n\\n  // Process comment\\n  const flatComment = {\\n    body: body || '',\\n    score: finalScore,\\n    author: author || 'Unknown',\\n    replies: [],\\n  };\\n\\n  // Process replies\\n  if (\\n    replies &&\\n    replies.data &&\\n    Array.isArray(replies.data.children)\\n  ) {\\n    flatComment.replies = replies.data.children\\n      .filter(child => child.kind === 't1' && child.data)\\n      .map(child => flattenCommentTree(child.data)) // Recursively flatten replies\\n      .filter(reply => reply !== null); // Filter out null replies (deleted comments)\\n  }\\n\\n  return flatComment;\\n}\\n\\n// Flatten all comments, preserving hierarchy\\nconst allComments = commentsArray\\n  .map(flattenCommentTree)\\n  .filter(comment => comment !== null); // Filter out null comments (deleted comments)\\n\\n// Flatten the hierarchy to a list for scoring and filtering\\nfunction flattenForScoring(tree) {\\n  const result = [];\\n  tree.forEach(comment => {\\n    result.push(comment); // Add current comment\\n    if (comment.replies && comment.replies.length > 0) {\\n      result.push(...flattenForScoring(comment.replies)); // Add replies recursively\\n    }\\n  });\\n  return result;\\n}\\n\\n// Flatten the hierarchy and sort by score\\nconst flatList = flattenForScoring(allComments);\\nflatList.sort((a, b) => b.score - a.score);\\n\\n// Select the top 30 comments\\nconst top30 = flatList.slice(0, 30);\\n\\n// Rebuild the hierarchy from the top 30\\nfunction filterHierarchy(tree, allowedBodies) {\\n  return tree\\n    .filter(comment => allowedBodies.has(comment.body))\\n    .map(comment => ({\\n      ...comment,\\n      replies: filterHierarchy(comment.replies || [], allowedBodies), // Recurse for replies\\n    }));\\n}\\n\\nconst allowedBodies = new Set(top30.map(comment => comment.body));\\nconst filteredHierarchy = filterHierarchy(allComments, allowedBodies);\\n\\n// Return in n8n format\\nreturn [\\n  {\\n    json: {\\n      comments: filteredHierarchy,\\n    },\\n  },\\n];\"},\"executeOnce\":true,\"typeVersion\":2},{\"id\":\"e709d131-b8fa-42d5-bc66-479cb13574e6\",\"name\":\"Format Comments\",\"type\":\"n8n-nodes-base.code\",\"position\":[840,640],\"parameters\":{\"jsCode\":\"/**\\n * Convert comments data into Markdown format with accurate hierarchy visualization.\\n * Excludes deleted comments.\\n */\\n\\n// Input data (replace this with your actual comments data)\\nconst data = $input.all()[0].json.comments;\\n\\n/**\\n * Checks if a comment is deleted.\\n * @param {Object} comment - The comment to check.\\n * @returns {boolean} - True if the comment is deleted, false otherwise.\\n */\\nfunction isDeletedComment(comment) {\\n  return comment.author === \\\"[deleted]\\\" && comment.body === \\\"[removed]\\\";\\n}\\n\\n/**\\n * Filters out deleted comments and their replies.\\n * @param {Array} comments - Array of comments.\\n * @returns {Array} - Filtered array of comments.\\n */\\nfunction filterDeletedComments(comments) {\\n  if (!comments || !comments.length) return [];\\n  \\n  return comments\\n    .filter(comment => !isDeletedComment(comment))\\n    .map(comment => {\\n      if (comment.replies && comment.replies.length > 0) {\\n        comment.replies = filterDeletedComments(comment.replies);\\n      }\\n      return comment;\\n    });\\n}\\n\\n/**\\n * Recursive function to format comments and replies into Markdown.\\n * @param {Array} comments - Array of comments.\\n * @param {number} level - Current level of the comment hierarchy for indentation.\\n * @returns {string} - Formatted Markdown string.\\n */\\nfunction formatCommentsToMarkdown(comments, level = 0) {\\n  let markdown = '';\\n  const indent = '  '.repeat(level); // Indentation for replies\\n\\n  for (const comment of comments) {\\n    // Format the main comment\\n    markdown += `${indent}- **Author**: ${comment.author}\\\\n`;\\n    markdown += `${indent}  **Score**: ${comment.score}\\\\n`;\\n    markdown += `${indent}  **Comment**:\\\\n\\\\n`;\\n    markdown += `${indent}    > ${comment.body.replace(/\\\\n/g, `\\\\n${indent}    > `)}\\\\n\\\\n`;\\n\\n    // Process replies if they exist\\n    if (comment.replies && comment.replies.length > 0) {\\n      markdown += `${indent}  **Replies:**\\\\n\\\\n`;\\n      markdown += formatCommentsToMarkdown(comment.replies, level + 1);\\n    }\\n  }\\n\\n  return markdown;\\n}\\n\\n// Filter out deleted comments first\\nconst filteredData = filterDeletedComments(data);\\n\\n// Generate the Markdown\\nconst markdownOutput = formatCommentsToMarkdown(filteredData);\\n\\n// Return the Markdown as an output for n8n\\nreturn [\\n  {\\n    json: {\\n      markdown: markdownOutput,\\n    },\\n  },\\n];\"},\"typeVersion\":2},{\"id\":\"284d511b-7d80-46ba-add0-6ff59aff176c\",\"name\":\"Set for Loop\",\"type\":\"n8n-nodes-base.set\",\"position\":[280,640],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"ac7c257d-544f-44e5-abc6-d0436f12517f\",\"name\":\"Title\",\"type\":\"string\",\"value\":\"={{ $json.Title }}\"},{\"id\":\"fb22c6a5-a809-4588-9f6e-49c3e11f5ed2\",\"name\":\"Subreddit\",\"type\":\"string\",\"value\":\"={{ $json.Subreddit }}\"},{\"id\":\"4bfcc849-539b-48cd-856f-1b7f3be113ed\",\"name\":\"Upvotes\",\"type\":\"string\",\"value\":\"={{ $json.Upvotes }}\"},{\"id\":\"9a3a3a2a-8f43-4419-9203-bc83f5b0c0bc\",\"name\":\"Comments\",\"type\":\"string\",\"value\":\"={{ $json.Comments }}\"},{\"id\":\"2d31f321-fbdc-43d3-8a92-a78f418f112f\",\"name\":\"Reddit URL\",\"type\":\"string\",\"value\":\"={{ $json[\\\"Reddit URL\\\"] }}\"},{\"id\":\"f224323a-79ef-4f66-ae10-d77c8fddbccd\",\"name\":\"URL\",\"type\":\"string\",\"value\":\"={{ $json.URL }}\"},{\"id\":\"dbbc5a98-b5e2-45bb-bc18-2c438522d683\",\"name\":\"Date\",\"type\":\"string\",\"value\":\"={{ $json.Date }}\"},{\"id\":\"837cae4e-858a-48ba-bab9-bb66a2e51837\",\"name\":\"Post ID\",\"type\":\"string\",\"value\":\"={{ $json[\\\"Post ID\\\"] }}\"}]}},\"typeVersion\":3.4},{\"id\":\"b88fad49-edc4-4749-8984-a8e81f6a2899\",\"name\":\"Get News Content\",\"type\":\"n8n-nodes-base.httpRequest\",\"maxTries\":5,\"position\":[1360,640],\"parameters\":{\"url\":\"=https://r.jina.ai/{{ $('Set for Loop').first().json.URL }}\",\"options\":{},\"sendHeaders\":true,\"headerParameters\":{\"parameters\":[{\"name\":\"Accept\",\"value\":\"text/event-stream\"},{\"name\":\"Authorization\",\"value\":\"=Bearer {{ $('Set Data').first().json['Jina API Key'] }}\"},{\"name\":\"X-Retain-Images\",\"value\":\"none\"},{\"name\":\"X-Respond-With\",\"value\":\"readerlm-v2\"},{\"name\":\"X-Remove-Selector\",\"value\":\"header, footer, sidebar\"}]}},\"retryOnFail\":true,\"typeVersion\":4.2,\"waitBetweenTries\":5000},{\"id\":\"26a8906c-2966-4ebf-8465-18a48b359f7d\",\"name\":\"Set Final Report\",\"type\":\"n8n-nodes-base.set\",\"position\":[2400,640],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"0782b9a6-d659-4695-8696-6ff0e574f77a\",\"name\":\"Final Report\",\"type\":\"string\",\"value\":\"=// Reddit Metrics:\\nPost Link: {{ $('Set for Loop').first().json['Reddit URL'] }}\\nUpvotes: {{ $('Set for Loop').first().json.Upvotes }}\\nComments: {{ $('Set for Loop').first().json.Comments }}\\n\\n# FINAL REPORT\\n{{ $json.text.replace(/[\\\\s\\\\S]*<new_stories_report>/, '').replace(/<\\\\/new_stories_report>[\\\\s\\\\S]*/, '') }}\\n\\n# RAW ANALYSIS DATA (FOR FURTHER ANALYSIS)\\n\\n## NEWS CONTENT ANALYSIS\\n{{ $('News Analysis').item.json.text.replace(/[\\\\s\\\\S]*<news_analysis>/, '').replace(/<\\\\/news_analysis>[\\\\s\\\\S]*/, '') }}\\n\\n## REDDIT COMMENTS ANALYSIS\\n{{ $('Comments Analysis').first().json.text.replace(/[\\\\s\\\\S]*<comments_analysis>/, '').replace(/<\\\\/comments_analysis>[\\\\s\\\\S]*/, '') }}\"}]}},\"typeVersion\":3.4},{\"id\":\"219ccb20-1b36-4c70-866a-0fded9c9b9fd\",\"name\":\"Convert to File\",\"type\":\"n8n-nodes-base.convertToFile\",\"position\":[2580,640],\"parameters\":{\"options\":{\"encoding\":\"utf8\",\"fileName\":\"={{ $json[\\\"Final Report\\\"].match(/Headline:\\\\s*[\\\"â€œ](.*?)[\\\"â€]/i)?.[1] }}.txt\"},\"operation\":\"toText\",\"sourceProperty\":\"Final Report\"},\"typeVersion\":1.1},{\"id\":\"427d5a2d-6927-4427-9902-e033736410ca\",\"name\":\"Compress files\",\"type\":\"n8n-nodes-base.compression\",\"position\":[600,940],\"parameters\":{\"fileName\":\"=Trending_Stories_{{$now.format(\\\"yyyy_MM_dd\\\")}}_{{Math.floor(Math.random() * 10000).toString().padStart(4, '0')}}.zip\",\"operation\":\"compress\",\"outputFormat\":\"zip\",\"binaryPropertyName\":\"={{ $json[\\\"binary_keys\\\"] }}\",\"binaryPropertyOutput\":\"files_combined\"},\"typeVersion\":1},{\"id\":\"7f6ef656-0f76-433f-95a8-782de21caa53\",\"name\":\"Merge Binary Files\",\"type\":\"n8n-nodes-base.code\",\"position\":[420,940],\"parameters\":{\"jsCode\":\"// Get the first (and only) item since you're using Aggregate\\nconst item = items[0];\\nlet binary_keys = [];\\n\\n// Generate the list of binary keys from your aggregated item\\nfor (let key in item.binary) {\\n    binary_keys.push(key);\\n}\\n\\nreturn [{\\n    json: {\\n        binary_keys: binary_keys.join(',')\\n    },\\n    binary: item.binary  // Keep the original binary data\\n}];\"},\"executeOnce\":true,\"typeVersion\":2},{\"id\":\"20411444-5ce8-452b-869c-97928200b205\",\"name\":\"Google Drive6\",\"type\":\"n8n-nodes-base.googleDrive\",\"position\":[780,940],\"parameters\":{\"driveId\":{\"__rl\":true,\"mode\":\"list\",\"value\":\"My Drive\",\"cachedResultUrl\":\"https://drive.google.com/drive/my-drive\",\"cachedResultName\":\"My Drive\"},\"options\":{},\"folderId\":{\"__rl\":true,\"mode\":\"id\",\"value\":\"1HCTq5YupRHcgRd7FIlSeUMMjqqOZ4Q9x\"},\"inputDataFieldName\":\"files_combined\"},\"typeVersion\":3},{\"id\":\"2eb8112a-8655-4f06-998f-a9ffef74d72a\",\"name\":\"Google Drive7\",\"type\":\"n8n-nodes-base.googleDrive\",\"position\":[960,940],\"parameters\":{\"fileId\":{\"__rl\":true,\"mode\":\"id\",\"value\":\"={{ $json.id }}\"},\"options\":{},\"operation\":\"share\",\"permissionsUi\":{\"permissionsValues\":{\"role\":\"reader\",\"type\":\"anyone\"}}},\"typeVersion\":3},{\"id\":\"7f4e5e0c-49cc-4024-b62b-f7e099d4867d\",\"name\":\"Send files to Mattermost3\",\"type\":\"n8n-nodes-base.httpRequest\",\"position\":[1140,940],\"parameters\":{\"url\":\"https://team.YOUR_DOMAIN.com/hooks/REPLACE_THIS_WITH_YOUR_HOOK_ID\",\"method\":\"POST\",\"options\":{},\"jsonBody\":\"={\\n    \\\"channel\\\": \\\"digital-pr\\\",\\n    \\\"username\\\": \\\"NotifyBot\\\",\\n    \\\"icon_url\\\": \\\"https://team.YOUR_DOMAIN.com/api/v4/users/YOUR_USER_ID/image?_=0\\\",\\n    \\\"text\\\": \\\"@channel New trending stories have been generated ðŸŽ‰\\\\n\\\\n\\\\n You can download it here: https://drive.google.com/file/d/{{ $('Google Drive6').item.json.id }}/view?usp=drive_link\\\"\\n}\",\"sendBody\":true,\"specifyBody\":\"json\"},\"typeVersion\":4.2},{\"id\":\"3c47f58d-8006-4565-b220-033d71239126\",\"name\":\"Aggregate\",\"type\":\"n8n-nodes-base.aggregate\",\"position\":[260,940],\"parameters\":{\"options\":{\"includeBinaries\":true},\"aggregate\":\"aggregateAllItemData\"},\"executeOnce\":false,\"typeVersion\":1},{\"id\":\"5611cdce-91ae-4037-9479-3b513eb07b77\",\"name\":\"Schedule Trigger\",\"type\":\"n8n-nodes-base.scheduleTrigger\",\"position\":[40,420],\"parameters\":{\"rule\":{\"interval\":[{\"field\":\"weeks\",\"triggerAtDay\":[1],\"triggerAtHour\":6}]}},\"typeVersion\":1.2},{\"id\":\"5cfeb9ea-45b6-4a0a-8702-34539738f280\",\"name\":\"Anthropic Chat Model\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatAnthropic\",\"position\":[960,800],\"parameters\":{\"model\":\"=claude-3-7-sonnet-20250219\",\"options\":{\"temperature\":0.5,\"maxTokensToSample\":8096}},\"typeVersion\":1.2},{\"id\":\"b11b2fa6-f92a-4791-b255-51ce1b07181b\",\"name\":\"Anthropic Chat Model1\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatAnthropic\",\"position\":[1640,800],\"parameters\":{\"model\":\"=claude-3-7-sonnet-20250219\",\"options\":{\"temperature\":0.5,\"maxTokensToSample\":8096}},\"typeVersion\":1.2},{\"id\":\"ffa45242-1dd4-46be-bacc-55bde63d0227\",\"name\":\"Keep Last\",\"type\":\"n8n-nodes-base.code\",\"position\":[1540,640],\"parameters\":{\"jsCode\":\"// Extract input data from n8n\\nconst inputData = $json.data;\\n\\n// Ensure input is valid\\nif (!inputData || typeof inputData !== 'string') {\\n    return [{ error: \\\"Invalid input data\\\" }];\\n}\\n\\n// Split the data into lines\\nlet lines = inputData.split(\\\"\\\\n\\\");\\n\\n// Extract only JSON entries\\nlet jsonEntries = lines\\n    .map(line => line.trim()) // Remove spaces\\n    .filter(line => line.startsWith('data: {')) // Keep valid JSON objects\\n    .map(line => line.replace('data: ', '')); // Remove the prefix\\n\\n// Ensure there are entries\\nif (jsonEntries.length === 0) {\\n    return [{ error: \\\"No valid JSON entries found\\\" }];\\n}\\n\\n// Get only the LAST entry\\nlet lastEntry = jsonEntries[jsonEntries.length - 1];\\n\\ntry {\\n    // Parse the last entry as JSON\\n    let jsonObject = JSON.parse(lastEntry);\\n\\n    // Extract title and content\\n    return [{\\n        title: jsonObject.title || \\\"No Title\\\",\\n        content: jsonObject.content || \\\"No Content\\\"\\n    }];\\n} catch (error) {\\n    return [{ error: \\\"JSON parsing failed\\\", raw: lastEntry }];\\n}\"},\"typeVersion\":2},{\"id\":\"956672cc-8ceb-4a2c-93e8-bad2b9497043\",\"name\":\"Anthropic Chat Model2\",\"type\":\"@n8n/n8n-nodes-langchain.lmChatAnthropic\",\"position\":[1980,800],\"parameters\":{\"model\":\"=claude-3-7-sonnet-20250219\",\"options\":{\"temperature\":0.5,\"maxTokensToSample\":8096}},\"typeVersion\":1.2},{\"id\":\"b55df80f-dbdf-4d8d-8b62-93533d1fb6ef\",\"name\":\"Sticky Note\",\"type\":\"n8n-nodes-base.stickyNote\",\"position\":[0,0],\"parameters\":{\"width\":1020,\"height\":340,\"content\":\"## Automatic Weekly Digital PR Stories Suggestions\\nA weekly automated system that identifies trending news on Reddit, evaluates public sentiment through comment analysis, extracts key information from source articles, and generates strategic angles for potential digital PR campaigns. This workflow delivers curated, sentiment-analyzed news opportunities based on current social media trends. The final comprehensive report is automatically uploaded to Google Drive for storage and simultaneously shared with team members via a dedicated Mattermost channel for immediate collaboration.\\n\\n### Set up instructions:\\n1. Add a new credential \\\"Reddit OAuth2 API\\\" by following this [guide](https://docs.n8n.io/integrations/builtin/credentials/reddit/). Assign your Reddit OAuth2 account to the Reddit nodes.\\n2. Add a new credential \\\"Anthropic Account\\\" by following this [guide]\\n(https://docs.n8n.io/integrations/builtin/credentials/anthropic/). Assign your Anthropic account to the nodes \\\"Anthropic Chat Model\\\".\\n3. Add a new credential \\\"Google Drive OAuth2 API\\\" by following this [guide](https://docs.n8n.io/integrations/builtin/credentials/google/oauth-single-service/). Assign your Google Drive OAuth2 account to the node \\\"Gmail Drive\\\" nodes.\\n4. Set your interested topics (one per line) and Jina API key in the \\\"Set Data\\\" node. You can obtain your Jina API key [here](https://jina.ai/api-dashboard/key-manager).\\n5. Update your Mattermost information (Mattermost instance URL, Webhook ID and Channel) in the Mattermost node. You can follow this [guide](https://developers.mattermost.com/integrate/webhooks/incoming/).\\n6. You can adjust the cron if needed. It currently run every Monday at 6am.\"},\"typeVersion\":1},{\"id\":\"07f1e0ff-892c-4aaf-ad77-e636138570a1\",\"name\":\"Comments Analysis\",\"type\":\"@n8n/n8n-nodes-langchain.chainLlm\",\"position\":[1020,640],\"parameters\":{\"text\":\"=Please analyze the following Reddit post and its comments:\\n\\nCONTEXT:\\n<Reddit_Post_Info>\\nPost Title: {{ $('Set for Loop').first().json.Title.replace(/\\\\\\\"/g, '\\\\\\\\\\\\\\\"') }}\\nPost Date: {{ $('Set for Loop').first().json.Date }}\\nShared URL: {{ $('Set for Loop').first().json.URL }}\\nTotal Upvotes: {{ $('Set for Loop').first().json.Upvotes }}\\nTotal Comments: {{ $('Set for Loop').first().json.Comments }}\\n</Reddit_Post_Info>\\n\\nComment Thread Data:\\n<Reddit_Post_Top_Comments>\\n{{ $json.markdown.replace(/\\\\\\\"/g, '\\\\\\\\\\\\\\\"') }}\\n</Reddit_Post_Top_Comments>\\n\\nAnalyze this discussion through these dimensions:\\n\\n1. CONTENT CONTEXT:\\n   â€¢ Main topic/subject matter\\n   â€¢ Why this is trending (based on engagement metrics)\\n   â€¢ News cycle timing implications\\n   â€¢ Relationship to broader industry/market trends\\n\\n2. SENTIMENT ANALYSIS:\\n   â€¢ Overall sentiment score (Scale: -5 to +5)\\n   â€¢ Primary emotional undertones\\n   â€¢ Sentiment progression in discussion threads\\n   â€¢ Consensus vs. controversial viewpoints\\n   â€¢ Changes in sentiment based on comment depth\\n\\n3. ENGAGEMENT INSIGHTS:\\n   â€¢ Most upvoted perspectives (with exact scores)\\n   â€¢ Controversial discussion points\\n   â€¢ Comment chains with deepest engagement\\n   â€¢ Types of responses generating most interaction\\n\\n4. NARRATIVE MAPPING:\\n   â€¢ Dominant narratives\\n   â€¢ Counter-narratives\\n   â€¢ Emerging sub-themes\\n   â€¢ Unexplored angles\\n   â€¢ Missing perspectives\\n\\nOutput Format (Place inside XML tags <comments_analysis>):\\n\\nPOST OVERVIEW:\\nTitle: [Original title]\\nEngagement Metrics:\\nâ€¢ Upvotes: [count]\\nâ€¢ Comments: [count]\\nâ€¢ Virality Assessment: [analysis of why this gained traction]\\n\\nSENTIMENT ANALYSIS:\\nâ€¢ Overall Score: [numerical score with explanation]\\nâ€¢ Sentiment Distribution: [percentage breakdown]\\nâ€¢ Key Emotional Drivers:\\n  - Primary: [emotion]\\n  - Secondary: [emotion]\\n  - Notable Shifts: [pattern analysis]\\n\\nTOP NARRATIVES:\\n[List 3-5 dominant narratives]\\nFor each narrative:\\nâ€¢ Key Points\\nâ€¢ Supporting Comments [with scores]\\nâ€¢ Counter-Arguments\\nâ€¢ Engagement Level\\n\\nAUDIENCE INSIGHTS:\\nâ€¢ Knowledge Level: [assessment]\\nâ€¢ Pain Points: [list key concerns]\\nâ€¢ Misconceptions: [list with evidence]\\nâ€¢ Information Gaps: [identified missing information]\\n\\nPR IMPLICATIONS:\\n1. Story Opportunities:\\n   â€¢ [List potential angles]\\n   â€¢ [Supporting evidence from comments]\\n\\n2. Risk Factors:\\n   â€¢ [List potential PR risks]\\n   â€¢ [Supporting evidence from comments]\\n\\n3. Narrative Recommendations:\\n   â€¢ [Strategic guidance for messaging]\\n   â€¢ [Areas to address/avoid]\\n\\nNEXT STEPS CONSIDERATIONS:\\nâ€¢ Key data points for content analysis\\nâ€¢ Suggested focus areas for PR story development\\nâ€¢ Critical elements to address in messaging\\nâ€¢ Potential expert perspectives needed\\n\\nMETA INSIGHTS:\\nâ€¢ Pattern connections to similar discussions\\nâ€¢ Unique aspects of this conversation\\nâ€¢ Viral elements to note\\nâ€¢ Community-specific nuances\\n\\nFocus on extracting insights that will:\\n1. Inform the subsequent content analysis step\\n2. Guide PR story development\\n3. Identify unique angles and opportunities\\n4. Highlight potential risks and challenges\\n5. Suggest effective narrative approaches\\n\\nNote: Prioritize insights that will be valuable for the following workflow steps of content analysis and PR story development. Flag any particularly unique or compelling elements that could inform breakthrough story angles.\",\"messages\":{\"messageValues\":[{\"message\":\"=You are an expert Social Media Intelligence Analyst specialized in Reddit discourse analysis. Your task is to analyze Reddit posts and comments to extract meaningful patterns, sentiments, and insights for PR strategy development.\"}]},\"promptType\":\"define\"},\"typeVersion\":1.5},{\"id\":\"4cdc4e49-6aae-4e6a-844e-c3c339638950\",\"name\":\"News Analysis\",\"type\":\"@n8n/n8n-nodes-langchain.chainLlm\",\"position\":[1720,640],\"parameters\":{\"text\":\"=CONTEXT IMPORTANCE:\\nReddit data is used as a critical indicator of news story potential because:\\nâ€¢ High upvotes indicate strong public interest\\nâ€¢ Comment volume shows discussion engagement\\nâ€¢ Comment sentiment reveals public perception\\nâ€¢ Discussion threads expose knowledge gaps and controversies\\nâ€¢ Community reaction predicts potential viral spread\\nâ€¢ Sub-discussions highlight unexplored angles\\nâ€¢ Engagement patterns suggest story longevity\\n\\nINPUT CONTEXT:\\nNews URL: {{ $('Set for Loop').first().json.URL }}\\nNews Content:\\n<News_Content>\\n{{ $json.content }}\\n</News_Content>\\nReddit Metrics:\\nâ€¢ Post Title (Understanding how the story was shared): {{ $('Set for Loop').first().json.Title }}\\nâ€¢ Upvotes (Indicator of initial interest): {{ $('Set for Loop').first().json.Upvotes }}\\nâ€¢ Total Comments (Engagement level): {{ $('Set for Loop').first().json.Comments }}\\nReddit Sentiment Analysis:\\n<Sentiment_Analysis>\\n{{ $('Comments Analysis').first().json.text.replace(/[\\\\s\\\\S]*<comments_analysis>/, '').replace(/<\\\\/comments_analysis>[\\\\s\\\\S]*/, '') }}\\n</Sentiment_Analysis>\\n\\nFor each story, analyze through these dimensions:\\n\\n1. POPULARITY ASSESSMENT:\\n   A. Reddit Performance:\\n      â€¢ Upvote ratio and volume\\n      â€¢ Comment engagement rate\\n      â€¢ Discussion quality metrics\\n      â€¢ Viral spread indicators\\n      \\n   B. Audience Reception:\\n      â€¢ Initial reaction patterns\\n      â€¢ Discussion evolution\\n      â€¢ Community consensus vs. debate\\n      â€¢ Information seeking behavior\\n\\n1. CONTENT ANALYSIS:\\n   A. Core Story Elements:\\n      â€¢ Central narrative\\n      â€¢ Key stakeholders\\n      â€¢ Market implications\\n      â€¢ Industry impact\\n      \\n   B. Technical Analysis:\\n      â€¢ Writing style\\n      â€¢ Data presentation\\n      â€¢ Expert citations\\n      â€¢ Supporting evidence\\n\\n2. SOCIAL PROOF INTEGRATION:\\n   A. Engagement Metrics:\\n      â€¢ Reddit performance metrics\\n      â€¢ Discussion quality indicators\\n      â€¢ Viral spread patterns\\n      \\n   B. Sentiment Patterns:\\n      â€¢ Primary audience reactions\\n      â€¢ Controversial elements\\n      â€¢ Support vs. criticism ratio\\n      â€¢ Knowledge gaps identified\\n\\n3. NARRATIVE OPPORTUNITY MAPPING:\\n   A. Current Coverage:\\n      â€¢ Main angles covered\\n      â€¢ Supporting arguments\\n      â€¢ Counter-arguments\\n      â€¢ Expert perspectives\\n      \\n   B. Gap Analysis:\\n      â€¢ Unexplored perspectives\\n      â€¢ Missing stakeholder voices\\n      â€¢ Underutilized data points\\n      â€¢ Potential counter-narratives\\n\\nOUTPUT FORMAT (Place inside XML tags <news_analysis>):\\n\\nSTORY OVERVIEW:\\nTitle: [Most compelling angle]\\nURL: [Source]\\nCategory: [Industry/Topic]\\n\\nCONTENT SUMMARY:\\nTLDR: [3-5 sentences emphasizing viral potential]\\nCore Message: [One-line essence]\\n\\nKEY POINTS:\\nâ€¢ [Strategic point 1]\\nâ€¢ [Strategic point 2]\\nâ€¢ [Continue as needed]\\n\\nSOCIAL PROOF ANALYSIS:\\nEngagement Metrics:\\nâ€¢ Reddit Performance: [Metrics + Interpretation]\\nâ€¢ Discussion Quality: [Analysis of conversation depth]\\nâ€¢ Sentiment Distribution: [From sentiment analysis]\\n\\nVIRAL ELEMENTS:\\n1. Current Drivers:\\n   â€¢ [What's making it spread]\\n   â€¢ [Why people are engaging]\\n   â€¢ [Emotional triggers identified]\\n\\n2. Potential Amplifiers:\\n   â€¢ [Untapped viral elements]\\n   â€¢ [Engagement opportunities]\\n   â€¢ [Emotional hooks not yet used]\\n\\nNARRATIVE OPPORTUNITIES:\\n1. Unexplored Angles:\\n   â€¢ [Angle 1 + Why it matters]\\n   â€¢ [Angle 2 + Why it matters]\\n   â€¢ [Angle 3 + Why it matters]\\n\\n2. Content Gaps:\\n   â€¢ [Missing perspectives]\\n   â€¢ [Underutilized data]\\n   â€¢ [Stakeholder voices needed]\\n\\n3. Controversy Points:\\n   â€¢ [Debate opportunities]\\n   â€¢ [Conflicting viewpoints]\\n   â€¢ [Areas of misconception]\\n\\nSTRATEGIC RECOMMENDATIONS:\\n1. Immediate Opportunities:\\n   â€¢ [Quick-win suggestions]\\n   â€¢ [Timing considerations]\\n\\n2. Development Needs:\\n   â€¢ [Required research]\\n   â€¢ [Expert input needed]\\n   â€¢ [Data gaps to fill]\\n\\nPR POTENTIAL SCORE: [1-10 scale with explanation]\\n\\nFocus on elements that:\\nâ€¢ Show strong viral potential\\nâ€¢ Address identified audience concerns\\nâ€¢ Fill gaps in current coverage\\nâ€¢ Leverage positive sentiment patterns\\nâ€¢ Address or utilize controversial elements\\nâ€¢ Can be developed into unique angles\\n\\nNote: Prioritize insights that:\\n1. Build on identified sentiment patterns\\n2. Address audience knowledge gaps\\n3. Leverage existing engagement drivers\\n4. Can create breakthrough narratives\\n5. Have immediate PR potential\",\"messages\":{\"messageValues\":[{\"message\":\"=You are an expert PR Content Analyst specialized in identifying viral potential in news stories. Your mission is to analyze news content while leveraging Reddit engagement metrics and sentiment data to evaluate news popularity and potential PR opportunities.\"}]},\"promptType\":\"define\"},\"typeVersion\":1.5},{\"id\":\"c4905ed1-324a-4b08-a1f4-f5465229b56c\",\"name\":\"Stories Report\",\"type\":\"@n8n/n8n-nodes-langchain.chainLlm\",\"position\":[2060,640],\"parameters\":{\"text\":\"=INPUT CONTEXT:\\nNews Analysis: \\n<News_Analysis>\\n{{ $json.text.replace(/[\\\\s\\\\S]*<news_analysis>/, '').replace(/<\\\\/news_analysis>[\\\\s\\\\S]*/, '') }}\\n</News_Analysis>\\nReddit Metrics:\\nâ€¢ Post Title (Understanding how the story was shared): {{ $('Set for Loop').first().json.Title }}\\nâ€¢ Upvotes (Indicator of initial interest): {{ $('Set for Loop').first().json.Upvotes }}\\nâ€¢ Total Comments (Engagement level): {{ $('Set for Loop').first().json.Comments }}\\nReddit Sentiment Analysis:\\n<Sentiment_Analysis>\\n{{ $('Comments Analysis').first().json.text.replace(/[\\\\s\\\\S]*<comments_analysis>/, '').replace(/<\\\\/comments_analysis>[\\\\s\\\\S]*/, '') }}\\n</Sentiment_Analysis>\\n\\nOUTPUT FORMAT (Place inside XML tags <new_stories_report>):\\n\\nTREND ANALYSIS SUMMARY:\\nTopic: [News topic/category]\\nCurrent Coverage Status: [Overview of existing coverage]\\nAudience Reception: [From Reddit/sentiment analysis]\\nMarket Timing: [Why now is relevant]\\n\\nSTORY OPPORTUNITIES:\\n\\n1. FIRST-MOVER STORIES:\\n[For each story idea (2-3)]\\n\\nStory #1:\\nâ€¢ Headline: [Compelling title]\\nâ€¢ Hook: [One-line grabber]\\nâ€¢ Story Summary: [2-3 sentences]\\nâ€¢ Why It Works:\\n  - Audience Evidence: [From Reddit data]\\n  - Market Gap: [From news analysis]\\n  - Timing Advantage: [Why now]\\nâ€¢ Development Needs:\\n  - Research Required: [List]\\n  - Expert Input: [Specific needs]\\n  - Supporting Data: [What's needed]\\nâ€¢ Media Strategy:\\n  - Primary Targets: [Publications]\\n  - Secondary Targets: [Publications]\\n  - Exclusive Potential: [Yes/No + Rationale]\\nâ€¢ Success Metrics:\\n  - Coverage Goals: [Specific targets]\\n  - Engagement Expectations: [Based on Reddit data]\\n\\n2. TREND-AMPLIFIER STORIES:\\n[Same format as above for 2-3 stories]\\n\\nPRIORITY RANKING:\\n1. [Story Title] - Score: [X/10]\\n   â€¢ Impact Potential: [Score + Rationale]\\n   â€¢ Resource Requirements: [High/Medium/Low]\\n   â€¢ Timeline: [Immediate/Short-term/Long-term]\\n   \\n2. [Continue for all stories]\\n\\nEXECUTION ROADMAP:\\nâ€¢ Immediate Actions (24-48 hours)\\nâ€¢ Week 1 Priorities\\nâ€¢ Risk Management\\nâ€¢ Contingency Plans\\n\\nSTRATEGIC RECOMMENDATIONS:\\nâ€¢ Core Strategy\\nâ€¢ Alternative Angles\\nâ€¢ Resource Requirements\\nâ€¢ Timeline Considerations\\n\\nANALYTICAL FRAMEWORK:\\n\\n1. TREND VALIDATION:\\n   A. Story Performance Indicators:\\n      â€¢ Reddit engagement metrics\\n      â€¢ Public sentiment patterns\\n      â€¢ Discussion quality\\n      â€¢ Viral elements identified\\n\\n   B. Current Narrative Landscape:\\n      â€¢ Dominant themes from news analysis\\n      â€¢ Public perception gaps\\n      â€¢ Controversial elements\\n      â€¢ Underserved perspectives\\n\\n2. OPPORTUNITY MAPPING:\\n   A. Content Gap Analysis:\\n      â€¢ Unexplored angles from news analysis\\n      â€¢ Audience questions from comments\\n      â€¢ Missing expert perspectives\\n      â€¢ Data/research opportunities\\n\\n   B. Timing Assessment:\\n      â€¢ News cycle position\\n      â€¢ Trend trajectory\\n      â€¢ Optimal launch window\\n      â€¢ Competition consideration\\n\\nPR STORY OPPORTUNITIES:\\nGenerate 4-6 high-potential story ideas, categorized as:\\n\\nA. \\\\\\\"FIRST-MOVER\\\\\\\" OPPORTUNITIES (2-3 ideas):\\nFor each idea:\\n\\n1. Story Concept:\\n   â€¢ Headline\\n   â€¢ Sub-headline\\n   â€¢ Key message\\n   â€¢ Unique selling point\\n\\n2. Why It Works:\\n   â€¢ Gap in current coverage\\n   â€¢ Evidence from Reddit discussions\\n   â€¢ Sentiment analysis support\\n   â€¢ Market timing rationale\\n\\n3. Development Requirements:\\n   â€¢ Required data/research\\n   â€¢ Expert perspectives needed\\n   â€¢ Supporting elements\\n   â€¢ Potential challenges\\n\\n4. Media Strategy:\\n   â€¢ Target publications\\n   â€¢ Journalist appeal factors\\n   â€¢ Exclusive potential\\n   â€¢ Supporting assets needed\\n\\nB. \\\\\\\"TREND-AMPLIFIER\\\\\\\" OPPORTUNITIES (2-3 ideas):\\n[Same structure as above, but focused on enhancing existing narratives]\\n\\nSTORY PRIORITIZATION MATRIX:\\nFor each story idea:\\n1. Impact Potential (1-10):\\n   â€¢ Audience interest indicators\\n   â€¢ Media appeal factors\\n   â€¢ Viral potential\\n   â€¢ Business value\\n\\n2. Resource Requirements:\\n   â€¢ Time to develop\\n   â€¢ Research needs\\n   â€¢ Expert input\\n   â€¢ Asset creation\\n\\n3. Risk Assessment:\\n   â€¢ Competition factors\\n   â€¢ Timing risks\\n   â€¢ Narrative challenges\\n   â€¢ Mitigation strategies\\n\\nEXECUTION ROADMAP:\\n1. Immediate Actions (Next 24-48 hours):\\n   â€¢ Priority research needs\\n   â€¢ Expert outreach\\n   â€¢ Data gathering\\n   â€¢ Asset development\\n\\n2. Development Timeline:\\n   â€¢ Story development sequence\\n   â€¢ Key milestones\\n   â€¢ Decision points\\n   â€¢ Launch windows\\n\\n3. Success Metrics:\\n   â€¢ Coverage targets\\n   â€¢ Engagement goals\\n   â€¢ Share of voice objectives\\n   â€¢ Impact measurements\\n\\nSTRATEGIC RECOMMENDATIONS:\\n1. Primary Strategy:\\n   â€¢ Core approach\\n   â€¢ Key differentiators\\n   â€¢ Critical success factors\\n   â€¢ Risk mitigation\\n\\n2. Alternative Approaches:\\n   â€¢ Backup angles\\n   â€¢ Pivot opportunities\\n   â€¢ Alternative narratives\\n   â€¢ Contingency plans\\n\\nFocus on creating stories that:\\nâ€¢ Address identified audience interests (from Reddit data)\\nâ€¢ Fill gaps in current coverage\\nâ€¢ Leverage positive sentiment patterns\\nâ€¢ Solve for identified pain points\\nâ€¢ Offer unique, data-backed perspectives\\nâ€¢ Present clear competitive advantages\\n\\nBased on the provided news analysis, Reddit metrics, and sentiment analysis, please generate a comprehensive PR strategy report following the format above.\",\"messages\":{\"messageValues\":[{\"message\":\"=You are an elite PR Strategy Consultant specialized in crafting breakthrough story angles that capture media attention. Your mission is to analyze trending story patterns and develop high-impact PR opportunities based on comprehensive data analysis.\\n\\nCONTEXT IMPORTANCE:\\nThis analysis combines three critical data sources:\\n1. Reddit Engagement Data:\\n   â€¢ Indicates public interest levels\\n   â€¢ Shows organic discussion patterns\\n   â€¢ Reveals audience sentiment\\n   â€¢ Highlights knowledge gaps\\n   â€¢ Demonstrates viral potential\\n\\n2. News Content Analysis:\\n   â€¢ Provides core story elements\\n   â€¢ Shows current media angles\\n   â€¢ Identifies market implications\\n   â€¢ Reveals coverage gaps\\n   â€¢ Maps expert perspectives\\n\\n3. Sentiment Analysis:\\n   â€¢ Reveals public perception\\n   â€¢ Identifies controversy points\\n   â€¢ Shows emotional triggers\\n   â€¢ Highlights audience concerns\\n   â€¢ Indicates story longevity\\n\\nThis combined data helps us:\\nâ€¢ Validate story potential\\nâ€¢ Identify unexplored angles\\nâ€¢ Understand audience needs\\nâ€¢ Predict media interest\\nâ€¢ Craft compelling narratives\"}]},\"promptType\":\"define\"},\"typeVersion\":1.5},{\"id\":\"1379c60b-387c-4eba-a7c2-2bcb1cda48fd\",\"name\":\"Set Data\",\"type\":\"n8n-nodes-base.set\",\"position\":[240,420],\"parameters\":{\"options\":{},\"assignments\":{\"assignments\":[{\"id\":\"b4da0605-b5e1-47e1-8e7e-00158ecaba33\",\"name\":\"Topics\",\"type\":\"string\",\"value\":\"=Donald Trump\\nPolitics\"},{\"id\":\"d7602355-7082-4e98-a0b5-a400fade6dbc\",\"name\":\"Jina API Key\",\"type\":\"string\",\"value\":\"YOUR_API_KEY\"}]}},\"typeVersion\":3.4}],\"active\":false,\"pinData\":{},\"settings\":{\"executionOrder\":\"v1\"},\"versionId\":\"dad1fb7a-599f-4b98-9461-8b27baa774d9\",\"connections\":{\"Set Data\":{\"main\":[[{\"node\":\"Split Topics into Items\",\"type\":\"main\",\"index\":0}]]},\"Aggregate\":{\"main\":[[{\"node\":\"Merge Binary Files\",\"type\":\"main\",\"index\":0}]]},\"Keep Last\":{\"main\":[[{\"node\":\"News Analysis\",\"type\":\"main\",\"index\":0}]]},\"Get Comments\":{\"main\":[[{\"node\":\"Extract Top Comments\",\"type\":\"main\",\"index\":0}]]},\"Search Posts\":{\"main\":[[{\"node\":\"Upvotes Requirement Filtering\",\"type\":\"main\",\"index\":0}]]},\"Set for Loop\":{\"main\":[[{\"node\":\"Get Comments\",\"type\":\"main\",\"index\":0}]]},\"Google Drive6\":{\"main\":[[{\"node\":\"Google Drive7\",\"type\":\"main\",\"index\":0}]]},\"Google Drive7\":{\"main\":[[{\"node\":\"Send files to Mattermost3\",\"type\":\"main\",\"index\":0}]]},\"News Analysis\":{\"main\":[[{\"node\":\"Stories Report\",\"type\":\"main\",\"index\":0}]]},\"Compress files\":{\"main\":[[{\"node\":\"Google Drive6\",\"type\":\"main\",\"index\":0}]]},\"Stories Report\":{\"main\":[[{\"node\":\"Set Final Report\",\"type\":\"main\",\"index\":0}]]},\"Convert to File\":{\"main\":[[{\"node\":\"Loop Over Items\",\"type\":\"main\",\"index\":0}]]},\"Format Comments\":{\"main\":[[{\"node\":\"Comments Analysis\",\"type\":\"main\",\"index\":0}]]},\"Loop Over Items\":{\"main\":[[{\"node\":\"Aggregate\",\"type\":\"main\",\"index\":0}],[{\"node\":\"Set for Loop\",\"type\":\"main\",\"index\":0}]]},\"Get News Content\":{\"main\":[[{\"node\":\"Keep Last\",\"type\":\"main\",\"index\":0}]]},\"Schedule Trigger\":{\"main\":[[{\"node\":\"Set Data\",\"type\":\"main\",\"index\":0}]]},\"Set Final Report\":{\"main\":[[{\"node\":\"Convert to File\",\"type\":\"main\",\"index\":0}]]},\"Set Reddit Posts\":{\"main\":[[{\"node\":\"Remove Duplicates\",\"type\":\"main\",\"index\":0}]]},\"Comments Analysis\":{\"main\":[[{\"node\":\"Get News Content\",\"type\":\"main\",\"index\":0}]]},\"Remove Duplicates\":{\"main\":[[{\"node\":\"Loop Over Items\",\"type\":\"main\",\"index\":0}]]},\"Merge Binary Files\":{\"main\":[[{\"node\":\"Compress files\",\"type\":\"main\",\"index\":0}]]},\"Anthropic Chat Model\":{\"ai_languageModel\":[[{\"node\":\"Comments Analysis\",\"type\":\"ai_languageModel\",\"index\":0}]]},\"Extract Top Comments\":{\"main\":[[{\"node\":\"Format Comments\",\"type\":\"main\",\"index\":0}]]},\"Anthropic Chat Model1\":{\"ai_languageModel\":[[{\"node\":\"News Analysis\",\"type\":\"ai_languageModel\",\"index\":0}]]},\"Anthropic Chat Model2\":{\"ai_languageModel\":[[{\"node\":\"Stories Report\",\"type\":\"ai_languageModel\",\"index\":0}]]},\"Split Topics into Items\":{\"main\":[[{\"node\":\"Search Posts\",\"type\":\"main\",\"index\":0}]]},\"Upvotes Requirement Filtering\":{\"main\":[[{\"node\":\"Set Reddit Posts\",\"type\":\"main\",\"index\":0}]]}}}",
  "readme": "## Introduction\n\nThe \"Automatic Weekly Digital PR Stories Suggestions\" workflow is a sophisticated automated system designed to identify trending news stories on Reddit, analyze public sentiment through comment analysis, extract key information from source articles, and generate strategic angles for potential digital PR campaigns.\n\nThis workflow leverages the power of social media trends, natural language processing, and AI-driven analysis to deliver curated, sentiment-analyzed news opportunities for PR professionals.\n\nOperating on a weekly schedule, the workflow searches Reddit for posts related to specified topics, filters them based on engagement metrics, and performs a deep analysis of both the content and public reaction.\n\nIt then generates comprehensive reports that include story opportunities, audience insights, and strategic recommendations. These reports are automatically compiled, stored in Google Drive, and shared with team members via Mattermost for immediate collaboration.\n\nThis workflow solves the time-consuming process of manually monitoring social media for trending stories, analyzing public sentiment, and identifying PR opportunities.\n\nBy automating these tasks, PR professionals can focus on strategy development and execution rather than spending hours on research and analysis.\n\n## Who is this for?\n\nThis workflow is designed for digital PR professionals, content marketers, communications teams, and media relations specialists who need to stay on top of trending stories and public sentiment to develop timely and effective PR campaigns. It's particularly valuable for:\n\n  * PR agencies managing multiple clients across different industries\n  * In-house PR teams needing to identify media opportunities quickly\n  * Content marketers looking for trending topics to create timely content\n  * Communications professionals monitoring public perception of industry news\n\n\n\nUsers should have basic familiarity with n8n workflows and the PR strategy development process. While technical knowledge of the integrated APIs is not required to use the workflow, some understanding of Reddit, sentiment analysis, and PR campaign development would be beneficial for interpreting and acting on the generated reports.\n\n## What problem is this workflow solving?\n\nDigital PR professionals face several challenges that this workflow addresses:\n\n  1. **Information Overload** : Manually monitoring social media platforms for trending stories is time-consuming and often results in missed opportunities.\n\n  2. **Sentiment Analysis Complexity** : Understanding public perception of news stories requires reading through hundreds of comments and identifying patterns, which is labor-intensive and subjective.\n\n  3. **Content Extraction** : Visiting multiple news sources to read and analyze articles takes significant time.\n\n  4. **Strategic Angle Development** : Identifying unique PR angles that leverage trending stories and public sentiment requires synthesizing large amounts of information.\n\n  5. **Team Collaboration** : Sharing findings and insights with team members in a structured format can be cumbersome.\n\n\n\n\nBy automating these processes, the workflow enables PR professionals to quickly identify trending stories with PR potential, understand public sentiment, and develop strategic angles based on comprehensive analysis, all while maintaining a structured approach to team collaboration.\n\n## What this workflow does\n\n### Overview\n\nThe workflow automatically identifies trending posts on Reddit related to specified topics, analyzes both the content of linked articles and public sentiment from comments, and generates comprehensive PR strategy reports. These reports include story opportunities, audience insights, and strategic recommendations based on the analysis. The final reports are compiled, stored in Google Drive, and shared with team members via Mattermost.\n\n### Process\n\n  1. **Topic Selection and Reddit Search** :\n\n     * The workflow starts with a list of topics specified in the \"Set Data\" node\n     * It searches Reddit for posts related to these topics\n     * Posts are filtered based on upvotes and other criteria to focus on trending content\n  2. **Comment Analysis** :\n\n     * For each post, the workflow retrieves comments\n     * It extracts the top 30 comments based on score\n     * Using Claude AI, it analyzes the comments to understand: \n       * Overall sentiment\n       * Dominant narratives\n       * Audience insights\n       * PR implications\n  3. **Content Analysis** :\n\n     * The workflow extracts the content of the linked article using Jina AI\n     * It analyzes the content to identify: \n       * Core story elements\n       * Technical aspects\n       * Narrative opportunities\n       * Viral elements\n  4. **PR Strategy Development** :\n\n     * Based on the combined analysis of comments and content, the workflow generates: \n       * First-mover story opportunities\n       * Trend-amplifier story ideas\n       * Priority rankings\n       * Execution roadmap\n       * Strategic recommendations\n  5. **Report Generation and Distribution** :\n\n     * The workflow compiles comprehensive reports for each post\n     * Reports are converted to text files\n     * All files are compressed into a ZIP archive\n     * The archive is uploaded to Google Drive\n     * A link to the archive is shared with team members via Mattermost\n\n\n\n## Setup\n\nTo set up this workflow, follow these steps:\n\n  1. **Import the Workflow** :\n\n     * Download the workflow JSON file\n     * Import it into your n8n instance\n  2. **Configure API Credentials** :\n\n     * Reddit: Add a new credential \"Reddit OAuth2 API\" by following the guide at <https://docs.n8n.io/integrations/builtin/credentials/reddit/>\n     * Anthropic: Add a new credential \"Anthropic Account\" by following the guide at <https://docs.n8n.io/integrations/builtin/credentials/anthropic/>\n     * Google Drive: Add a new credential \"Google Drive OAuth2 API\" by following the guide at <https://docs.n8n.io/integrations/builtin/credentials/google/oauth-single-service/>\n  3. **Configure the \"Set Data\" Node** :\n\n     * Set your interested topics (one per line)\n     * Add your Jina API key (obtain from <https://jina.ai/api-dashboard/key-manager>)\n  4. **Configure the Mattermost Node** :\n\n     * Update your Mattermost instance URL\n     * Set your Webhook ID and Channel\n     * Follow the guide at <https://developers.mattermost.com/integrate/webhooks/incoming/> for webhook setup\n  5. **Adjust the Schedule (Optional)** :\n\n     * The workflow is set to run every Monday at 6am\n     * Modify the \"Schedule Trigger\" node if you need a different schedule\n  6. **Test the Workflow** :\n\n     * Run the workflow manually to ensure all connections are working properly\n     * Check the output to verify the reports are being generated correctly\n\n\n\n## How to customize this workflow to your needs\n\nThis workflow can be customized in several ways to better suit your specific requirements:\n\n  1. **Topic Selection** :\n\n     * Modify the topics in the \"Set Data\" node to focus on industries or subjects relevant to your PR strategy\n     * Add multiple topics to cover different client interests or market segments\n  2. **Filtering Criteria** :\n\n     * Adjust the \"Upvotes Requirement Filtering\" node to change the minimum upvotes threshold\n     * Modify the filtering conditions to include or exclude certain types of posts\n  3. **Analysis Parameters** :\n\n     * Customize the prompts in the \"Comments Analysis,\" \"News Analysis,\" and \"Stories Report\" nodes to focus on specific aspects of the content or comments\n     * Adjust the temperature settings in the Anthropic Chat Model nodes to control the creativity of the AI responses\n  4. **Report Format** :\n\n     * Modify the \"Set Final Report\" node to change the structure or content of the final reports\n     * Add or remove sections based on your specific reporting needs\n  5. **Distribution Method** :\n\n     * Replace or supplement the Mattermost notification with email notifications, Slack messages, or other communication channels\n     * Add additional storage options beyond Google Drive\n  6. **Schedule Frequency** :\n\n     * Change the \"Schedule Trigger\" node to run the workflow more or less frequently\n     * Set up multiple triggers for different topics or clients\n  7. **Integration with Other Systems** :\n\n     * Add nodes to integrate with your CRM, content management system, or project management tools\n     * Create connections to automatically populate content calendars or task management systems\n\n\n",
  "readme_html": "<!--[--><div data-v-50766329=\"\"><h2>Introduction</h2>\n<p>The \"Automatic Weekly Digital PR Stories Suggestions\" workflow is a sophisticated automated system designed to identify trending news stories on Reddit, analyze public sentiment through comment analysis, extract key information from source articles, and generate strategic angles for potential digital PR campaigns.</p>\n<p>This workflow leverages the power of social media trends, natural language processing, and AI-driven analysis to deliver curated, sentiment-analyzed news opportunities for PR professionals.</p>\n<p>Operating on a weekly schedule, the workflow searches Reddit for posts related to specified topics, filters them based on engagement metrics, and performs a deep analysis of both the content and public reaction.</p>\n<p>It then generates comprehensive reports that include story opportunities, audience insights, and strategic recommendations. These reports are automatically compiled, stored in Google Drive, and shared with team members via Mattermost for immediate collaboration.</p>\n<p>This workflow solves the time-consuming process of manually monitoring social media for trending stories, analyzing public sentiment, and identifying PR opportunities.</p>\n<p>By automating these tasks, PR professionals can focus on strategy development and execution rather than spending hours on research and analysis.</p>\n<h2>Who is this for?</h2>\n<p>This workflow is designed for digital PR professionals, content marketers, communications teams, and media relations specialists who need to stay on top of trending stories and public sentiment to develop timely and effective PR campaigns. It's particularly valuable for:</p>\n<ul>\n<li>PR agencies managing multiple clients across different industries</li>\n<li>In-house PR teams needing to identify media opportunities quickly</li>\n<li>Content marketers looking for trending topics to create timely content</li>\n<li>Communications professionals monitoring public perception of industry news</li>\n</ul>\n<p>Users should have basic familiarity with n8n workflows and the PR strategy development process. While technical knowledge of the integrated APIs is not required to use the workflow, some understanding of Reddit, sentiment analysis, and PR campaign development would be beneficial for interpreting and acting on the generated reports.</p>\n<h2>What problem is this workflow solving?</h2>\n<p>Digital PR professionals face several challenges that this workflow addresses:</p>\n<ol>\n<li>\n<p><strong>Information Overload</strong>: Manually monitoring social media platforms for trending stories is time-consuming and often results in missed opportunities.</p>\n</li>\n<li>\n<p><strong>Sentiment Analysis Complexity</strong>: Understanding public perception of news stories requires reading through hundreds of comments and identifying patterns, which is labor-intensive and subjective.</p>\n</li>\n<li>\n<p><strong>Content Extraction</strong>: Visiting multiple news sources to read and analyze articles takes significant time.</p>\n</li>\n<li>\n<p><strong>Strategic Angle Development</strong>: Identifying unique PR angles that leverage trending stories and public sentiment requires synthesizing large amounts of information.</p>\n</li>\n<li>\n<p><strong>Team Collaboration</strong>: Sharing findings and insights with team members in a structured format can be cumbersome.</p>\n</li>\n</ol>\n<p>By automating these processes, the workflow enables PR professionals to quickly identify trending stories with PR potential, understand public sentiment, and develop strategic angles based on comprehensive analysis, all while maintaining a structured approach to team collaboration.</p>\n<h2>What this workflow does</h2>\n<h3>Overview</h3>\n<p>The workflow automatically identifies trending posts on Reddit related to specified topics, analyzes both the content of linked articles and public sentiment from comments, and generates comprehensive PR strategy reports. These reports include story opportunities, audience insights, and strategic recommendations based on the analysis. The final reports are compiled, stored in Google Drive, and shared with team members via Mattermost.</p>\n<h3>Process</h3>\n<ol>\n<li>\n<p><strong>Topic Selection and Reddit Search</strong>:</p>\n<ul>\n<li>The workflow starts with a list of topics specified in the \"Set Data\" node</li>\n<li>It searches Reddit for posts related to these topics</li>\n<li>Posts are filtered based on upvotes and other criteria to focus on trending content</li>\n</ul>\n</li>\n<li>\n<p><strong>Comment Analysis</strong>:</p>\n<ul>\n<li>For each post, the workflow retrieves comments</li>\n<li>It extracts the top 30 comments based on score</li>\n<li>Using Claude AI, it analyzes the comments to understand:\n<ul>\n<li>Overall sentiment</li>\n<li>Dominant narratives</li>\n<li>Audience insights</li>\n<li>PR implications</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><strong>Content Analysis</strong>:</p>\n<ul>\n<li>The workflow extracts the content of the linked article using Jina AI</li>\n<li>It analyzes the content to identify:\n<ul>\n<li>Core story elements</li>\n<li>Technical aspects</li>\n<li>Narrative opportunities</li>\n<li>Viral elements</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><strong>PR Strategy Development</strong>:</p>\n<ul>\n<li>Based on the combined analysis of comments and content, the workflow generates:\n<ul>\n<li>First-mover story opportunities</li>\n<li>Trend-amplifier story ideas</li>\n<li>Priority rankings</li>\n<li>Execution roadmap</li>\n<li>Strategic recommendations</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><strong>Report Generation and Distribution</strong>:</p>\n<ul>\n<li>The workflow compiles comprehensive reports for each post</li>\n<li>Reports are converted to text files</li>\n<li>All files are compressed into a ZIP archive</li>\n<li>The archive is uploaded to Google Drive</li>\n<li>A link to the archive is shared with team members via Mattermost</li>\n</ul>\n</li>\n</ol>\n<h2>Setup</h2>\n<p>To set up this workflow, follow these steps:</p>\n<ol>\n<li>\n<p><strong>Import the Workflow</strong>:</p>\n<ul>\n<li>Download the workflow JSON file</li>\n<li>Import it into your n8n instance</li>\n</ul>\n</li>\n<li>\n<p><strong>Configure API Credentials</strong>:</p>\n<ul>\n<li>Reddit: Add a new credential \"Reddit OAuth2 API\" by following the guide at <a href=\"https://docs.n8n.io/integrations/builtin/credentials/reddit/\" rel=\"ugc nofollow\" target=\"_blank\">https://docs.n8n.io/integrations/builtin/credentials/reddit/</a></li>\n<li>Anthropic: Add a new credential \"Anthropic Account\" by following the guide at <a href=\"https://docs.n8n.io/integrations/builtin/credentials/anthropic/\" rel=\"ugc nofollow\" target=\"_blank\">https://docs.n8n.io/integrations/builtin/credentials/anthropic/</a></li>\n<li>Google Drive: Add a new credential \"Google Drive OAuth2 API\" by following the guide at <a href=\"https://docs.n8n.io/integrations/builtin/credentials/google/oauth-single-service/\" rel=\"ugc nofollow\" target=\"_blank\">https://docs.n8n.io/integrations/builtin/credentials/google/oauth-single-service/</a></li>\n</ul>\n</li>\n<li>\n<p><strong>Configure the \"Set Data\" Node</strong>:</p>\n<ul>\n<li>Set your interested topics (one per line)</li>\n<li>Add your Jina API key (obtain from <a href=\"https://jina.ai/api-dashboard/key-manager\" rel=\"ugc nofollow\" target=\"_blank\">https://jina.ai/api-dashboard/key-manager</a>)</li>\n</ul>\n</li>\n<li>\n<p><strong>Configure the Mattermost Node</strong>:</p>\n<ul>\n<li>Update your Mattermost instance URL</li>\n<li>Set your Webhook ID and Channel</li>\n<li>Follow the guide at <a href=\"https://developers.mattermost.com/integrate/webhooks/incoming/\" rel=\"ugc nofollow\" target=\"_blank\">https://developers.mattermost.com/integrate/webhooks/incoming/</a> for webhook setup</li>\n</ul>\n</li>\n<li>\n<p><strong>Adjust the Schedule (Optional)</strong>:</p>\n<ul>\n<li>The workflow is set to run every Monday at 6am</li>\n<li>Modify the \"Schedule Trigger\" node if you need a different schedule</li>\n</ul>\n</li>\n<li>\n<p><strong>Test the Workflow</strong>:</p>\n<ul>\n<li>Run the workflow manually to ensure all connections are working properly</li>\n<li>Check the output to verify the reports are being generated correctly</li>\n</ul>\n</li>\n</ol>\n<h2>How to customize this workflow to your needs</h2>\n<p>This workflow can be customized in several ways to better suit your specific requirements:</p>\n<ol>\n<li>\n<p><strong>Topic Selection</strong>:</p>\n<ul>\n<li>Modify the topics in the \"Set Data\" node to focus on industries or subjects relevant to your PR strategy</li>\n<li>Add multiple topics to cover different client interests or market segments</li>\n</ul>\n</li>\n<li>\n<p><strong>Filtering Criteria</strong>:</p>\n<ul>\n<li>Adjust the \"Upvotes Requirement Filtering\" node to change the minimum upvotes threshold</li>\n<li>Modify the filtering conditions to include or exclude certain types of posts</li>\n</ul>\n</li>\n<li>\n<p><strong>Analysis Parameters</strong>:</p>\n<ul>\n<li>Customize the prompts in the \"Comments Analysis,\" \"News Analysis,\" and \"Stories Report\" nodes to focus on specific aspects of the content or comments</li>\n<li>Adjust the temperature settings in the Anthropic Chat Model nodes to control the creativity of the AI responses</li>\n</ul>\n</li>\n<li>\n<p><strong>Report Format</strong>:</p>\n<ul>\n<li>Modify the \"Set Final Report\" node to change the structure or content of the final reports</li>\n<li>Add or remove sections based on your specific reporting needs</li>\n</ul>\n</li>\n<li>\n<p><strong>Distribution Method</strong>:</p>\n<ul>\n<li>Replace or supplement the Mattermost notification with email notifications, Slack messages, or other communication channels</li>\n<li>Add additional storage options beyond Google Drive</li>\n</ul>\n</li>\n<li>\n<p><strong>Schedule Frequency</strong>:</p>\n<ul>\n<li>Change the \"Schedule Trigger\" node to run the workflow more or less frequently</li>\n<li>Set up multiple triggers for different topics or clients</li>\n</ul>\n</li>\n<li>\n<p><strong>Integration with Other Systems</strong>:</p>\n<ul>\n<li>Add nodes to integrate with your CRM, content management system, or project management tools</li>\n<li>Create connections to automatically populate content calendars or task management systems</li>\n</ul>\n</li>\n</ol>\n</div><!--]-->",
  "readme_zh": "## ç®€ä»‹\n\n\"æ•°å­—å…¬å…³çƒ­ç‚¹å‘¨æŠ¥è‡ªåŠ¨ç”Ÿæˆç³»ç»Ÿ\"æ˜¯ä¸€å¥—ç²¾å¯†è‡ªåŠ¨åŒ–æµç¨‹ï¼Œé€šè¿‡è¿½è¸ªRedditå¹³å°çƒ­ç‚¹è¯é¢˜ã€åˆ†æžè¯„è®ºæƒ…æ„Ÿå€¾å‘ã€æå–æºæ–‡ç« æ ¸å¿ƒä¿¡æ¯ï¼Œæœ€ç»ˆç”Ÿæˆå…·å¤‡æˆ˜ç•¥ä»·å€¼çš„æ•°å­—å…¬å…³ä¼ æ’­æ–¹æ¡ˆã€‚\n\nè¯¥ç³»ç»Ÿæ·±åº¦èžåˆç¤¾äº¤åª’ä½“è¶‹åŠ¿è¿½è¸ªã€è‡ªç„¶è¯­è¨€å¤„ç†ä¸Žäººå·¥æ™ºèƒ½åˆ†æžæŠ€æœ¯ï¼Œä¸ºå…¬å…³ä»Žä¸šè€…æä¾›ç»è¿‡æƒ…æ„Ÿåˆ†æžç­›é€‰çš„æ–°é—»çƒ­ç‚¹æœºä¼šåº“ã€‚\n\nç³»ç»Ÿæ¯å‘¨è‡ªåŠ¨è¿è¡Œï¼Œæ ¹æ®é¢„è®¾ä¸»é¢˜æ£€ç´¢Reddité«˜çƒ­å¸–æ–‡ï¼Œé€šè¿‡äº’åŠ¨æŒ‡æ ‡ç­›é€‰å†…å®¹ï¼Œå¹¶å¯¹åŽŸå§‹å†…å®¹åŠå…¬ä¼—åé¦ˆè¿›è¡Œæ·±åº¦è§£æžã€‚æœ€ç»ˆç”ŸæˆåŒ…å«çƒ­ç‚¹æœºä¼šã€å—ä¼—æ´žå¯Ÿä¸Žæˆ˜ç•¥å»ºè®®çš„å®Œæ•´æŠ¥å‘Šï¼Œè‡ªåŠ¨å½’æ¡£è‡³Googleäº‘ç«¯ç¡¬ç›˜å¹¶é€šè¿‡Mattermostå›¢é˜Ÿåä½œå¹³å°å³æ—¶å…±äº«ã€‚\n\nè¯¥ç³»ç»Ÿæœ‰æ•ˆè§£å†³äº†äººå·¥ç›‘æµ‹ç¤¾äº¤åª’ä½“è¶‹åŠ¿ã€åˆ†æžå…¬ä¼—æƒ…ç»ªã€å‘æŽ˜å…¬å…³æœºä¼šçš„è€—æ—¶éš¾é¢˜ï¼Œè®©å…¬å…³ä»Žä¸šè€…èƒ½å°†ç²¾åŠ›é›†ä¸­äºŽç­–ç•¥åˆ¶å®šä¸Žæ‰§è¡ŒçŽ¯èŠ‚ã€‚\n\n## é€‚ç”¨äººç¾¤\n\næœ¬ç³»ç»Ÿä¸“ä¸ºéœ€è¦å®žæ—¶æŽŒæ¡èˆ†æƒ…åŠ¨æ€çš„æ•°å­—å…¬å…³ä»Žä¸šè€…ã€å†…å®¹è¥é”€äººå‘˜ã€ä¼ æ’­å›¢é˜ŸåŠåª’ä½“å…³ç³»ä¸“å®¶è®¾è®¡ï¼Œç‰¹åˆ«é€‚ç”¨äºŽï¼š\n\n* æœåŠ¡å¤šè¡Œä¸šå®¢æˆ·çš„å…¬å…³ä»£ç†æœºæž„\n* éœ€è¦å¿«é€Ÿæ•æ‰åª’ä½“æœºä¼šçš„ä¼ä¸šå…¬å…³å›¢é˜Ÿ\n* å¯»æ‰¾çƒ­ç‚¹è¯é¢˜çš„å†…å®¹è¥é”€äººå‘˜\n* ç›‘æµ‹è¡Œä¸šèˆ†æƒ…åŠ¨æ€çš„ä¼ æ’­ä¸“ä¸šäººå£«\n\nä½¿ç”¨è€…éœ€å…·å¤‡n8nå·¥ä½œæµåŸºç¡€æ“ä½œèƒ½åŠ›åŠå…¬å…³ç­–ç•¥åˆ¶å®šç»éªŒã€‚è™½ç„¶ä¸è¦æ±‚æŽŒæ¡APIæŠ€æœ¯ç»†èŠ‚ï¼Œä½†ç†Ÿæ‚‰Redditå¹³å°æœºåˆ¶ã€æƒ…æ„Ÿåˆ†æžåŽŸç†åŠå…¬å…³æ´»åŠ¨ç­–åˆ’å°†æœ‰åŠ©äºŽæ›´å¥½åœ°è§£è¯»å’Œåº”ç”¨ç³»ç»Ÿç”Ÿæˆçš„æŠ¥å‘Šã€‚\n\n## æ ¸å¿ƒè§£å†³æ–¹æ¡ˆ\n\næœ¬ç³»ç»Ÿé’ˆå¯¹æ€§è§£å†³æ•°å­—å…¬å…³é¢†åŸŸçš„äº”å¤§ç—›ç‚¹ï¼š\n\n1. **ä¿¡æ¯è¿‡è½½**ï¼šäººå·¥ç›‘æµ‹ç¤¾äº¤åª’ä½“æ•ˆçŽ‡ä½Žä¸‹ï¼Œæ˜“é”™å¤±çƒ­ç‚¹è‰¯æœº\n2. **æƒ…æ„Ÿåˆ†æžå¤æ‚**ï¼šäººå·¥ç ”åˆ¤æµ·é‡è¯„è®ºè€—æ—¶è´¹åŠ›ä¸”ä¸»è§‚æ€§å¼º\n3. **å†…å®¹æå–ä½Žæ•ˆ**ï¼šè·¨å¹³å°æŸ¥é˜…åŽŸå§‹æ–‡ç« æ¶ˆè€—å¤§é‡æ—¶é—´\n4. **ç­–ç•¥å¼€å‘å›°éš¾**ï¼šéœ€æ•´åˆå¤šç»´ä¿¡æ¯æ‰èƒ½å½¢æˆç‹¬ç‰¹ä¼ æ’­è§’åº¦\n5. **å›¢é˜Ÿåä½œç¹ç**ï¼šç»“æž„åŒ–å…±äº«æ´žå¯Ÿæˆæžœæµç¨‹å†—é•¿\n\né€šè¿‡è‡ªåŠ¨åŒ–å¤„ç†ï¼Œç³»ç»Ÿèƒ½å¿«é€Ÿè¯†åˆ«å…·å¤‡å…¬å…³æ½œåŠ›çš„çƒ­ç‚¹è¯é¢˜ï¼Œç²¾å‡†æŠŠæ¡èˆ†è®ºé£Žå‘ï¼ŒåŸºäºŽå…¨é¢åˆ†æžç”Ÿæˆæˆ˜ç•¥å»ºè®®ï¼ŒåŒæ—¶ä¿æŒå›¢é˜Ÿåä½œçš„æ ‡å‡†åŒ–æµç¨‹ã€‚\n\n## ç³»ç»ŸåŠŸèƒ½\n\n### æ€»ä½“æž¶æž„\n\nç³»ç»Ÿè‡ªåŠ¨æŠ“å–Redditå¹³å°æŒ‡å®šä¸»é¢˜çš„çƒ­é—¨å¸–æ–‡ï¼ŒåŒæ­¥åˆ†æžåŽŸæ–‡å†…å®¹ä¸Žè¯„è®ºæƒ…æ„Ÿå€¾å‘ï¼Œç”ŸæˆåŒ…å«ä¼ æ’­æœºä¼šã€å—ä¼—ç”»åƒåŠç­–ç•¥å»ºè®®çš„å®Œæ•´æŠ¥å‘Šã€‚æœ€ç»ˆæˆæžœè‡ªåŠ¨æ‰“åŒ…å­˜å‚¨è‡³Googleäº‘ç«¯ç¡¬ç›˜ï¼Œå¹¶é€šè¿‡MattermostæŽ¨é€å›¢é˜Ÿé€šçŸ¥ã€‚\n\n### è¿è¡Œæµç¨‹\n\n1. **ä¸»é¢˜è®¾ç½®ä¸Žå†…å®¹æŠ“å–**ï¼š\n   * é€šè¿‡\"æ•°æ®è®¾ç½®\"èŠ‚ç‚¹é…ç½®ç›‘æµ‹ä¸»é¢˜æ¸…å•\n   * è‡ªåŠ¨æ£€ç´¢Redditç›¸å…³è¯é¢˜å¸–æ–‡\n   * æ ¹æ®ç‚¹èµžæ•°ç­‰æŒ‡æ ‡ç­›é€‰é«˜çƒ­å†…å®¹\n\n2. **è¯„è®ºæƒ…æ„Ÿåˆ†æž**ï¼š\n   * æå–æ¯ç¯‡å¸–æ–‡çš„çƒ­é—¨è¯„è®ºï¼ˆå‰30æ¡ï¼‰\n   * è°ƒç”¨Claude AIè¿›è¡Œå¤šç»´åˆ†æžï¼š\n     * æ•´ä½“æƒ…æ„Ÿå€¾å‘\n     * ä¸»æµè§‚ç‚¹è¯†åˆ«\n     * å—ä¼—ç‰¹å¾æ´žå¯Ÿ\n     * å…¬å…³ä»·å€¼è¯„ä¼°\n\n3. **åŽŸæ–‡å†…å®¹è§£æž**ï¼š\n   * é€šè¿‡Jina AIæå–é“¾æŽ¥æ–‡ç« æ ¸å¿ƒå†…å®¹\n   * æ·±åº¦åˆ†æžï¼š\n     * å…³é”®æ–°é—»è¦ç´ \n     * ä¸“ä¸šæŠ€æœ¯ç»´åº¦\n     * å™äº‹æ‹“å±•ç©ºé—´\n     * ç—…æ¯’ä¼ æ’­æ½œåŠ›\n\n4. **ç­–ç•¥æ–¹æ¡ˆç”Ÿæˆ**ï¼š\n   * ç»¼åˆè¯„è®ºä¸Žå†…å®¹åˆ†æžäº§å‡ºï¼š\n     * å…ˆå‘åˆ¶äººåž‹ä¼ æ’­æœºä¼š\n     * è¶‹åŠ¿åŠ©æŽ¨åž‹å†…å®¹åˆ›æ„\n     * ä¼˜å…ˆçº§æŽ’åºå»ºè®®\n     * æ‰§è¡Œè·¯çº¿å›¾\n     * æˆ˜ç•¥è¡ŒåŠ¨æŒ‡å—\n\n5. **æŠ¥å‘Šè¾“å‡ºä¸Žåˆ†å‘**ï¼š\n   * è‡ªåŠ¨ç”Ÿæˆå•ç¯‡åˆ†æžæŠ¥å‘Š\n   * è½¬æ¢ä¸ºæ ‡å‡†åŒ–æ–‡æœ¬æ ¼å¼\n   * åŽ‹ç¼©æ‰“åŒ…ä¸ºZIPæ–‡ä»¶\n   * ä¸Šä¼ è‡³Googleäº‘ç«¯ç¡¬ç›˜\n   * é€šè¿‡MattermostæŽ¨é€å›¢é˜Ÿé€šçŸ¥\n\n## éƒ¨ç½²æŒ‡å—\n\n### ç³»ç»Ÿé…ç½®æ­¥éª¤\n\n1. **å·¥ä½œæµå¯¼å…¥**ï¼š\n   * ä¸‹è½½JSONæ ¼å¼å·¥ä½œæµæ–‡ä»¶\n   * å¯¼å…¥è‡³n8næ“ä½œå¹³å°\n\n2. **APIå‡­è¯é…ç½®**ï¼š\n   * Redditï¼šå‚ç…§<https://docs.n8n.io/integrations/builtin/credentials/reddit/>é…ç½®OAuth2å‡­è¯\n   * Anthropicï¼šå‚ç…§<https://docs.n8n.io/integrations/builtin/credentials/anthropic/>è®¾ç½®è´¦æˆ·ä¿¡æ¯\n   * Googleäº‘ç«¯ç¡¬ç›˜ï¼šå‚ç…§<https://docs.n8n.io/integrations/builtin/credentials/google/oauth-single-service/>å®ŒæˆOAuth2è®¤è¯\n\n3. **æ•°æ®èŠ‚ç‚¹è®¾ç½®**ï¼š\n   * åœ¨\"æ•°æ®è®¾ç½®\"èŠ‚ç‚¹é…ç½®ç›‘æµ‹ä¸»é¢˜ï¼ˆæ¯è¡Œä¸€ä¸ªä¸»é¢˜ï¼‰\n   * æ·»åŠ ä»Ž<https://jina.ai/api-dashboard/key-manager>èŽ·å–çš„APIå¯†é’¥\n\n4. **é€šè®¯å¹³å°é…ç½®**ï¼š\n   * æ›´æ–°Mattermostå®žä¾‹URL\n   * è®¾ç½®Webhook IDåŠç›®æ ‡é¢‘é“\n   * å‚ç…§<https://developers.mattermost.com/integrate/webhooks/incoming/>å®ŒæˆWebhooké…ç½®\n\n5. **æ‰§è¡Œè®¡åˆ’è°ƒæ•´ï¼ˆå¯é€‰ï¼‰**ï¼š\n   * é»˜è®¤æ¯å‘¨ä¸€6:00è‡ªåŠ¨æ‰§è¡Œ\n   * é€šè¿‡\"è®¡åˆ’è§¦å‘å™¨\"èŠ‚ç‚¹ä¿®æ”¹æ‰§è¡Œé¢‘çŽ‡\n\n6. **ç³»ç»Ÿæµ‹è¯•**ï¼š\n   * æ‰‹åŠ¨æ‰§è¡Œæµ‹è¯•æµç¨‹\n   * éªŒè¯å„çŽ¯èŠ‚è¿žæŽ¥çŠ¶æ€\n   * æ£€æŸ¥æŠ¥å‘Šç”Ÿæˆè´¨é‡\n\n## å®šåˆ¶åŒ–æ–¹æ¡ˆ\n\næœ¬ç³»ç»Ÿæ”¯æŒå¤šç»´åº¦çµæ´»è°ƒæ•´ï¼š\n\n1. **ä¸»é¢˜é…ç½®**ï¼š\n   * ä¿®æ”¹\"æ•°æ®è®¾ç½®\"èŠ‚ç‚¹åŒ¹é…ç›®æ ‡è¡Œä¸š\n   * æ”¯æŒå¤šä¸»é¢˜å¹¶è¡Œç›‘æµ‹\n\n2. **ç­›é€‰æ¡ä»¶**ï¼š\n   * è°ƒæ•´\"ç‚¹èµžæ•°è¿‡æ»¤\"èŠ‚ç‚¹é˜ˆå€¼\n   * è‡ªå®šä¹‰å¸–æ–‡ç±»åž‹ç­›é€‰è§„åˆ™\n\n3. **åˆ†æžç»´åº¦**ï¼š\n   * å®šåˆ¶å„AIåˆ†æžèŠ‚ç‚¹çš„æç¤ºè¯æ¨¡æ¿\n   * è°ƒèŠ‚Claudeæ¨¡åž‹å‚æ•°æŽ§åˆ¶ç”Ÿæˆåˆ›æ„åº¦\n\n4. **æŠ¥å‘Šæ ¼å¼**ï¼š\n   * é‡æž„æœ€ç»ˆæŠ¥å‘Šå†…å®¹ç»“æž„\n   * æŒ‰éœ€å¢žåˆ åˆ†æžæ¨¡å—\n\n5. **åˆ†å‘æ¸ é“**ï¼š\n   * æ‰©å±•é‚®ä»¶/Slackç­‰é€šçŸ¥æ–¹å¼\n   * å¢žåŠ å¤šå¹³å°å­˜å‚¨æ–¹æ¡ˆ\n\n6. **æ‰§è¡Œé¢‘çŽ‡**ï¼š\n   * æ”¯æŒå·®å¼‚åŒ–è°ƒåº¦ç­–ç•¥\n   * å¯è®¾ç½®å¤šå‘¨æœŸè§¦å‘æœºåˆ¶\n\n7. **ç³»ç»Ÿé›†æˆ**ï¼š\n   * å¯¹æŽ¥CRM/å†…å®¹ç®¡ç†ç³»ç»Ÿ\n   * è¿žæŽ¥é¡¹ç›®ç®¡ç†ç³»ç»Ÿè‡ªåŠ¨åˆ›å»ºä»»åŠ¡\n   * é›†æˆå†…å®¹æ—¥åŽ†å·¥å…·å®žçŽ°è‡ªåŠ¨æŽ’æœŸ",
  "title_zh": "åˆ©ç”¨Redditå’ŒAnthropicå®žçŽ°æ¯å‘¨æ•°å­—å…¬å…³æ•…äº‹è‡ªåŠ¨æŽ¨è",
  "publish_date_zh": "æœ€åŽæ›´æ–°äºŽ2ä¸ªæœˆå‰",
  "workflow_json_zh": "{\n  \"id\": \"h2uiciRa1D3ntSTT\",\n  \"meta\": {\n    \"instanceId\": \"ddfdf733df99a65c801a91865dba5b7c087c95cc22a459ff3647e6deddf2aee6\"\n  },\n  \"name\": \"My workflow\",\n  \"tags\": [],\n  \"nodes\": [\n    {\n      \"id\": \"4b885b7d-0976-4dd3-bc1c-091ab0dff437\",\n      \"name\": \"Split Topics into Items\",\n      \"type\": \"n8n-nodes-base.code\",\n      \"position\": [\n        420,\n        420\n      ],\n      \"parameters\": {\n        \"jsCode\": \"// Input data (from $json.Topics)\\nconst topicsString = $json.Topics;\\n\\n// Split the string by newlines and trim whitespace\\nconst topicsArray = topicsString.split('\\\\n').map(topic => topic.trim());\\n\\n// Create an array of items for each topic\\nconst items = topicsArray.map(topic => {\\n  return { json: { Topic: topic } };\\n});\\n\\n// Output the new array of items\\nreturn items;\\n\"\n      },\n      \"typeVersion\": 2\n    },\n    {\n      \"id\": \"935d0266-feda-48cb-b441-b4da19d8b163\",\n      \"name\": \"Search Posts\",\n      \"type\": \"n8n-nodes-base.reddit\",\n      \"position\": [\n        620,\n        420\n      ],\n      \"parameters\": {\n        \"keyword\": \"meta\",\n        \"location\": \"allReddit\",\n        \"operation\": \"search\",\n        \"returnAll\": true,\n        \"additionalFields\": {\n          \"sort\": \"hot\"\n        }\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"cea577c8-c025-4132-926a-74d6946d81b8\",\n      \"name\": \"Upvotes Requirement Filtering\",\n      \"type\": \"n8n-nodes-base.if\",\n      \"position\": [\n        800,\n        420\n      ],\n      \"parameters\": {\n        \"options\": {},\n        \"conditions\": {\n          \"options\": {\n            \"version\": 2,\n            \"leftValue\": \"\",\n            \"caseSensitive\": true,\n            \"typeValidation\": \"strict\"\n          },\n          \"combinator\": \"and\",\n          \"conditions\": [\n            {\n              \"id\": \"f767f7a8-a2e8-4566-be80-bd735249e069\",\n              \"operator\": {\n                \"type\": \"number\",\n                \"operation\": \"gt\"\n              },\n              \"leftValue\": \"={{ $json.ups }}\",\n              \"rightValue\": 100\n            },\n            {\n              \"id\": \"3af82bef-5a78-4e6e-91ef-a5bd0141c87f\",\n              \"operator\": {\n                \"name\": \"filter.operator.equals\",\n                \"type\": \"string\",\n                \"operation\": \"equals\"\n              },\n              \"leftValue\": \"={{ $json.post_hint }}\",\n              \"rightValue\": \"link\"\n            },\n            {\n              \"id\": \"980a84ed-d640-47a7-b49a-bf638e811f20\",\n              \"operator\": {\n                \"type\": \"string\",\n                \"operation\": \"notContains\"\n              },\n              \"leftValue\": \"={{ $json.url }}\",\n              \"rightValue\": \"bsky.app\"\n            }\n          ]\n        }\n      },\n      \"typeVersion\": 2.2\n    },\n    {\n      \"id\": \"eec2d833-9a63-4cf6-a6bd-56b300ede5e0\",\n      \"name\": \"Set Reddit Posts\",\n      \"type\": \"n8n-nodes-base.set\",\n      \"position\": [\n        1040,\n        420\n      ],\n      \"parameters\": {\n        \"options\": {},\n        \"assignments\": {\n          \"assignments\": [\n            {\n              \"id\": \"8d5ae4fa-2f54-48d7-8f61-766f4ecf9d96\",\n              \"name\": \"Title\",\n              \"type\": \"string\",\n              \"value\": \"={{ $json.title }}\"\n            },\n            {\n              \"id\": \"8eb33a06-d8e7-4eea-bcd3-f956e20e06e6\",\n              \"name\": \"Subreddit\",\n              \"type\": \"string\",\n              \"value\": \"={{ $json.subreddit }}\"\n            },\n            {\n              \"id\": \"5ff8c76e-a8d5-4f76-a7d0-faa69b7960e4\",\n              \"name\": \"Upvotes\",\n              \"type\": \"string\",\n              \"value\": \"={{ $json.ups }}\"\n            },\n            {\n              \"id\": \"05a2b453-0e29-4a81-8f10-5934ae721f64\",\n              \"name\": \"Comments\",\n              \"type\": \"string\",\n              \"value\": \"={{ $json.num_comments }}\"\n            },\n            {\n              \"id\": \"78f73e89-19a7-4dd5-9db0-ead55dfd5606\",\n              \"name\": \"Reddit URL\",\n              \"type\": \"string\",\n              \"value\": \"=https://www.reddit.com{{ $json.permalink }}\"\n            },\n            {\n              \"id\": \"6f92bce7-2dc5-4dfd-b216-efc12c5411bb\",\n              \"name\": \"URL\",\n              \"type\": \"string\",\n              \"value\": \"={{ $json.url }}\"\n            },\n            {\n              \"id\": \"0b20d78c-1d6b-4c84-99ef-978ee39fd35e\",\n              \"name\": \"Is_URL\",\n              \"type\": \"string\",\n              \"value\": \"={{ $json.post_hint }}\"\n            },\n            {\n              \"id\": \"489807f6-25ef-47d5-bd47-711ca75dedea\",\n              \"name\": \"Date\",\n              \"type\": \"string\",\n              \"value\": \"={{ new Date($json.created * 1000).toISOString().split('T')[0] }}\"\n            },\n            {\n              \"id\": \"0a9fb817-bfb7-4ea7-9182-1eddc404035f\",\n              \"name\": \"Post ID\",\n              \"type\": \"string\",\n              \"value\": \"={{ $json.id }}\"\n            }\n          ]\n        }\n      },\n      \"typeVersion\": 3.4\n    },\n    {\n      \"id\": \"9b45abb0-866a-47f4-b2b3-03e4cf41c988\",\n      \"name\": \"Remove Duplicates\",\n      \"type\": \"n8n-nodes-base.code\",\n      \"position\": [\n        1220,\n        420\n      ],\n      \"parameters\": {\n        \"jsCode\": \"// Get all input items\\nconst inputItems = $input.all();\\n\\n// Create a Map to store the most upvoted item for each URL\\nconst uniqueItemsMap = new Map();\\n\\nfor (const item of inputItems) {\\n  const url = item.json.URL;\\n  \\n  // Skip items where URL contains \\\"redd.it\\\"\\n  if (url && url.includes(\\\"redd.it\\\")) {\\n    continue;\\n  }\\n  \\n  const upvotes = parseInt(item.json.Upvotes, 10) || 0; // Ensure upvotes is a number\\n\\n  if (!uniqueItemsMap.has(url)) {\\n    // Add the first occurrence of the URL\\n    uniqueItemsMap.set(url, item);\\n  } else {\\n    // Compare upvotes and keep the item with the most upvotes\\n    const existingItem = uniqueItemsMap.get(url);\\n    const existingUpvotes = parseInt(existingItem.json.Upvotes, 10) || 0;\\n    if (upvotes > existingUpvotes) {\\n      uniqueItemsMap.set(url, item);\\n    }\\n  }\\n}\\n\\n// Extract all unique items\\nconst uniqueItems = Array.from(uniqueItemsMap.values());\\n\\n// Return each unique item as a separate output\\nreturn uniqueItems;\"\n      },\n      \"typeVersion\": 2\n    },\n    {\n      \"id\": \"39672fd4-3f8c-4cdb-acd5-bb862ae5eddd\",\n      \"name\": \"Loop Over Items\",\n      \"type\": \"n8n-nodes-base.splitInBatches\",\n      \"position\": [\n        40,\n        660\n      ],\n      \"parameters\": {\n        \"options\": {}\n      },\n      \"typeVersion\": 3\n    },\n    {\n      \"id\": \"ad70aec7-a610-42f8-b87c-0d3dbee00e7b\",\n      \"name\": \"Get Comments\",\n      \"type\": \"n8n-nodes-base.reddit\",\n      \"position\": [\n        480,\n        640\n      ],\n      \"parameters\": {\n        \"postId\": \"={{ $json[\\\"Post ID\\\"] }}\",\n        \"resource\": \"postComment\",\n        \"operation\": \"getAll\",\n        \"subreddit\": \"={{ $json.Subreddit }}\"\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"af7f0b35-4250-49e5-afa7-608155df0fd5\",\n      \"name\": \"Extract Top Comments\",\n      \"type\": \"n8n-nodes-base.code\",\n      \"position\": [\n        660,\n        640\n      ],\n      \"parameters\": {\n        \"jsCode\": \"/**\\n * n8n Code Node for filtering top 30 Reddit-style comments by score/ups\\n * and ensuring replies are included in the comment tree.\\n * Excludes deleted comments.\\n */\\n\\n// Get all input items\\nconst inputItems = $input.all();\\nconst commentsArray = inputItems.flatMap(item => item.json);\\n\\n/**\\n * Checks if a comment is deleted.\\n * @param {Object} commentObj - The comment to check.\\n * @returns {boolean} - True if the comment is deleted, false otherwise.\\n */\\nfunction isDeletedComment(commentObj) {\\n  return commentObj.author === \\\"[deleted]\\\" && commentObj.body === \\\"[removed]\\\";\\n}\\n\\n// Function to recursively flatten a comment and its replies\\nfunction flattenCommentTree(commentObj) {\\n  // Skip deleted comments\\n  if (isDeletedComment(commentObj)) {\\n    return null;\\n  }\\n\\n  const { body, ups, score, replies, author } = commentObj;\\n\\n  // Calculate score\\n  const finalScore = typeof ups === 'number' ? ups : (score || 0);\\n\\n  // Process comment\\n  const flatComment = {\\n    body: body || '',\\n    score: finalScore,\\n    author: author || 'Unknown',\\n    replies: [],\\n  };\\n\\n  // Process replies\\n  if (\\n    replies &&\\n    replies.data &&\\n    Array.isArray(replies.data.children)\\n  ) {\\n    flatComment.replies = replies.data.children\\n      .filter(child => child.kind === 't1' && child.data)\\n      .map(child => flattenCommentTree(child.data)) // Recursively flatten replies\\n      .filter(reply => reply !== null); // Filter out null replies (deleted comments)\\n  }\\n\\n  return flatComment;\\n}\\n\\n// Flatten all comments, preserving hierarchy\\nconst allComments = commentsArray\\n  .map(flattenCommentTree)\\n  .filter(comment => comment !== null); // Filter out null comments (deleted comments)\\n\\n// Flatten the hierarchy to a list for scoring and filtering\\nfunction flattenForScoring(tree) {\\n  const result = [];\\n  tree.forEach(comment => {\\n    result.push(comment); // Add current comment\\n    if (comment.replies && comment.replies.length > 0) {\\n      result.push(...flattenForScoring(comment.replies)); // Add replies recursively\\n    }\\n  });\\n  return result;\\n}\\n\\n// Flatten the hierarchy and sort by score\\nconst flatList = flattenForScoring(allComments);\\nflatList.sort((a, b) => b.score - a.score);\\n\\n// Select the top 30 comments\\nconst top30 = flatList.slice(0, 30);\\n\\n// Rebuild the hierarchy from the top 30\\nfunction filterHierarchy(tree, allowedBodies) {\\n  return tree\\n    .filter(comment => allowedBodies.has(comment.body))\\n    .map(comment => ({\\n      ...comment,\\n      replies: filterHierarchy(comment.replies || [], allowedBodies), // Recurse for replies\\n    }));\\n}\\n\\nconst allowedBodies = new Set(top30.map(comment => comment.body));\\nconst filteredHierarchy = filterHierarchy(allComments, allowedBodies);\\n\\n// Return in n8n format\\nreturn [\\n  {\\n    json: {\\n      comments: filteredHierarchy,\\n    },\\n  },\\n];\"\n      },\n      \"executeOnce\": true,\n      \"typeVersion\": 2\n    },\n    {\n      \"id\": \"e709d131-b8fa-42d5-bc66-479cb13574e6\",\n      \"name\": \"Format Comments\",\n      \"type\": \"n8n-nodes-base.code\",\n      \"position\": [\n        840,\n        640\n      ],\n      \"parameters\": {\n        \"jsCode\": \"/**\\n * Convert comments data into Markdown format with accurate hierarchy visualization.\\n * Excludes deleted comments.\\n */\\n\\n// Input data (replace this with your actual comments data)\\nconst data = $input.all()[0].json.comments;\\n\\n/**\\n * Checks if a comment is deleted.\\n * @param {Object} comment - The comment to check.\\n * @returns {boolean} - True if the comment is deleted, false otherwise.\\n */\\nfunction isDeletedComment(comment) {\\n  return comment.author === \\\"[deleted]\\\" && comment.body === \\\"[removed]\\\";\\n}\\n\\n/**\\n * Filters out deleted comments and their replies.\\n * @param {Array} comments - Array of comments.\\n * @returns {Array} - Filtered array of comments.\\n */\\nfunction filterDeletedComments(comments) {\\n  if (!comments || !comments.length) return [];\\n  \\n  return comments\\n    .filter(comment => !isDeletedComment(comment))\\n    .map(comment => {\\n      if (comment.replies && comment.replies.length > 0) {\\n        comment.replies = filterDeletedComments(comment.replies);\\n      }\\n      return comment;\\n    });\\n}\\n\\n/**\\n * Recursive function to format comments and replies into Markdown.\\n * @param {Array} comments - Array of comments.\\n * @param {number} level - Current level of the comment hierarchy for indentation.\\n * @returns {string} - Formatted Markdown string.\\n */\\nfunction formatCommentsToMarkdown(comments, level = 0) {\\n  let markdown = '';\\n  const indent = '  '.repeat(level); // Indentation for replies\\n\\n  for (const comment of comments) {\\n    // Format the main comment\\n    markdown += `${indent}- **Author**: ${comment.author}\\\\n`;\\n    markdown += `${indent}  **Score**: ${comment.score}\\\\n`;\\n    markdown += `${indent}  **Comment**:\\\\n\\\\n`;\\n    markdown += `${indent}    > ${comment.body.replace(/\\\\n/g, `\\\\n${indent}    > `)}\\\\n\\\\n`;\\n\\n    // Process replies if they exist\\n    if (comment.replies && comment.replies.length > 0) {\\n      markdown += `${indent}  **Replies:**\\\\n\\\\n`;\\n      markdown += formatCommentsToMarkdown(comment.replies, level + 1);\\n    }\\n  }\\n\\n  return markdown;\\n}\\n\\n// Filter out deleted comments first\\nconst filteredData = filterDeletedComments(data);\\n\\n// Generate the Markdown\\nconst markdownOutput = formatCommentsToMarkdown(filteredData);\\n\\n// Return the Markdown as an output for n8n\\nreturn [\\n  {\\n    json: {\\n      markdown: markdownOutput,\\n    },\\n  },\\n];\"\n      },\n      \"typeVersion\": 2\n    },\n    {\n      \"id\": \"284d511b-7d80-46ba-add0-6ff59aff176c\",\n      \"name\": \"Set for Loop\",\n      \"type\": \"n8n-nodes-base.set\",\n      \"position\": [\n        280,\n        640\n      ],\n      \"parameters\": {\n        \"options\": {},\n        \"assignments\": {\n          \"assignments\": [\n            {\n              \"id\": \"ac7c257d-544f-44e5-abc6-d0436f12517f\",\n              \"name\": \"Title\",\n              \"type\": \"string\",\n              \"value\": \"={{ $json.Title }}\"\n            },\n            {\n              \"id\": \"fb22c6a5-a809-4588-9f6e-49c3e11f5ed2\",\n              \"name\": \"Subreddit\",\n              \"type\": \"string\",\n              \"value\": \"={{ $json.Subreddit }}\"\n            },\n            {\n              \"id\": \"4bfcc849-539b-48cd-856f-1b7f3be113ed\",\n              \"name\": \"Upvotes\",\n              \"type\": \"string\",\n              \"value\": \"={{ $json.Upvotes }}\"\n            },\n            {\n              \"id\": \"9a3a3a2a-8f43-4419-9203-bc83f5b0c0bc\",\n              \"name\": \"Comments\",\n              \"type\": \"string\",\n              \"value\": \"={{ $json.Comments }}\"\n            },\n            {\n              \"id\": \"2d31f321-fbdc-43d3-8a92-a78f418f112f\",\n              \"name\": \"Reddit URL\",\n              \"type\": \"string\",\n              \"value\": \"={{ $json[\\\"Reddit URL\\\"] }}\"\n            },\n            {\n              \"id\": \"f224323a-79ef-4f66-ae10-d77c8fddbccd\",\n              \"name\": \"URL\",\n              \"type\": \"string\",\n              \"value\": \"={{ $json.URL }}\"\n            },\n            {\n              \"id\": \"dbbc5a98-b5e2-45bb-bc18-2c438522d683\",\n              \"name\": \"Date\",\n              \"type\": \"string\",\n              \"value\": \"={{ $json.Date }}\"\n            },\n            {\n              \"id\": \"837cae4e-858a-48ba-bab9-bb66a2e51837\",\n              \"name\": \"Post ID\",\n              \"type\": \"string\",\n              \"value\": \"={{ $json[\\\"Post ID\\\"] }}\"\n            }\n          ]\n        }\n      },\n      \"typeVersion\": 3.4\n    },\n    {\n      \"id\": \"b88fad49-edc4-4749-8984-a8e81f6a2899\",\n      \"name\": \"Get News Content\",\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"maxTries\": 5,\n      \"position\": [\n        1360,\n        640\n      ],\n      \"parameters\": {\n        \"url\": \"=https://r.jina.ai/{{ $('Set for Loop').first().json.URL }}\",\n        \"options\": {},\n        \"sendHeaders\": true,\n        \"headerParameters\": {\n          \"parameters\": [\n            {\n              \"name\": \"Accept\",\n              \"value\": \"text/event-stream\"\n            },\n            {\n              \"name\": \"Authorization\",\n              \"value\": \"=Bearer {{ $('Set Data').first().json['Jina API Key'] }}\"\n            },\n            {\n              \"name\": \"X-Retain-Images\",\n              \"value\": \"none\"\n            },\n            {\n              \"name\": \"X-Respond-With\",\n              \"value\": \"readerlm-v2\"\n            },\n            {\n              \"name\": \"X-Remove-Selector\",\n              \"value\": \"header, footer, sidebar\"\n            }\n          ]\n        }\n      },\n      \"retryOnFail\": true,\n      \"typeVersion\": 4.2,\n      \"waitBetweenTries\": 5000\n    },\n    {\n      \"id\": \"26a8906c-2966-4ebf-8465-18a48b359f7d\",\n      \"name\": \"Set Final Report\",\n      \"type\": \"n8n-nodes-base.set\",\n      \"position\": [\n        2400,\n        640\n      ],\n      \"parameters\": {\n        \"options\": {},\n        \"assignments\": {\n          \"assignments\": [\n            {\n              \"id\": \"0782b9a6-d659-4695-8696-6ff0e574f77a\",\n              \"name\": \"Final Report\",\n              \"type\": \"string\",\n              \"value\": \"=// Reddit Metrics:\\nPost Link: {{ $('Set for Loop').first().json['Reddit URL'] }}\\nUpvotes: {{ $('Set for Loop').first().json.Upvotes }}\\nComments: {{ $('Set for Loop').first().json.Comments }}\\n\\n# FINAL REPORT\\n{{ $json.text.replace(/[\\\\s\\\\S]*<new_stories_report>/, '').replace(/<\\\\/new_stories_report>[\\\\s\\\\S]*/, '') }}\\n\\n# RAW ANALYSIS DATA (FOR FURTHER ANALYSIS)\\n\\n## NEWS CONTENT ANALYSIS\\n{{ $('News Analysis').item.json.text.replace(/[\\\\s\\\\S]*<news_analysis>/, '').replace(/<\\\\/news_analysis>[\\\\s\\\\S]*/, '') }}\\n\\n## REDDIT COMMENTS ANALYSIS\\n{{ $('Comments Analysis').first().json.text.replace(/[\\\\s\\\\S]*<comments_analysis>/, '').replace(/<\\\\/comments_analysis>[\\\\s\\\\S]*/, '') }}\"\n            }\n          ]\n        }\n      },\n      \"typeVersion\": 3.4\n    },\n    {\n      \"id\": \"219ccb20-1b36-4c70-866a-0fded9c9b9fd\",\n      \"name\": \"Convert to File\",\n      \"type\": \"n8n-nodes-base.convertToFile\",\n      \"position\": [\n        2580,\n        640\n      ],\n      \"parameters\": {\n        \"options\": {\n          \"encoding\": \"utf8\",\n          \"fileName\": \"={{ $json[\\\"Final Report\\\"].match(/Headline:\\\\s*[\\\"â€œ](.*?)[\\\"â€]/i)?.[1] }}.txt\"\n        },\n        \"operation\": \"toText\",\n        \"sourceProperty\": \"Final Report\"\n      },\n      \"typeVersion\": 1.1\n    },\n    {\n      \"id\": \"427d5a2d-6927-4427-9902-e033736410ca\",\n      \"name\": \"Compress files\",\n      \"type\": \"n8n-nodes-base.compression\",\n      \"position\": [\n        600,\n        940\n      ],\n      \"parameters\": {\n        \"fileName\": \"=Trending_Stories_{{$now.format(\\\"yyyy_MM_dd\\\")}}_{{Math.floor(Math.random() * 10000).toString().padStart(4, '0')}}.zip\",\n        \"operation\": \"compress\",\n        \"outputFormat\": \"zip\",\n        \"binaryPropertyName\": \"={{ $json[\\\"binary_keys\\\"] }}\",\n        \"binaryPropertyOutput\": \"files_combined\"\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"7f6ef656-0f76-433f-95a8-782de21caa53\",\n      \"name\": \"Merge Binary Files\",\n      \"type\": \"n8n-nodes-base.code\",\n      \"position\": [\n        420,\n        940\n      ],\n      \"parameters\": {\n        \"jsCode\": \"// Get the first (and only) item since you're using Aggregate\\nconst item = items[0];\\nlet binary_keys = [];\\n\\n// Generate the list of binary keys from your aggregated item\\nfor (let key in item.binary) {\\n    binary_keys.push(key);\\n}\\n\\nreturn [{\\n    json: {\\n        binary_keys: binary_keys.join(',')\\n    },\\n    binary: item.binary  // Keep the original binary data\\n}];\"\n      },\n      \"executeOnce\": true,\n      \"typeVersion\": 2\n    },\n    {\n      \"id\": \"20411444-5ce8-452b-869c-97928200b205\",\n      \"name\": \"Google Drive6\",\n      \"type\": \"n8n-nodes-base.googleDrive\",\n      \"position\": [\n        780,\n        940\n      ],\n      \"parameters\": {\n        \"driveId\": {\n          \"__rl\": true,\n          \"mode\": \"list\",\n          \"value\": \"My Drive\",\n          \"cachedResultUrl\": \"https://drive.google.com/drive/my-drive\",\n          \"cachedResultName\": \"My Drive\"\n        },\n        \"options\": {},\n        \"folderId\": {\n          \"__rl\": true,\n          \"mode\": \"id\",\n          \"value\": \"1HCTq5YupRHcgRd7FIlSeUMMjqqOZ4Q9x\"\n        },\n        \"inputDataFieldName\": \"files_combined\"\n      },\n      \"typeVersion\": 3\n    },\n    {\n      \"id\": \"2eb8112a-8655-4f06-998f-a9ffef74d72a\",\n      \"name\": \"Google Drive7\",\n      \"type\": \"n8n-nodes-base.googleDrive\",\n      \"position\": [\n        960,\n        940\n      ],\n      \"parameters\": {\n        \"fileId\": {\n          \"__rl\": true,\n          \"mode\": \"id\",\n          \"value\": \"={{ $json.id }}\"\n        },\n        \"options\": {},\n        \"operation\": \"share\",\n        \"permissionsUi\": {\n          \"permissionsValues\": {\n            \"role\": \"reader\",\n            \"type\": \"anyone\"\n          }\n        }\n      },\n      \"typeVersion\": 3\n    },\n    {\n      \"id\": \"7f4e5e0c-49cc-4024-b62b-f7e099d4867d\",\n      \"name\": \"Send files to Mattermost3\",\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"position\": [\n        1140,\n        940\n      ],\n      \"parameters\": {\n        \"url\": \"https://team.YOUR_DOMAIN.com/hooks/REPLACE_THIS_WITH_YOUR_HOOK_ID\",\n        \"method\": \"POST\",\n        \"options\": {},\n        \"jsonBody\": \"={\\n    \\\"channel\\\": \\\"digital-pr\\\",\\n    \\\"username\\\": \\\"NotifyBot\\\",\\n    \\\"icon_url\\\": \\\"https://team.YOUR_DOMAIN.com/api/v4/users/YOUR_USER_ID/image?_=0\\\",\\n    \\\"text\\\": \\\"@channel New trending stories have been generated ðŸŽ‰\\\\n\\\\n\\\\n You can download it here: https://drive.google.com/file/d/{{ $('Google Drive6').item.json.id }}/view?usp=drive_link\\\"\\n}\",\n        \"sendBody\": true,\n        \"specifyBody\": \"json\"\n      },\n      \"typeVersion\": 4.2\n    },\n    {\n      \"id\": \"3c47f58d-8006-4565-b220-033d71239126\",\n      \"name\": \"Aggregate\",\n      \"type\": \"n8n-nodes-base.aggregate\",\n      \"position\": [\n        260,\n        940\n      ],\n      \"parameters\": {\n        \"options\": {\n          \"includeBinaries\": true\n        },\n        \"aggregate\": \"aggregateAllItemData\"\n      },\n      \"executeOnce\": false,\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"5611cdce-91ae-4037-9479-3b513eb07b77\",\n      \"name\": \"Schedule Trigger\",\n      \"type\": \"n8n-nodes-base.scheduleTrigger\",\n      \"position\": [\n        40,\n        420\n      ],\n      \"parameters\": {\n        \"rule\": {\n          \"interval\": [\n            {\n              \"field\": \"weeks\",\n              \"triggerAtDay\": [\n                1\n              ],\n              \"triggerAtHour\": 6\n            }\n          ]\n        }\n      },\n      \"typeVersion\": 1.2\n    },\n    {\n      \"id\": \"5cfeb9ea-45b6-4a0a-8702-34539738f280\",\n      \"name\": \"Anthropic Chat Model\",\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatAnthropic\",\n      \"position\": [\n        960,\n        800\n      ],\n      \"parameters\": {\n        \"model\": \"=claude-3-7-sonnet-20250219\",\n        \"options\": {\n          \"temperature\": 0.5,\n          \"maxTokensToSample\": 8096\n        }\n      },\n      \"typeVersion\": 1.2\n    },\n    {\n      \"id\": \"b11b2fa6-f92a-4791-b255-51ce1b07181b\",\n      \"name\": \"Anthropic Chat Model1\",\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatAnthropic\",\n      \"position\": [\n        1640,\n        800\n      ],\n      \"parameters\": {\n        \"model\": \"=claude-3-7-sonnet-20250219\",\n        \"options\": {\n          \"temperature\": 0.5,\n          \"maxTokensToSample\": 8096\n        }\n      },\n      \"typeVersion\": 1.2\n    },\n    {\n      \"id\": \"ffa45242-1dd4-46be-bacc-55bde63d0227\",\n      \"name\": \"Keep Last\",\n      \"type\": \"n8n-nodes-base.code\",\n      \"position\": [\n        1540,\n        640\n      ],\n      \"parameters\": {\n        \"jsCode\": \"// Extract input data from n8n\\nconst inputData = $json.data;\\n\\n// Ensure input is valid\\nif (!inputData || typeof inputData !== 'string') {\\n    return [{ error: \\\"Invalid input data\\\" }];\\n}\\n\\n// Split the data into lines\\nlet lines = inputData.split(\\\"\\\\n\\\");\\n\\n// Extract only JSON entries\\nlet jsonEntries = lines\\n    .map(line => line.trim()) // Remove spaces\\n    .filter(line => line.startsWith('data: {')) // Keep valid JSON objects\\n    .map(line => line.replace('data: ', '')); // Remove the prefix\\n\\n// Ensure there are entries\\nif (jsonEntries.length === 0) {\\n    return [{ error: \\\"No valid JSON entries found\\\" }];\\n}\\n\\n// Get only the LAST entry\\nlet lastEntry = jsonEntries[jsonEntries.length - 1];\\n\\ntry {\\n    // Parse the last entry as JSON\\n    let jsonObject = JSON.parse(lastEntry);\\n\\n    // Extract title and content\\n    return [{\\n        title: jsonObject.title || \\\"No Title\\\",\\n        content: jsonObject.content || \\\"No Content\\\"\\n    }];\\n} catch (error) {\\n    return [{ error: \\\"JSON parsing failed\\\", raw: lastEntry }];\\n}\"\n      },\n      \"typeVersion\": 2\n    },\n    {\n      \"id\": \"956672cc-8ceb-4a2c-93e8-bad2b9497043\",\n      \"name\": \"Anthropic Chat Model2\",\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatAnthropic\",\n      \"position\": [\n        1980,\n        800\n      ],\n      \"parameters\": {\n        \"model\": \"=claude-3-7-sonnet-20250219\",\n        \"options\": {\n          \"temperature\": 0.5,\n          \"maxTokensToSample\": 8096\n        }\n      },\n      \"typeVersion\": 1.2\n    },\n    {\n      \"id\": \"b55df80f-dbdf-4d8d-8b62-93533d1fb6ef\",\n      \"name\": \"Sticky Note\",\n      \"type\": \"n8n-nodes-base.stickyNote\",\n      \"position\": [\n        0,\n        0\n      ],\n      \"parameters\": {\n        \"width\": 1020,\n        \"height\": 340,\n        \"content\": \"## æ¯å‘¨è‡ªåŠ¨æ•°å­—å…¬å…³é€‰é¢˜æŽ¨èç³»ç»Ÿ\\næ¯å‘¨è‡ªåŠ¨è¿è¡Œçš„ç³»ç»Ÿï¼Œé€šè¿‡ä»¥ä¸‹æµç¨‹æŒ–æŽ˜æ½œåœ¨æ•°å­—å…¬å…³æœºä¼šï¼šæŠ“å–Redditçƒ­ç‚¹è¯é¢˜ã€åˆ†æžè¯„è®ºæƒ…æ„Ÿå€¾å‘ã€æå–æºæ–‡ç« å…³é”®ä¿¡æ¯ã€ç”Ÿæˆæˆ˜ç•¥ä¼ æ’­è§’åº¦ã€‚è¯¥å·¥ä½œæµåŸºäºŽå½“å‰ç¤¾äº¤åª’ä½“è¶‹åŠ¿ï¼Œæä¾›ç»è¿‡æƒ…æ„Ÿåˆ†æžä¸Žå†…å®¹ç­›é€‰çš„æ–°é—»é€‰é¢˜ã€‚æœ€ç»ˆç”Ÿæˆçš„å®Œæ•´æŠ¥å‘Šå°†è‡ªåŠ¨ä¸Šä¼ è‡³Google Driveå­˜å‚¨ï¼Œå¹¶é€šè¿‡Mattermostä¸“ç”¨é¢‘é“åŒæ­¥åˆ†äº«ç»™å›¢é˜Ÿæˆå‘˜ä»¥ä¾¿å³æ—¶åä½œã€‚\\n\\n### é…ç½®è¯´æ˜Žï¼š\\n1. æŒ‰ç…§[æŒ‡å—](https://docs.n8n.io/integrations/builtin/credentials/reddit/)æ·»åŠ \\\"Reddit OAuth2 API\\\"å‡­è¯ï¼Œå¹¶å°†Reddit OAuth2è´¦æˆ·åˆ†é…ç»™RedditèŠ‚ç‚¹\\n2. æŒ‰ç…§[æŒ‡å—](https://docs.n8n.io/integrations/builtin/credentials/anthropic/)æ·»åŠ \\\"Anthropicè´¦æˆ·\\\"å‡­è¯ï¼Œå¹¶å°†Anthropicè´¦æˆ·åˆ†é…ç»™\\\"AnthropicèŠå¤©æ¨¡åž‹\\\"èŠ‚ç‚¹\\n3. æŒ‰ç…§[æŒ‡å—](https://docs.n8n.io/integrations/builtin/credentials/google/oauth-single-service/)æ·»åŠ \\\"Google Drive OAuth2 API\\\"å‡­è¯ï¼Œå¹¶å°†Google Drive OAuth2è´¦æˆ·åˆ†é…ç»™\\\"Google Drive\\\"èŠ‚ç‚¹\\n4. åœ¨\\\"Set Data\\\"èŠ‚ç‚¹è®¾ç½®å…³æ³¨ä¸»é¢˜ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰åŠJina APIå¯†é’¥ï¼Œå¯é€šè¿‡[æ­¤å¤„](https://jina.ai/api-dashboard/key-manager)èŽ·å–Jina APIå¯†é’¥\\n5. åœ¨MattermostèŠ‚ç‚¹æ›´æ–°æ‚¨çš„Mattermostä¿¡æ¯ï¼ˆå®žä¾‹URLã€Webhook IDå’Œé¢‘é“ï¼‰ï¼Œé…ç½®å¯å‚è€ƒ[æŒ‡å—](https://developers.mattermost.com/integrate/webhooks/incoming/)\\n6. å¯è‡ªè¡Œè°ƒæ•´å®šæ—¶ä»»åŠ¡è®¾ç½®ï¼Œå½“å‰ä¸ºæ¯å‘¨ä¸€æ—©ä¸Š6ç‚¹è¿è¡Œ\"\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"07f1e0ff-892c-4aaf-ad77-e636138570a1\",\n      \"name\": \"Comments Analysis\",\n      \"type\": \"@n8n/n8n-nodes-langchain.chainLlm\",\n      \"position\": [\n        1020,\n        640\n      ],\n      \"parameters\": {\n        \"text\": \"=Please analyze the following Reddit post and its comments:\\n\\nCONTEXT:\\n<Reddit_Post_Info>\\nPost Title: {{ $('Set for Loop').first().json.Title.replace(/\\\\\\\"/g, '\\\\\\\\\\\\\\\"') }}\\nPost Date: {{ $('Set for Loop').first().json.Date }}\\nShared URL: {{ $('Set for Loop').first().json.URL }}\\nTotal Upvotes: {{ $('Set for Loop').first().json.Upvotes }}\\nTotal Comments: {{ $('Set for Loop').first().json.Comments }}\\n</Reddit_Post_Info>\\n\\nComment Thread Data:\\n<Reddit_Post_Top_Comments>\\n{{ $json.markdown.replace(/\\\\\\\"/g, '\\\\\\\\\\\\\\\"') }}\\n</Reddit_Post_Top_Comments>\\n\\nAnalyze this discussion through these dimensions:\\n\\n1. CONTENT CONTEXT:\\n   â€¢ Main topic/subject matter\\n   â€¢ Why this is trending (based on engagement metrics)\\n   â€¢ News cycle timing implications\\n   â€¢ Relationship to broader industry/market trends\\n\\n2. SENTIMENT ANALYSIS:\\n   â€¢ Overall sentiment score (Scale: -5 to +5)\\n   â€¢ Primary emotional undertones\\n   â€¢ Sentiment progression in discussion threads\\n   â€¢ Consensus vs. controversial viewpoints\\n   â€¢ Changes in sentiment based on comment depth\\n\\n3. ENGAGEMENT INSIGHTS:\\n   â€¢ Most upvoted perspectives (with exact scores)\\n   â€¢ Controversial discussion points\\n   â€¢ Comment chains with deepest engagement\\n   â€¢ Types of responses generating most interaction\\n\\n4. NARRATIVE MAPPING:\\n   â€¢ Dominant narratives\\n   â€¢ Counter-narratives\\n   â€¢ Emerging sub-themes\\n   â€¢ Unexplored angles\\n   â€¢ Missing perspectives\\n\\nOutput Format (Place inside XML tags <comments_analysis>):\\n\\nPOST OVERVIEW:\\nTitle: [Original title]\\nEngagement Metrics:\\nâ€¢ Upvotes: [count]\\nâ€¢ Comments: [count]\\nâ€¢ Virality Assessment: [analysis of why this gained traction]\\n\\nSENTIMENT ANALYSIS:\\nâ€¢ Overall Score: [numerical score with explanation]\\nâ€¢ Sentiment Distribution: [percentage breakdown]\\nâ€¢ Key Emotional Drivers:\\n  - Primary: [emotion]\\n  - Secondary: [emotion]\\n  - Notable Shifts: [pattern analysis]\\n\\nTOP NARRATIVES:\\n[List 3-5 dominant narratives]\\nFor each narrative:\\nâ€¢ Key Points\\nâ€¢ Supporting Comments [with scores]\\nâ€¢ Counter-Arguments\\nâ€¢ Engagement Level\\n\\nAUDIENCE INSIGHTS:\\nâ€¢ Knowledge Level: [assessment]\\nâ€¢ Pain Points: [list key concerns]\\nâ€¢ Misconceptions: [list with evidence]\\nâ€¢ Information Gaps: [identified missing information]\\n\\nPR IMPLICATIONS:\\n1. Story Opportunities:\\n   â€¢ [List potential angles]\\n   â€¢ [Supporting evidence from comments]\\n\\n2. Risk Factors:\\n   â€¢ [List potential PR risks]\\n   â€¢ [Supporting evidence from comments]\\n\\n3. Narrative Recommendations:\\n   â€¢ [Strategic guidance for messaging]\\n   â€¢ [Areas to address/avoid]\\n\\nNEXT STEPS CONSIDERATIONS:\\nâ€¢ Key data points for content analysis\\nâ€¢ Suggested focus areas for PR story development\\nâ€¢ Critical elements to address in messaging\\nâ€¢ Potential expert perspectives needed\\n\\nMETA INSIGHTS:\\nâ€¢ Pattern connections to similar discussions\\nâ€¢ Unique aspects of this conversation\\nâ€¢ Viral elements to note\\nâ€¢ Community-specific nuances\\n\\nFocus on extracting insights that will:\\n1. Inform the subsequent content analysis step\\n2. Guide PR story development\\n3. Identify unique angles and opportunities\\n4. Highlight potential risks and challenges\\n5. Suggest effective narrative approaches\\n\\nNote: Prioritize insights that will be valuable for the following workflow steps of content analysis and PR story development. Flag any particularly unique or compelling elements that could inform breakthrough story angles.\",\n        \"messages\": {\n          \"messageValues\": [\n            {\n              \"message\": \"=You are an expert Social Media Intelligence Analyst specialized in Reddit discourse analysis. Your task is to analyze Reddit posts and comments to extract meaningful patterns, sentiments, and insights for PR strategy development.\"\n            }\n          ]\n        },\n        \"promptType\": \"define\"\n      },\n      \"typeVersion\": 1.5\n    },\n    {\n      \"id\": \"4cdc4e49-6aae-4e6a-844e-c3c339638950\",\n      \"name\": \"News Analysis\",\n      \"type\": \"@n8n/n8n-nodes-langchain.chainLlm\",\n      \"position\": [\n        1720,\n        640\n      ],\n      \"parameters\": {\n        \"text\": \"=CONTEXT IMPORTANCE:\\nReddit data is used as a critical indicator of news story potential because:\\nâ€¢ High upvotes indicate strong public interest\\nâ€¢ Comment volume shows discussion engagement\\nâ€¢ Comment sentiment reveals public perception\\nâ€¢ Discussion threads expose knowledge gaps and controversies\\nâ€¢ Community reaction predicts potential viral spread\\nâ€¢ Sub-discussions highlight unexplored angles\\nâ€¢ Engagement patterns suggest story longevity\\n\\nINPUT CONTEXT:\\nNews URL: {{ $('Set for Loop').first().json.URL }}\\nNews Content:\\n<News_Content>\\n{{ $json.content }}\\n</News_Content>\\nReddit Metrics:\\nâ€¢ Post Title (Understanding how the story was shared): {{ $('Set for Loop').first().json.Title }}\\nâ€¢ Upvotes (Indicator of initial interest): {{ $('Set for Loop').first().json.Upvotes }}\\nâ€¢ Total Comments (Engagement level): {{ $('Set for Loop').first().json.Comments }}\\nReddit Sentiment Analysis:\\n<Sentiment_Analysis>\\n{{ $('Comments Analysis').first().json.text.replace(/[\\\\s\\\\S]*<comments_analysis>/, '').replace(/<\\\\/comments_analysis>[\\\\s\\\\S]*/, '') }}\\n</Sentiment_Analysis>\\n\\nFor each story, analyze through these dimensions:\\n\\n1. POPULARITY ASSESSMENT:\\n   A. Reddit Performance:\\n      â€¢ Upvote ratio and volume\\n      â€¢ Comment engagement rate\\n      â€¢ Discussion quality metrics\\n      â€¢ Viral spread indicators\\n      \\n   B. Audience Reception:\\n      â€¢ Initial reaction patterns\\n      â€¢ Discussion evolution\\n      â€¢ Community consensus vs. debate\\n      â€¢ Information seeking behavior\\n\\n1. CONTENT ANALYSIS:\\n   A. Core Story Elements:\\n      â€¢ Central narrative\\n      â€¢ Key stakeholders\\n      â€¢ Market implications\\n      â€¢ Industry impact\\n      \\n   B. Technical Analysis:\\n      â€¢ Writing style\\n      â€¢ Data presentation\\n      â€¢ Expert citations\\n      â€¢ Supporting evidence\\n\\n2. SOCIAL PROOF INTEGRATION:\\n   A. Engagement Metrics:\\n      â€¢ Reddit performance metrics\\n      â€¢ Discussion quality indicators\\n      â€¢ Viral spread patterns\\n      \\n   B. Sentiment Patterns:\\n      â€¢ Primary audience reactions\\n      â€¢ Controversial elements\\n      â€¢ Support vs. criticism ratio\\n      â€¢ Knowledge gaps identified\\n\\n3. NARRATIVE OPPORTUNITY MAPPING:\\n   A. Current Coverage:\\n      â€¢ Main angles covered\\n      â€¢ Supporting arguments\\n      â€¢ Counter-arguments\\n      â€¢ Expert perspectives\\n      \\n   B. Gap Analysis:\\n      â€¢ Unexplored perspectives\\n      â€¢ Missing stakeholder voices\\n      â€¢ Underutilized data points\\n      â€¢ Potential counter-narratives\\n\\nOUTPUT FORMAT (Place inside XML tags <news_analysis>):\\n\\nSTORY OVERVIEW:\\nTitle: [Most compelling angle]\\nURL: [Source]\\nCategory: [Industry/Topic]\\n\\nCONTENT SUMMARY:\\nTLDR: [3-5 sentences emphasizing viral potential]\\nCore Message: [One-line essence]\\n\\nKEY POINTS:\\nâ€¢ [Strategic point 1]\\nâ€¢ [Strategic point 2]\\nâ€¢ [Continue as needed]\\n\\nSOCIAL PROOF ANALYSIS:\\nEngagement Metrics:\\nâ€¢ Reddit Performance: [Metrics + Interpretation]\\nâ€¢ Discussion Quality: [Analysis of conversation depth]\\nâ€¢ Sentiment Distribution: [From sentiment analysis]\\n\\nVIRAL ELEMENTS:\\n1. Current Drivers:\\n   â€¢ [What's making it spread]\\n   â€¢ [Why people are engaging]\\n   â€¢ [Emotional triggers identified]\\n\\n2. Potential Amplifiers:\\n   â€¢ [Untapped viral elements]\\n   â€¢ [Engagement opportunities]\\n   â€¢ [Emotional hooks not yet used]\\n\\nNARRATIVE OPPORTUNITIES:\\n1. Unexplored Angles:\\n   â€¢ [Angle 1 + Why it matters]\\n   â€¢ [Angle 2 + Why it matters]\\n   â€¢ [Angle 3 + Why it matters]\\n\\n2. Content Gaps:\\n   â€¢ [Missing perspectives]\\n   â€¢ [Underutilized data]\\n   â€¢ [Stakeholder voices needed]\\n\\n3. Controversy Points:\\n   â€¢ [Debate opportunities]\\n   â€¢ [Conflicting viewpoints]\\n   â€¢ [Areas of misconception]\\n\\nSTRATEGIC RECOMMENDATIONS:\\n1. Immediate Opportunities:\\n   â€¢ [Quick-win suggestions]\\n   â€¢ [Timing considerations]\\n\\n2. Development Needs:\\n   â€¢ [Required research]\\n   â€¢ [Expert input needed]\\n   â€¢ [Data gaps to fill]\\n\\nPR POTENTIAL SCORE: [1-10 scale with explanation]\\n\\nFocus on elements that:\\nâ€¢ Show strong viral potential\\nâ€¢ Address identified audience concerns\\nâ€¢ Fill gaps in current coverage\\nâ€¢ Leverage positive sentiment patterns\\nâ€¢ Address or utilize controversial elements\\nâ€¢ Can be developed into unique angles\\n\\nNote: Prioritize insights that:\\n1. Build on identified sentiment patterns\\n2. Address audience knowledge gaps\\n3. Leverage existing engagement drivers\\n4. Can create breakthrough narratives\\n5. Have immediate PR potential\",\n        \"messages\": {\n          \"messageValues\": [\n            {\n              \"message\": \"=You are an expert PR Content Analyst specialized in identifying viral potential in news stories. Your mission is to analyze news content while leveraging Reddit engagement metrics and sentiment data to evaluate news popularity and potential PR opportunities.\"\n            }\n          ]\n        },\n        \"promptType\": \"define\"\n      },\n      \"typeVersion\": 1.5\n    },\n    {\n      \"id\": \"c4905ed1-324a-4b08-a1f4-f5465229b56c\",\n      \"name\": \"Stories Report\",\n      \"type\": \"@n8n/n8n-nodes-langchain.chainLlm\",\n      \"position\": [\n        2060,\n        640\n      ],\n      \"parameters\": {\n        \"text\": \"=INPUT CONTEXT:\\nNews Analysis: \\n<News_Analysis>\\n{{ $json.text.replace(/[\\\\s\\\\S]*<news_analysis>/, '').replace(/<\\\\/news_analysis>[\\\\s\\\\S]*/, '') }}\\n</News_Analysis>\\nReddit Metrics:\\nâ€¢ Post Title (Understanding how the story was shared): {{ $('Set for Loop').first().json.Title }}\\nâ€¢ Upvotes (Indicator of initial interest): {{ $('Set for Loop').first().json.Upvotes }}\\nâ€¢ Total Comments (Engagement level): {{ $('Set for Loop').first().json.Comments }}\\nReddit Sentiment Analysis:\\n<Sentiment_Analysis>\\n{{ $('Comments Analysis').first().json.text.replace(/[\\\\s\\\\S]*<comments_analysis>/, '').replace(/<\\\\/comments_analysis>[\\\\s\\\\S]*/, '') }}\\n</Sentiment_Analysis>\\n\\nOUTPUT FORMAT (Place inside XML tags <new_stories_report>):\\n\\nTREND ANALYSIS SUMMARY:\\nTopic: [News topic/category]\\nCurrent Coverage Status: [Overview of existing coverage]\\nAudience Reception: [From Reddit/sentiment analysis]\\nMarket Timing: [Why now is relevant]\\n\\nSTORY OPPORTUNITIES:\\n\\n1. FIRST-MOVER STORIES:\\n[For each story idea (2-3)]\\n\\nStory #1:\\nâ€¢ Headline: [Compelling title]\\nâ€¢ Hook: [One-line grabber]\\nâ€¢ Story Summary: [2-3 sentences]\\nâ€¢ Why It Works:\\n  - Audience Evidence: [From Reddit data]\\n  - Market Gap: [From news analysis]\\n  - Timing Advantage: [Why now]\\nâ€¢ Development Needs:\\n  - Research Required: [List]\\n  - Expert Input: [Specific needs]\\n  - Supporting Data: [What's needed]\\nâ€¢ Media Strategy:\\n  - Primary Targets: [Publications]\\n  - Secondary Targets: [Publications]\\n  - Exclusive Potential: [Yes/No + Rationale]\\nâ€¢ Success Metrics:\\n  - Coverage Goals: [Specific targets]\\n  - Engagement Expectations: [Based on Reddit data]\\n\\n2. TREND-AMPLIFIER STORIES:\\n[Same format as above for 2-3 stories]\\n\\nPRIORITY RANKING:\\n1. [Story Title] - Score: [X/10]\\n   â€¢ Impact Potential: [Score + Rationale]\\n   â€¢ Resource Requirements: [High/Medium/Low]\\n   â€¢ Timeline: [Immediate/Short-term/Long-term]\\n   \\n2. [Continue for all stories]\\n\\nEXECUTION ROADMAP:\\nâ€¢ Immediate Actions (24-48 hours)\\nâ€¢ Week 1 Priorities\\nâ€¢ Risk Management\\nâ€¢ Contingency Plans\\n\\nSTRATEGIC RECOMMENDATIONS:\\nâ€¢ Core Strategy\\nâ€¢ Alternative Angles\\nâ€¢ Resource Requirements\\nâ€¢ Timeline Considerations\\n\\nANALYTICAL FRAMEWORK:\\n\\n1. TREND VALIDATION:\\n   A. Story Performance Indicators:\\n      â€¢ Reddit engagement metrics\\n      â€¢ Public sentiment patterns\\n      â€¢ Discussion quality\\n      â€¢ Viral elements identified\\n\\n   B. Current Narrative Landscape:\\n      â€¢ Dominant themes from news analysis\\n      â€¢ Public perception gaps\\n      â€¢ Controversial elements\\n      â€¢ Underserved perspectives\\n\\n2. OPPORTUNITY MAPPING:\\n   A. Content Gap Analysis:\\n      â€¢ Unexplored angles from news analysis\\n      â€¢ Audience questions from comments\\n      â€¢ Missing expert perspectives\\n      â€¢ Data/research opportunities\\n\\n   B. Timing Assessment:\\n      â€¢ News cycle position\\n      â€¢ Trend trajectory\\n      â€¢ Optimal launch window\\n      â€¢ Competition consideration\\n\\nPR STORY OPPORTUNITIES:\\nGenerate 4-6 high-potential story ideas, categorized as:\\n\\nA. \\\\\\\"FIRST-MOVER\\\\\\\" OPPORTUNITIES (2-3 ideas):\\nFor each idea:\\n\\n1. Story Concept:\\n   â€¢ Headline\\n   â€¢ Sub-headline\\n   â€¢ Key message\\n   â€¢ Unique selling point\\n\\n2. Why It Works:\\n   â€¢ Gap in current coverage\\n   â€¢ Evidence from Reddit discussions\\n   â€¢ Sentiment analysis support\\n   â€¢ Market timing rationale\\n\\n3. Development Requirements:\\n   â€¢ Required data/research\\n   â€¢ Expert perspectives needed\\n   â€¢ Supporting elements\\n   â€¢ Potential challenges\\n\\n4. Media Strategy:\\n   â€¢ Target publications\\n   â€¢ Journalist appeal factors\\n   â€¢ Exclusive potential\\n   â€¢ Supporting assets needed\\n\\nB. \\\\\\\"TREND-AMPLIFIER\\\\\\\" OPPORTUNITIES (2-3 ideas):\\n[Same structure as above, but focused on enhancing existing narratives]\\n\\nSTORY PRIORITIZATION MATRIX:\\nFor each story idea:\\n1. Impact Potential (1-10):\\n   â€¢ Audience interest indicators\\n   â€¢ Media appeal factors\\n   â€¢ Viral potential\\n   â€¢ Business value\\n\\n2. Resource Requirements:\\n   â€¢ Time to develop\\n   â€¢ Research needs\\n   â€¢ Expert input\\n   â€¢ Asset creation\\n\\n3. Risk Assessment:\\n   â€¢ Competition factors\\n   â€¢ Timing risks\\n   â€¢ Narrative challenges\\n   â€¢ Mitigation strategies\\n\\nEXECUTION ROADMAP:\\n1. Immediate Actions (Next 24-48 hours):\\n   â€¢ Priority research needs\\n   â€¢ Expert outreach\\n   â€¢ Data gathering\\n   â€¢ Asset development\\n\\n2. Development Timeline:\\n   â€¢ Story development sequence\\n   â€¢ Key milestones\\n   â€¢ Decision points\\n   â€¢ Launch windows\\n\\n3. Success Metrics:\\n   â€¢ Coverage targets\\n   â€¢ Engagement goals\\n   â€¢ Share of voice objectives\\n   â€¢ Impact measurements\\n\\nSTRATEGIC RECOMMENDATIONS:\\n1. Primary Strategy:\\n   â€¢ Core approach\\n   â€¢ Key differentiators\\n   â€¢ Critical success factors\\n   â€¢ Risk mitigation\\n\\n2. Alternative Approaches:\\n   â€¢ Backup angles\\n   â€¢ Pivot opportunities\\n   â€¢ Alternative narratives\\n   â€¢ Contingency plans\\n\\nFocus on creating stories that:\\nâ€¢ Address identified audience interests (from Reddit data)\\nâ€¢ Fill gaps in current coverage\\nâ€¢ Leverage positive sentiment patterns\\nâ€¢ Solve for identified pain points\\nâ€¢ Offer unique, data-backed perspectives\\nâ€¢ Present clear competitive advantages\\n\\nBased on the provided news analysis, Reddit metrics, and sentiment analysis, please generate a comprehensive PR strategy report following the format above.\",\n        \"messages\": {\n          \"messageValues\": [\n            {\n              \"message\": \"=You are an elite PR Strategy Consultant specialized in crafting breakthrough story angles that capture media attention. Your mission is to analyze trending story patterns and develop high-impact PR opportunities based on comprehensive data analysis.\\n\\nCONTEXT IMPORTANCE:\\nThis analysis combines three critical data sources:\\n1. Reddit Engagement Data:\\n   â€¢ Indicates public interest levels\\n   â€¢ Shows organic discussion patterns\\n   â€¢ Reveals audience sentiment\\n   â€¢ Highlights knowledge gaps\\n   â€¢ Demonstrates viral potential\\n\\n2. News Content Analysis:\\n   â€¢ Provides core story elements\\n   â€¢ Shows current media angles\\n   â€¢ Identifies market implications\\n   â€¢ Reveals coverage gaps\\n   â€¢ Maps expert perspectives\\n\\n3. Sentiment Analysis:\\n   â€¢ Reveals public perception\\n   â€¢ Identifies controversy points\\n   â€¢ Shows emotional triggers\\n   â€¢ Highlights audience concerns\\n   â€¢ Indicates story longevity\\n\\nThis combined data helps us:\\nâ€¢ Validate story potential\\nâ€¢ Identify unexplored angles\\nâ€¢ Understand audience needs\\nâ€¢ Predict media interest\\nâ€¢ Craft compelling narratives\"\n            }\n          ]\n        },\n        \"promptType\": \"define\"\n      },\n      \"typeVersion\": 1.5\n    },\n    {\n      \"id\": \"1379c60b-387c-4eba-a7c2-2bcb1cda48fd\",\n      \"name\": \"Set Data\",\n      \"type\": \"n8n-nodes-base.set\",\n      \"position\": [\n        240,\n        420\n      ],\n      \"parameters\": {\n        \"options\": {},\n        \"assignments\": {\n          \"assignments\": [\n            {\n              \"id\": \"b4da0605-b5e1-47e1-8e7e-00158ecaba33\",\n              \"name\": \"Topics\",\n              \"type\": \"string\",\n              \"value\": \"=Donald Trump\\nPolitics\"\n            },\n            {\n              \"id\": \"d7602355-7082-4e98-a0b5-a400fade6dbc\",\n              \"name\": \"Jina API Key\",\n              \"type\": \"string\",\n              \"value\": \"YOUR_API_KEY\"\n            }\n          ]\n        }\n      },\n      \"typeVersion\": 3.4\n    }\n  ],\n  \"active\": false,\n  \"pinData\": {},\n  \"settings\": {\n    \"executionOrder\": \"v1\"\n  },\n  \"versionId\": \"dad1fb7a-599f-4b98-9461-8b27baa774d9\",\n  \"connections\": {\n    \"Set Data\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Split Topics into Items\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Aggregate\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Merge Binary Files\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Keep Last\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"News Analysis\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Get Comments\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Extract Top Comments\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Search Posts\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Upvotes Requirement Filtering\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Set for Loop\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Get Comments\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Google Drive6\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Google Drive7\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Google Drive7\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Send files to Mattermost3\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"News Analysis\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Stories Report\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Compress files\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Google Drive6\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Stories Report\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Set Final Report\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Convert to File\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Loop Over Items\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Format Comments\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Comments Analysis\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Loop Over Items\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Aggregate\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ],\n        [\n          {\n            \"node\": \"Set for Loop\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Get News Content\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Keep Last\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Schedule Trigger\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Set Data\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Set Final Report\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Convert to File\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Set Reddit Posts\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Remove Duplicates\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Comments Analysis\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Get News Content\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Remove Duplicates\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Loop Over Items\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Merge Binary Files\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Compress files\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Anthropic Chat Model\": {\n      \"ai_languageModel\": [\n        [\n          {\n            \"node\": \"Comments Analysis\",\n            \"type\": \"ai_languageModel\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Extract Top Comments\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Format Comments\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Anthropic Chat Model1\": {\n      \"ai_languageModel\": [\n        [\n          {\n            \"node\": \"News Analysis\",\n            \"type\": \"ai_languageModel\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Anthropic Chat Model2\": {\n      \"ai_languageModel\": [\n        [\n          {\n            \"node\": \"Stories Report\",\n            \"type\": \"ai_languageModel\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Split Topics into Items\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Search Posts\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Upvotes Requirement Filtering\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Set Reddit Posts\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    }\n  }\n}"
}